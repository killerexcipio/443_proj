{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89d3c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2.2] Setup: import libraries used across dataset loading, DL model, training/eval, and benchmarks\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "from itertools import accumulate\n",
    "from typing import Optional, List, Tuple\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import PearsonCorrCoef\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from transformers import BertPreTrainedModel\n",
    "from transformers.modeling_utils import ModuleUtilsMixin\n",
    "from transformers.models.bert.modeling_bert import (\n",
    "    BertConfig, \n",
    "    BertEmbeddings, \n",
    "    BertEncoder, \n",
    "    BertLayer, \n",
    "    BertAttention\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7900a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2.1/2.3] Data pipeline: load EN-selected SNPs, compute LD-gap tokens, build sequences, standardize labels, and create KFold split-based PyTorch Dataset\n",
    "class DNADataset(Dataset):\n",
    "    # Updated __init__ to accept ld_threshold\n",
    "    def __init__(self, data_path, label_path, geno_path, trait, seed, sel_num, ld_threshold=0.8, is_training=True):\n",
    "        cs = pd.read_csv(f\"{data_path}{seed}.csv\").sort_values(by='cs', ascending=False)\n",
    "        Top = sorted(cs.index[:sel_num])  \n",
    "        Rawgeno = pd.read_csv(geno_path)\n",
    "\n",
    "        # DROP the first row (SNP index row)\n",
    "        Rawgeno = Rawgeno.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "        Top = [i for i in Top if i in Rawgeno.index]\n",
    "\n",
    "        geno = Rawgeno.loc[Top].copy()\n",
    "\n",
    "        # explicitly separate columns\n",
    "        geno_cols = [c for c in geno.columns if c not in ['chrom']]\n",
    "\n",
    "        LD = self.calculate_LD(geno)\n",
    "        \n",
    "        # Pass the variable threshold here\n",
    "        geno['gap'] = self.assign_gap_labels(LD, ld_threshold)\n",
    "        \n",
    "        geno = geno.drop(columns=geno.columns[[-3, -2]])  \n",
    "        lines = self.generate_geno_sequences(geno)\n",
    "        annos = pd.read_csv(label_path, index_col=0).iloc[:, [trait]]\n",
    "        annos = annos.fillna(annos.mean()) \n",
    "        annos = StandardScaler().fit_transform(annos).astype(np.float32)\n",
    "\n",
    "        kfold = KFold(n_splits=5, shuffle=True, random_state=27)\n",
    "        for i, (train_idx, val_idx) in enumerate(kfold.split(lines, annos)):\n",
    "            if i == seed:\n",
    "                train_lines, val_lines = lines[train_idx], lines[val_idx]\n",
    "                train_annos, val_annos = annos[train_idx], annos[val_idx]                \n",
    "                break\n",
    "\n",
    "        train_seqs, train_type_ids = self.process_sequences(train_lines)\n",
    "        val_seqs, val_type_ids = self.process_sequences(val_lines)\n",
    "\n",
    "        if is_training:\n",
    "            self.seqs, self.type_ids, self.annos = train_seqs, train_type_ids, train_annos\n",
    "        else:\n",
    "            self.seqs, self.type_ids, self.annos = val_seqs, val_type_ids, val_annos\n",
    "\n",
    "    def calculate_LD(self, geno):\n",
    "        geno_values = np.select(\n",
    "            [geno.iloc[:, :-2].values == 'H', geno.iloc[:, :-2].values == 'M', geno.iloc[:, :-2].values == 'L'],\n",
    "            [0, 1, 2],\n",
    "            default=-1\n",
    "        ) \n",
    "\n",
    "        a, b = geno_values[:-1], geno_values[1:]  \n",
    "        var_a, var_b = np.var(a, axis=1), np.var(b, axis=1)\n",
    "        mean_a, mean_b = np.mean(a, axis=1), np.mean(b, axis=1)\n",
    "        d = np.mean((a - mean_a[:, None]) * (b - mean_b[:, None]), axis=1)\n",
    "        \n",
    "        LD = np.where((var_a == 0) | (var_b == 0), 0, (d ** 2) / (var_a * var_b))\n",
    "        LD = np.append(LD, -1)  \n",
    "\n",
    "        chrom = sorted(set(geno['chrom']))\n",
    "        index = list(accumulate([len(geno.groupby('chrom').get_group(i)) for i in chrom])) \n",
    "        for idx in index:\n",
    "            LD[idx - 1] = -1        \n",
    "        return LD\n",
    "\n",
    "    # Updated to accept threshold parameter\n",
    "    def assign_gap_labels(self, LD, threshold):\n",
    "        return np.where(LD == -1, 'N', np.where(LD >= threshold, 'J', 'Y'))\n",
    "\n",
    "    def generate_geno_sequences(self, geno):\n",
    "        lines = []\n",
    "        for i in range(geno.shape[1] - 1):\n",
    "            geno.iloc[:, i] = geno.iloc[:, i] + geno['gap']\n",
    "            lines.append(''.join(geno.iloc[:, i]))\n",
    "        return np.stack(lines, axis=0)\n",
    "\n",
    "    def process_sequences(self, lines):\n",
    "        vocabs = {f\"{a}{b}\": i + 1 for i, (a, b) in enumerate([(\"H\", \"J\"), (\"H\", \"Y\"), (\"H\", \"N\"), (\"L\", \"J\"), (\"L\", \"Y\"), (\"L\", \"N\"), (\"M\", \"J\"), (\"M\", \"Y\"), (\"M\", \"N\")])}\n",
    "        type_vocabs = {\"J\": 1, \"Y\": 2, \"N\": 3}\n",
    "\n",
    "        seqs, type_ids = [], []\n",
    "        for raw_seq in lines:\n",
    "            seq, type_id = [], []\n",
    "            for i in range(0, len(raw_seq), 2):\n",
    "                seq.append(vocabs[raw_seq[i:i + 2]])\n",
    "                type_id.append(type_vocabs[raw_seq[i + 1]])\n",
    "            seqs.append(seq)\n",
    "            type_ids.append(type_id)\n",
    "\n",
    "        return np.asarray(seqs), np.asarray(type_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annos)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq = torch.tensor(self.seqs[index], dtype=torch.float32)\n",
    "        type_ids = torch.tensor(self.type_ids[index], dtype=torch.float32)\n",
    "        annos = torch.tensor(self.annos[index], dtype=torch.float32)\n",
    "        return seq, type_ids, annos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8adbe71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2.2] Model definitions: EBMGP architecture + pooling modules (MAP/AVG/MAX/LIP) to support pooling ablation experiments\n",
    "class soft_pool1d(nn.Module):\n",
    "    def __init__(self,  kernel_size=2):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride= kernel_size\n",
    "    def forward(self, x):\n",
    "        e_x = torch.sum(torch.exp(x),dim=1,keepdim=True)\n",
    "        return F.avg_pool1d(x.mul(e_x), self.kernel_size, stride=self.stride).mul_(self.kernel_size).div_(F.avg_pool1d(e_x, self.kernel_size, stride=self.stride).mul_(self.kernel_size))\n",
    "\n",
    "def lip1d(x, logit, kernel=3, stride=2, padding=1):\n",
    "    weight = logit.exp()\n",
    "    return F.avg_pool1d(x*weight, kernel, stride, padding)/F.avg_pool1d(weight, kernel, stride, padding)\n",
    "    \n",
    "class LIP(nn.Module):\n",
    "    # Updated to accept pool_size to match AttentionPool's stride/reduction behavior\n",
    "    def __init__(self, channels, pool_size=2):\n",
    "        super(LIP, self).__init__()\n",
    "        self.pool_size = pool_size\n",
    "        self.logit = nn.Sequential(\n",
    "                nn.Conv1d(channels, channels, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(channels, affine=True),\n",
    "                nn.ReLU(),\n",
    "        )\n",
    "    def init_layer(self):\n",
    "        self.logit[0].weight.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass pool_size as kernel and stride to mimic other pooling layers\n",
    "        # padding is set to pool_size // 2 to maintain consistency\n",
    "        frac = lip1d(x, self.logit(x), kernel=self.pool_size, stride=self.pool_size, padding=self.pool_size//2)\n",
    "        return frac\n",
    "\n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, dim, pool_size=2,dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.pool_size = pool_size\n",
    "        self.attn_dropout = nn.Dropout(dropout_prob)\n",
    "        self.to_attn_logits1 = nn.Conv2d(dim, dim, 1, bias=False)\n",
    "        self.to_attn_logits2 = nn.ModuleList([nn.Conv1d(dim, dim, 1, bias=False) for _ in range(pool_size)])\n",
    "        self.BN = nn.BatchNorm1d(dim)\n",
    "    def forward(self, x):\n",
    "        b, s, n = x.shape  \n",
    "        remainder = n % self.pool_size\n",
    "        needs_padding = remainder > 0      \n",
    "        if needs_padding:\n",
    "            x = F.pad(x, (0, (self.pool_size-remainder)), value=0) \n",
    "        x = x.unfold(-1,self.pool_size,self.pool_size) \n",
    "        outx = []\n",
    "        i = 0\n",
    "        for conv in self.to_attn_logits2:                \n",
    "            nx = x[:,:,:,i]\n",
    "            nx = self.BN(nx)\n",
    "            logit = conv(nx)           \n",
    "            outx.append(logit)\n",
    "            i+=1 \n",
    "        outx = torch.stack(outx, dim=-1)       \n",
    "        logits =  self.to_attn_logits1(outx)\n",
    "        logits = self.attn_dropout(logits)             \n",
    "        attn = logits.softmax(dim=-1)             \n",
    "        outs = (outx * attn).sum(dim=-1)\n",
    "        return outs\n",
    "  \n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(1.702 * x) * x\n",
    "        \n",
    "def ConvBlock(dim, dimout, kernel_size = 1,stride=2):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm1d(dim),\n",
    "        GELU(),\n",
    "        nn.Conv1d(dim,  dimout, kernel_size,stride=stride, padding = kernel_size // 2)\n",
    "    )\n",
    "\n",
    "class EBMGP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 10,\n",
    "        type_vocab_size: int = 4,\n",
    "        hidden_size: int = 64,\n",
    "        num_layers: int = 1,\n",
    "        num_attention_heads: int = 8,\n",
    "        intermediate_size: int = 256,\n",
    "        hidden_act: str = \"gelu\",\n",
    "        dropout_rate: float = 0.3,\n",
    "        pooling_type: str = \"MAP\" # New Argument\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = BertConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_hidden_layers=num_layers,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            intermediate_size=intermediate_size,\n",
    "            hidden_act=hidden_act,\n",
    "            type_vocab_size=type_vocab_size,\n",
    "            max_position_embeddings=5000, \n",
    "            dropout_rate = dropout_rate,\n",
    "        )\n",
    "\n",
    "        self.embeddings = BertEmbeddings(self.config)\n",
    "\n",
    "        # Factory method to select pooling layer\n",
    "        def get_pool(dim, size):\n",
    "            if pooling_type == \"MAP\":\n",
    "                return AttentionPool(dim, pool_size=size)\n",
    "            elif pooling_type == \"AVG\":\n",
    "                return nn.AvgPool1d(kernel_size=size, stride=size)\n",
    "            elif pooling_type == \"MAX\":\n",
    "                return nn.MaxPool1d(kernel_size=size, stride=size)\n",
    "            elif pooling_type == \"LIP\":\n",
    "                return LIP(dim, pool_size=size)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown pooling type: {pooling_type}\")\n",
    "\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            ConvBlock(hidden_size, 64,30,stride=2),\n",
    "            get_pool(64, 1), # Replaced hardcoded AttentionPool\n",
    "            nn.Dropout(dropout_rate), \n",
    "\n",
    "            nn.BatchNorm1d(64),\n",
    "            ConvBlock(64, 64, 3,stride=2),            \n",
    "            get_pool(64, 2), # Replaced hardcoded AttentionPool\n",
    "            nn.Dropout(dropout_rate), \n",
    "\n",
    "            nn.BatchNorm1d(64),\n",
    "            ConvBlock(64, 64, 30,stride=2),           \n",
    "            get_pool(64, 3), # Replaced hardcoded AttentionPool\n",
    "            nn.Dropout(dropout_rate), \n",
    "\n",
    "            nn.BatchNorm1d(64),\n",
    "            ConvBlock(64, 64, 3,stride=2),            \n",
    "            get_pool(64, 2), # Replaced hardcoded AttentionPool\n",
    "            nn.Dropout(dropout_rate), \n",
    "\n",
    "            nn.BatchNorm1d(64),\n",
    "            ConvBlock(64, 64, 30,stride=2),           \n",
    "            get_pool(64, 3), # Replaced hardcoded AttentionPool\n",
    "            nn.Dropout(dropout_rate), \n",
    "        )\n",
    "          \n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),       \n",
    "            Rearrange('b c l -> b (c l)'), \n",
    "            nn.Linear(64, 1),              \n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        token_type_ids: torch.Tensor,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "        output_hidden_states: bool = False,\n",
    "        return_dict: bool = False,\n",
    "    ):\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids.long(),\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids.long(),\n",
    "        )\n",
    "        x = embedding_output.permute(0, 2, 1) \n",
    "        x = self.convs(x)\n",
    "        logits = self.predictor(x) \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35cc86b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2.3] Training & evaluation: reproducibility seed, epoch loops, metrics (Pearson/RMSE/R2), and a train_and_evaluate wrapper used by experiments\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchmetrics import PearsonCorrCoef\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def setup_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, loss_fn, scheduler, device):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for batch_idx, (seqs, type_ids, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        seqs, type_ids, labels = seqs.to(device), type_ids.to(device), labels.to(device)\n",
    "        predict = model(seqs, type_ids)\n",
    "        loss = loss_fn(predict, labels)\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "\n",
    "def evaluate_epoch(model, test_loader, loss_fn, pearson, device):\n",
    "    model.eval()\n",
    "    valid_loss = []\n",
    "    vp, vt = [], []\n",
    "    with torch.no_grad():\n",
    "        for seqs, type_ids, labels in test_loader:\n",
    "            seqs, type_ids, labels = seqs.to(device), type_ids.to(device), labels.to(device)\n",
    "            pred = model(seqs, type_ids)\n",
    "            val_loss = loss_fn(pred, labels)\n",
    "            valid_loss.append(val_loss.item())\n",
    "            vp.extend(pred.squeeze().cpu().numpy())\n",
    "            vt.extend(labels.squeeze().cpu().numpy())\n",
    "    return np.mean(valid_loss), vp, vt\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Added pooling_type argument\n",
    "def train_and_evaluate(trait, data_path, label_path, geno_path, device,\n",
    "                       learning_rate, epochs, seed, sel_num,\n",
    "                       ld_threshold=0.8, pooling_type=\"MAP\"):\n",
    "\n",
    "    # Make runs reproducible BUT different per fold\n",
    "    setup_seed(3407 + seed)   # (you can also do setup_seed(seed) if you prefer)\n",
    "\n",
    "    loss_fn = nn.L1Loss()\n",
    "    bs = 32\n",
    "\n",
    "    traindataset = DNADataset(\n",
    "        data_path, label_path, geno_path, trait, seed, sel_num,\n",
    "        ld_threshold=ld_threshold, is_training=True\n",
    "    )\n",
    "    testdataset = DNADataset(\n",
    "        data_path, label_path, geno_path, trait, seed, sel_num,\n",
    "        ld_threshold=ld_threshold, is_training=False\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(traindataset, batch_size=bs, shuffle=True)\n",
    "    test_loader  = DataLoader(testdataset,  batch_size=bs, shuffle=False)\n",
    "\n",
    "    print(f\"Seed {seed} | Pool {pooling_type} | Train Samples: {len(traindataset)} | Test Samples: {len(testdataset)}\")\n",
    "\n",
    "    model = EBMGP(pooling_type=pooling_type).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    steps = math.ceil(len(traindataset) / bs) * epochs - 1\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=steps)\n",
    "\n",
    "    pearson = PearsonCorrCoef().to(device)\n",
    "\n",
    "    # ----- NEW: track best epoch by Pearson correlation -----\n",
    "    best_corr = -1e9\n",
    "    best_rmse = None\n",
    "    best_pred = None\n",
    "    best_obser = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    patience = 15\n",
    "    min_delta = 1e-4\n",
    "    no_improve = 0\n",
    "\n",
    "    # If runtime is slow, change this to 5 or 10\n",
    "    eval_every = 1\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, loss_fn, scheduler, device)\n",
    "\n",
    "        do_eval = (epoch % eval_every == 0) or (epoch == epochs)\n",
    "        if do_eval:\n",
    "            valid_loss, vp, vt = evaluate_epoch(model, test_loader, loss_fn, pearson, device)\n",
    "\n",
    "            # vp/vt are python lists in your evaluate_epoch() (cell [2.3])\n",
    "            val_rmse = np.sqrt(mean_squared_error(vp, vt))\n",
    "\n",
    "            v_corr = pearson(\n",
    "                torch.tensor(vp, device=device),\n",
    "                torch.tensor(vt, device=device)\n",
    "            ).item()\n",
    "\n",
    "            # Keep the best checkpoint by Pearson correlation\n",
    "            if v_corr > best_corr + min_delta:\n",
    "                best_corr = v_corr\n",
    "                best_rmse = val_rmse\n",
    "                best_pred = list(vp)\n",
    "                best_obser = list(vt)\n",
    "                best_epoch = epoch\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "\n",
    "            if no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} (best epoch {best_epoch}, best corr {best_corr:.4f})\")\n",
    "                break\n",
    "\n",
    "            if epoch % 10 == 0 or epoch == 1 or epoch == epochs:\n",
    "                print(f\"   Epoch {epoch}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {valid_loss:.4f} | Val Corr: {v_corr:.4f} | Val RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "    print(f\"BEST epoch = {best_epoch} | BEST Corr = {best_corr:.4f} | RMSE@BEST = {best_rmse:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Return best-by-corr metrics (this will directly affect your benchmark table)\n",
    "    return best_corr, best_rmse, best_pred, best_obser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ecf009a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "RUNNING EXPERIMENT: LD THRESHOLD ABLATION\n",
      "############################################################\n",
      "\n",
      "   >>> Processing LD Threshold: 0.2 | Species: rice | Feature Count: T5000\n",
      "      Processing Trait: SW\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.8110 | Val Loss: 0.6833 | Val Corr: 0.5132 | Val RMSE: 0.8941\n",
      "   Epoch 10/100 | Train Loss: 0.6960 | Val Loss: 0.5553 | Val Corr: 0.5883 | Val RMSE: 0.7300\n",
      "   Epoch 20/100 | Train Loss: 0.6229 | Val Loss: 0.5179 | Val Corr: 0.6327 | Val RMSE: 0.6966\n",
      "   Epoch 30/100 | Train Loss: 0.5650 | Val Loss: 0.5836 | Val Corr: 0.6070 | Val RMSE: 0.7694\n",
      "Early stopping at epoch 39 (best epoch 24, best corr 0.6645)\n",
      "BEST epoch = 24 | BEST Corr = 0.6645 | RMSE@BEST = 0.6970\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7621 | Val Loss: 0.7083 | Val Corr: 0.3723 | Val RMSE: 1.0496\n",
      "   Epoch 10/100 | Train Loss: 0.6257 | Val Loss: 0.7149 | Val Corr: 0.5293 | Val RMSE: 0.9899\n",
      "   Epoch 20/100 | Train Loss: 0.5686 | Val Loss: 0.6057 | Val Corr: 0.5604 | Val RMSE: 0.8808\n",
      "   Epoch 30/100 | Train Loss: 0.5169 | Val Loss: 0.6183 | Val Corr: 0.5631 | Val RMSE: 0.9010\n",
      "   Epoch 40/100 | Train Loss: 0.4445 | Val Loss: 0.6360 | Val Corr: 0.5573 | Val RMSE: 0.9075\n",
      "Early stopping at epoch 42 (best epoch 27, best corr 0.5779)\n",
      "BEST epoch = 27 | BEST Corr = 0.5779 | RMSE@BEST = 0.8728\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7697 | Val Loss: 0.8008 | Val Corr: 0.4657 | Val RMSE: 1.1143\n",
      "   Epoch 10/100 | Train Loss: 0.6308 | Val Loss: 0.6662 | Val Corr: 0.5550 | Val RMSE: 0.9297\n",
      "   Epoch 20/100 | Train Loss: 0.5566 | Val Loss: 0.6144 | Val Corr: 0.6358 | Val RMSE: 0.8922\n",
      "   Epoch 30/100 | Train Loss: 0.5124 | Val Loss: 0.6643 | Val Corr: 0.6457 | Val RMSE: 0.9267\n",
      "   Epoch 40/100 | Train Loss: 0.4701 | Val Loss: 0.6016 | Val Corr: 0.6349 | Val RMSE: 0.8755\n",
      "Early stopping at epoch 44 (best epoch 29, best corr 0.6631)\n",
      "BEST epoch = 29 | BEST Corr = 0.6631 | RMSE@BEST = 0.9057\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7672 | Val Loss: 0.7167 | Val Corr: 0.2937 | Val RMSE: 0.9370\n",
      "   Epoch 10/100 | Train Loss: 0.6280 | Val Loss: 0.6663 | Val Corr: 0.4390 | Val RMSE: 0.9063\n",
      "   Epoch 20/100 | Train Loss: 0.5304 | Val Loss: 0.6390 | Val Corr: 0.4362 | Val RMSE: 0.9015\n",
      "   Epoch 30/100 | Train Loss: 0.4833 | Val Loss: 0.6968 | Val Corr: 0.4640 | Val RMSE: 0.9504\n",
      "   Epoch 40/100 | Train Loss: 0.4426 | Val Loss: 0.7195 | Val Corr: 0.4730 | Val RMSE: 0.9893\n",
      "Early stopping at epoch 43 (best epoch 28, best corr 0.4795)\n",
      "BEST epoch = 28 | BEST Corr = 0.4795 | RMSE@BEST = 0.9006\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7703 | Val Loss: 0.7022 | Val Corr: 0.3724 | Val RMSE: 0.9796\n",
      "   Epoch 10/100 | Train Loss: 0.5899 | Val Loss: 0.6816 | Val Corr: 0.4933 | Val RMSE: 0.8921\n",
      "   Epoch 20/100 | Train Loss: 0.5431 | Val Loss: 0.7610 | Val Corr: 0.4615 | Val RMSE: 0.9969\n",
      "Early stopping at epoch 24 (best epoch 9, best corr 0.4933)\n",
      "BEST epoch = 9 | BEST Corr = 0.4933 | RMSE@BEST = 0.8594\n",
      "----------------------------------------\n",
      "      Processing Trait: FLW\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7410 | Val Loss: 0.6903 | Val Corr: 0.3512 | Val RMSE: 1.0361\n",
      "   Epoch 10/100 | Train Loss: 0.6513 | Val Loss: 0.6597 | Val Corr: 0.4203 | Val RMSE: 0.9428\n",
      "   Epoch 20/100 | Train Loss: 0.5883 | Val Loss: 0.6271 | Val Corr: 0.4830 | Val RMSE: 0.9296\n",
      "   Epoch 30/100 | Train Loss: 0.5744 | Val Loss: 0.6234 | Val Corr: 0.4501 | Val RMSE: 0.9277\n",
      "   Epoch 40/100 | Train Loss: 0.5386 | Val Loss: 0.6385 | Val Corr: 0.4297 | Val RMSE: 0.9484\n",
      "Early stopping at epoch 43 (best epoch 28, best corr 0.4980)\n",
      "BEST epoch = 28 | BEST Corr = 0.4980 | RMSE@BEST = 0.9078\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7337 | Val Loss: 0.7236 | Val Corr: 0.2712 | Val RMSE: 1.1505\n",
      "   Epoch 10/100 | Train Loss: 0.6573 | Val Loss: 0.7254 | Val Corr: 0.3694 | Val RMSE: 1.0741\n",
      "   Epoch 20/100 | Train Loss: 0.5660 | Val Loss: 0.6966 | Val Corr: 0.4428 | Val RMSE: 1.0349\n",
      "   Epoch 30/100 | Train Loss: 0.5262 | Val Loss: 0.6937 | Val Corr: 0.4453 | Val RMSE: 1.0317\n",
      "   Epoch 40/100 | Train Loss: 0.5338 | Val Loss: 0.7037 | Val Corr: 0.4412 | Val RMSE: 1.0349\n",
      "   Epoch 50/100 | Train Loss: 0.4818 | Val Loss: 0.7124 | Val Corr: 0.4452 | Val RMSE: 1.0323\n",
      "Early stopping at epoch 59 (best epoch 44, best corr 0.4847)\n",
      "BEST epoch = 44 | BEST Corr = 0.4847 | RMSE@BEST = 1.0117\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7257 | Val Loss: 0.7506 | Val Corr: 0.4137 | Val RMSE: 1.0745\n",
      "   Epoch 10/100 | Train Loss: 0.6464 | Val Loss: 0.6710 | Val Corr: 0.4153 | Val RMSE: 1.0049\n",
      "   Epoch 20/100 | Train Loss: 0.6028 | Val Loss: 0.6490 | Val Corr: 0.4662 | Val RMSE: 0.9756\n",
      "   Epoch 30/100 | Train Loss: 0.5684 | Val Loss: 0.7067 | Val Corr: 0.4424 | Val RMSE: 1.0448\n",
      "   Epoch 40/100 | Train Loss: 0.5381 | Val Loss: 0.6510 | Val Corr: 0.4651 | Val RMSE: 0.9728\n",
      "   Epoch 50/100 | Train Loss: 0.5317 | Val Loss: 0.6890 | Val Corr: 0.4353 | Val RMSE: 1.0317\n",
      "Early stopping at epoch 51 (best epoch 36, best corr 0.4919)\n",
      "BEST epoch = 36 | BEST Corr = 0.4919 | RMSE@BEST = 0.9511\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7489 | Val Loss: 0.6382 | Val Corr: 0.3145 | Val RMSE: 0.8505\n",
      "   Epoch 10/100 | Train Loss: 0.7095 | Val Loss: 0.5851 | Val Corr: 0.3189 | Val RMSE: 0.8033\n",
      "   Epoch 20/100 | Train Loss: 0.6104 | Val Loss: 0.5740 | Val Corr: 0.3199 | Val RMSE: 0.8162\n",
      "   Epoch 30/100 | Train Loss: 0.5965 | Val Loss: 0.5721 | Val Corr: 0.3235 | Val RMSE: 0.8193\n",
      "   Epoch 40/100 | Train Loss: 0.5257 | Val Loss: 0.6393 | Val Corr: 0.3412 | Val RMSE: 0.8651\n",
      "   Epoch 50/100 | Train Loss: 0.5085 | Val Loss: 0.6105 | Val Corr: 0.3679 | Val RMSE: 0.8370\n",
      "   Epoch 60/100 | Train Loss: 0.4948 | Val Loss: 0.6262 | Val Corr: 0.3218 | Val RMSE: 0.8501\n",
      "Early stopping at epoch 62 (best epoch 47, best corr 0.3890)\n",
      "BEST epoch = 47 | BEST Corr = 0.3890 | RMSE@BEST = 0.8200\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7449 | Val Loss: 0.5805 | Val Corr: 0.3767 | Val RMSE: 0.8302\n",
      "   Epoch 10/100 | Train Loss: 0.6230 | Val Loss: 0.6176 | Val Corr: 0.3741 | Val RMSE: 0.8163\n",
      "   Epoch 20/100 | Train Loss: 0.5654 | Val Loss: 0.6241 | Val Corr: 0.3879 | Val RMSE: 0.8336\n",
      "   Epoch 30/100 | Train Loss: 0.5405 | Val Loss: 0.6366 | Val Corr: 0.3758 | Val RMSE: 0.8425\n",
      "Early stopping at epoch 33 (best epoch 18, best corr 0.4023)\n",
      "BEST epoch = 18 | BEST Corr = 0.4023 | RMSE@BEST = 0.8374\n",
      "----------------------------------------\n",
      "      Processing Trait: AC\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7351 | Val Loss: 0.6010 | Val Corr: 0.2319 | Val RMSE: 0.9418\n",
      "   Epoch 10/100 | Train Loss: 0.6911 | Val Loss: 0.5189 | Val Corr: 0.2945 | Val RMSE: 0.9076\n",
      "   Epoch 20/100 | Train Loss: 0.6067 | Val Loss: 0.5014 | Val Corr: 0.4521 | Val RMSE: 0.8679\n",
      "   Epoch 30/100 | Train Loss: 0.5859 | Val Loss: 0.5120 | Val Corr: 0.3855 | Val RMSE: 0.8648\n",
      "   Epoch 40/100 | Train Loss: 0.5107 | Val Loss: 0.4860 | Val Corr: 0.4069 | Val RMSE: 0.8615\n",
      "   Epoch 50/100 | Train Loss: 0.4972 | Val Loss: 0.4585 | Val Corr: 0.4681 | Val RMSE: 0.8461\n",
      "   Epoch 60/100 | Train Loss: 0.4471 | Val Loss: 0.4747 | Val Corr: 0.4414 | Val RMSE: 0.8623\n",
      "Early stopping at epoch 62 (best epoch 47, best corr 0.4815)\n",
      "BEST epoch = 47 | BEST Corr = 0.4815 | RMSE@BEST = 0.8833\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7049 | Val Loss: 0.5907 | Val Corr: 0.1952 | Val RMSE: 0.9270\n",
      "   Epoch 10/100 | Train Loss: 0.6120 | Val Loss: 0.5185 | Val Corr: 0.4047 | Val RMSE: 0.8807\n",
      "   Epoch 20/100 | Train Loss: 0.5567 | Val Loss: 0.5095 | Val Corr: 0.4317 | Val RMSE: 0.8411\n",
      "Early stopping at epoch 29 (best epoch 14, best corr 0.4488)\n",
      "BEST epoch = 14 | BEST Corr = 0.4488 | RMSE@BEST = 0.8636\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.6823 | Val Loss: 0.6522 | Val Corr: 0.0091 | Val RMSE: 1.0667\n",
      "   Epoch 10/100 | Train Loss: 0.6233 | Val Loss: 0.6506 | Val Corr: 0.1957 | Val RMSE: 1.0634\n",
      "   Epoch 20/100 | Train Loss: 0.5223 | Val Loss: 0.5866 | Val Corr: 0.4054 | Val RMSE: 0.9955\n",
      "   Epoch 30/100 | Train Loss: 0.5133 | Val Loss: 0.5867 | Val Corr: 0.3933 | Val RMSE: 0.9877\n",
      "   Epoch 40/100 | Train Loss: 0.4918 | Val Loss: 0.5923 | Val Corr: 0.3981 | Val RMSE: 1.0003\n",
      "Early stopping at epoch 47 (best epoch 32, best corr 0.4453)\n",
      "BEST epoch = 32 | BEST Corr = 0.4453 | RMSE@BEST = 0.9566\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.6561 | Val Loss: 0.7657 | Val Corr: 0.1640 | Val RMSE: 1.1084\n",
      "   Epoch 10/100 | Train Loss: 0.5886 | Val Loss: 0.7706 | Val Corr: 0.0663 | Val RMSE: 1.1187\n",
      "   Epoch 20/100 | Train Loss: 0.5263 | Val Loss: 0.7178 | Val Corr: 0.3523 | Val RMSE: 1.0409\n",
      "   Epoch 30/100 | Train Loss: 0.4723 | Val Loss: 0.6387 | Val Corr: 0.4645 | Val RMSE: 1.0035\n",
      "   Epoch 40/100 | Train Loss: 0.4686 | Val Loss: 0.6636 | Val Corr: 0.3937 | Val RMSE: 1.0526\n",
      "   Epoch 50/100 | Train Loss: 0.4314 | Val Loss: 0.6575 | Val Corr: 0.4032 | Val RMSE: 1.0730\n",
      "Early stopping at epoch 56 (best epoch 41, best corr 0.4986)\n",
      "BEST epoch = 41 | BEST Corr = 0.4986 | RMSE@BEST = 0.9629\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7578 | Val Loss: 0.6268 | Val Corr: 0.1970 | Val RMSE: 0.9458\n",
      "   Epoch 10/100 | Train Loss: 0.6323 | Val Loss: 0.5205 | Val Corr: 0.3947 | Val RMSE: 0.9164\n",
      "   Epoch 20/100 | Train Loss: 0.5433 | Val Loss: 0.5485 | Val Corr: 0.4709 | Val RMSE: 0.8609\n",
      "   Epoch 30/100 | Train Loss: 0.5269 | Val Loss: 0.5270 | Val Corr: 0.4959 | Val RMSE: 0.8499\n",
      "   Epoch 40/100 | Train Loss: 0.4914 | Val Loss: 0.5390 | Val Corr: 0.5352 | Val RMSE: 0.8174\n",
      "   Epoch 50/100 | Train Loss: 0.4604 | Val Loss: 0.4957 | Val Corr: 0.5715 | Val RMSE: 0.7971\n",
      "   Epoch 60/100 | Train Loss: 0.4148 | Val Loss: 0.5156 | Val Corr: 0.5374 | Val RMSE: 0.8156\n",
      "Early stopping at epoch 65 (best epoch 50, best corr 0.5715)\n",
      "BEST epoch = 50 | BEST Corr = 0.5715 | RMSE@BEST = 0.7971\n",
      "----------------------------------------\n",
      "      Processing Trait: PH\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7665 | Val Loss: 0.6260 | Val Corr: 0.3985 | Val RMSE: 0.9037\n",
      "   Epoch 10/100 | Train Loss: 0.7004 | Val Loss: 0.5837 | Val Corr: 0.5445 | Val RMSE: 0.8288\n",
      "   Epoch 20/100 | Train Loss: 0.6517 | Val Loss: 0.5462 | Val Corr: 0.6261 | Val RMSE: 0.7447\n",
      "   Epoch 30/100 | Train Loss: 0.5815 | Val Loss: 0.5751 | Val Corr: 0.5858 | Val RMSE: 0.7525\n",
      "Early stopping at epoch 34 (best epoch 19, best corr 0.6449)\n",
      "BEST epoch = 19 | BEST Corr = 0.6449 | RMSE@BEST = 0.7593\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7803 | Val Loss: 0.5861 | Val Corr: 0.3647 | Val RMSE: 0.8678\n",
      "   Epoch 10/100 | Train Loss: 0.6739 | Val Loss: 0.6000 | Val Corr: 0.3929 | Val RMSE: 0.8256\n",
      "   Epoch 20/100 | Train Loss: 0.6132 | Val Loss: 0.5970 | Val Corr: 0.4045 | Val RMSE: 0.8138\n",
      "   Epoch 30/100 | Train Loss: 0.5122 | Val Loss: 0.6364 | Val Corr: 0.4119 | Val RMSE: 0.8640\n",
      "Early stopping at epoch 33 (best epoch 18, best corr 0.4339)\n",
      "BEST epoch = 18 | BEST Corr = 0.4339 | RMSE@BEST = 0.8015\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7610 | Val Loss: 0.7008 | Val Corr: 0.0119 | Val RMSE: 0.9990\n",
      "   Epoch 10/100 | Train Loss: 0.6773 | Val Loss: 0.6854 | Val Corr: 0.2021 | Val RMSE: 0.9770\n",
      "   Epoch 20/100 | Train Loss: 0.6015 | Val Loss: 0.6709 | Val Corr: 0.2708 | Val RMSE: 0.9576\n",
      "   Epoch 30/100 | Train Loss: 0.4968 | Val Loss: 0.6445 | Val Corr: 0.3500 | Val RMSE: 0.9330\n",
      "Early stopping at epoch 40 (best epoch 25, best corr 0.3878)\n",
      "BEST epoch = 25 | BEST Corr = 0.3878 | RMSE@BEST = 0.9189\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.6695 | Val Loss: 0.9057 | Val Corr: 0.1910 | Val RMSE: 1.2841\n",
      "   Epoch 10/100 | Train Loss: 0.6279 | Val Loss: 0.7990 | Val Corr: 0.2854 | Val RMSE: 1.2240\n",
      "   Epoch 20/100 | Train Loss: 0.5449 | Val Loss: 0.7745 | Val Corr: 0.3114 | Val RMSE: 1.2187\n",
      "   Epoch 30/100 | Train Loss: 0.4996 | Val Loss: 0.7217 | Val Corr: 0.3650 | Val RMSE: 1.1995\n",
      "   Epoch 40/100 | Train Loss: 0.4738 | Val Loss: 0.7084 | Val Corr: 0.4313 | Val RMSE: 1.1504\n",
      "   Epoch 50/100 | Train Loss: 0.4316 | Val Loss: 0.7113 | Val Corr: 0.4000 | Val RMSE: 1.1767\n",
      "Early stopping at epoch 54 (best epoch 39, best corr 0.4459)\n",
      "BEST epoch = 39 | BEST Corr = 0.4459 | RMSE@BEST = 1.1588\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7168 | Val Loss: 0.6092 | Val Corr: 0.3687 | Val RMSE: 0.9143\n",
      "   Epoch 10/100 | Train Loss: 0.6601 | Val Loss: 0.5939 | Val Corr: 0.4067 | Val RMSE: 0.8415\n",
      "   Epoch 20/100 | Train Loss: 0.5947 | Val Loss: 0.5939 | Val Corr: 0.4825 | Val RMSE: 0.7983\n",
      "   Epoch 30/100 | Train Loss: 0.5424 | Val Loss: 0.5559 | Val Corr: 0.5206 | Val RMSE: 0.7798\n",
      "   Epoch 40/100 | Train Loss: 0.5122 | Val Loss: 0.5431 | Val Corr: 0.5744 | Val RMSE: 0.7522\n",
      "   Epoch 50/100 | Train Loss: 0.4567 | Val Loss: 0.5563 | Val Corr: 0.5267 | Val RMSE: 0.8021\n",
      "   Epoch 60/100 | Train Loss: 0.4198 | Val Loss: 0.5747 | Val Corr: 0.5316 | Val RMSE: 0.8117\n",
      "Early stopping at epoch 66 (best epoch 51, best corr 0.5754)\n",
      "BEST epoch = 51 | BEST Corr = 0.5754 | RMSE@BEST = 0.7493\n",
      "----------------------------------------\n",
      "      Processing Trait: SNPP\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7707 | Val Loss: 0.6372 | Val Corr: 0.2535 | Val RMSE: 0.9024\n",
      "   Epoch 10/100 | Train Loss: 0.6875 | Val Loss: 0.6130 | Val Corr: 0.3534 | Val RMSE: 0.8583\n",
      "   Epoch 20/100 | Train Loss: 0.6452 | Val Loss: 0.6112 | Val Corr: 0.4054 | Val RMSE: 0.8614\n",
      "   Epoch 30/100 | Train Loss: 0.6208 | Val Loss: 0.6031 | Val Corr: 0.4498 | Val RMSE: 0.8080\n",
      "   Epoch 40/100 | Train Loss: 0.6125 | Val Loss: 0.5778 | Val Corr: 0.4528 | Val RMSE: 0.8043\n",
      "   Epoch 50/100 | Train Loss: 0.5675 | Val Loss: 0.5741 | Val Corr: 0.4645 | Val RMSE: 0.7918\n",
      "   Epoch 60/100 | Train Loss: 0.5612 | Val Loss: 0.5828 | Val Corr: 0.4276 | Val RMSE: 0.8035\n",
      "Early stopping at epoch 64 (best epoch 49, best corr 0.4696)\n",
      "BEST epoch = 49 | BEST Corr = 0.4696 | RMSE@BEST = 0.7918\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7719 | Val Loss: 0.6293 | Val Corr: 0.0886 | Val RMSE: 0.9648\n",
      "   Epoch 10/100 | Train Loss: 0.6543 | Val Loss: 0.6102 | Val Corr: 0.1866 | Val RMSE: 0.9465\n",
      "   Epoch 20/100 | Train Loss: 0.6419 | Val Loss: 0.5824 | Val Corr: 0.3515 | Val RMSE: 0.9066\n",
      "   Epoch 30/100 | Train Loss: 0.5938 | Val Loss: 0.5763 | Val Corr: 0.3897 | Val RMSE: 0.8888\n",
      "Early stopping at epoch 39 (best epoch 24, best corr 0.4563)\n",
      "BEST epoch = 24 | BEST Corr = 0.4563 | RMSE@BEST = 0.8626\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7331 | Val Loss: 0.7209 | Val Corr: 0.1423 | Val RMSE: 1.1179\n",
      "   Epoch 10/100 | Train Loss: 0.6503 | Val Loss: 0.6987 | Val Corr: 0.3907 | Val RMSE: 1.0725\n",
      "   Epoch 20/100 | Train Loss: 0.6186 | Val Loss: 0.7063 | Val Corr: 0.4700 | Val RMSE: 1.0324\n",
      "   Epoch 30/100 | Train Loss: 0.5697 | Val Loss: 0.6481 | Val Corr: 0.5655 | Val RMSE: 0.9413\n",
      "   Epoch 40/100 | Train Loss: 0.5633 | Val Loss: 0.7006 | Val Corr: 0.4993 | Val RMSE: 0.9789\n",
      "Early stopping at epoch 47 (best epoch 32, best corr 0.5837)\n",
      "BEST epoch = 32 | BEST Corr = 0.5837 | RMSE@BEST = 0.9256\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7048 | Val Loss: 0.7336 | Val Corr: 0.0401 | Val RMSE: 1.1017\n",
      "   Epoch 10/100 | Train Loss: 0.6574 | Val Loss: 0.7301 | Val Corr: 0.0631 | Val RMSE: 1.0619\n",
      "   Epoch 20/100 | Train Loss: 0.5789 | Val Loss: 0.7151 | Val Corr: 0.1505 | Val RMSE: 1.0779\n",
      "Early stopping at epoch 23 (best epoch 8, best corr 0.1689)\n",
      "BEST epoch = 8 | BEST Corr = 0.1689 | RMSE@BEST = 1.1155\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7432 | Val Loss: 0.5806 | Val Corr: 0.1854 | Val RMSE: 0.9536\n",
      "   Epoch 10/100 | Train Loss: 0.7259 | Val Loss: 0.5365 | Val Corr: 0.3206 | Val RMSE: 0.9096\n",
      "Early stopping at epoch 18 (best epoch 3, best corr 0.3361)\n",
      "BEST epoch = 3 | BEST Corr = 0.3361 | RMSE@BEST = 0.9550\n",
      "----------------------------------------\n",
      "   [Saved] rice_T5000_LD0.2.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.4 | Species: rice | Feature Count: T5000\n",
      "      Processing Trait: SW\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.8103 | Val Loss: 0.6808 | Val Corr: 0.4906 | Val RMSE: 0.8932\n",
      "   Epoch 10/100 | Train Loss: 0.7088 | Val Loss: 0.5670 | Val Corr: 0.5736 | Val RMSE: 0.7555\n",
      "   Epoch 20/100 | Train Loss: 0.6315 | Val Loss: 0.6072 | Val Corr: 0.6402 | Val RMSE: 0.7801\n",
      "   Epoch 30/100 | Train Loss: 0.5625 | Val Loss: 0.6182 | Val Corr: 0.6379 | Val RMSE: 0.7979\n",
      "Early stopping at epoch 40 (best epoch 25, best corr 0.6490)\n",
      "BEST epoch = 25 | BEST Corr = 0.6490 | RMSE@BEST = 0.7683\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7603 | Val Loss: 0.7058 | Val Corr: 0.3848 | Val RMSE: 1.0497\n",
      "   Epoch 10/100 | Train Loss: 0.6415 | Val Loss: 0.6793 | Val Corr: 0.5494 | Val RMSE: 0.9179\n",
      "   Epoch 20/100 | Train Loss: 0.5386 | Val Loss: 0.6302 | Val Corr: 0.5434 | Val RMSE: 0.8899\n",
      "Early stopping at epoch 24 (best epoch 9, best corr 0.5820)\n",
      "BEST epoch = 9 | BEST Corr = 0.5820 | RMSE@BEST = 0.8731\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7623 | Val Loss: 0.8020 | Val Corr: 0.4640 | Val RMSE: 1.1142\n",
      "   Epoch 10/100 | Train Loss: 0.6114 | Val Loss: 0.6176 | Val Corr: 0.6110 | Val RMSE: 0.8896\n",
      "   Epoch 20/100 | Train Loss: 0.5320 | Val Loss: 0.6121 | Val Corr: 0.6150 | Val RMSE: 0.8835\n",
      "   Epoch 30/100 | Train Loss: 0.4956 | Val Loss: 0.6319 | Val Corr: 0.6223 | Val RMSE: 0.9368\n",
      "   Epoch 40/100 | Train Loss: 0.4730 | Val Loss: 0.6730 | Val Corr: 0.5910 | Val RMSE: 0.9717\n",
      "Early stopping at epoch 44 (best epoch 29, best corr 0.6294)\n",
      "BEST epoch = 29 | BEST Corr = 0.6294 | RMSE@BEST = 0.9374\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7640 | Val Loss: 0.7164 | Val Corr: 0.2836 | Val RMSE: 0.9363\n",
      "   Epoch 10/100 | Train Loss: 0.6054 | Val Loss: 0.6011 | Val Corr: 0.4505 | Val RMSE: 0.8858\n",
      "   Epoch 20/100 | Train Loss: 0.5443 | Val Loss: 0.7767 | Val Corr: 0.4538 | Val RMSE: 1.1219\n",
      "Early stopping at epoch 28 (best epoch 13, best corr 0.4728)\n",
      "BEST epoch = 13 | BEST Corr = 0.4728 | RMSE@BEST = 0.8962\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7708 | Val Loss: 0.7006 | Val Corr: 0.3858 | Val RMSE: 0.9792\n",
      "   Epoch 10/100 | Train Loss: 0.6280 | Val Loss: 0.6804 | Val Corr: 0.4786 | Val RMSE: 0.9200\n",
      "   Epoch 20/100 | Train Loss: 0.5328 | Val Loss: 0.6945 | Val Corr: 0.5063 | Val RMSE: 0.9046\n",
      "   Epoch 30/100 | Train Loss: 0.4659 | Val Loss: 0.6500 | Val Corr: 0.5153 | Val RMSE: 0.8979\n",
      "Early stopping at epoch 40 (best epoch 25, best corr 0.5292)\n",
      "BEST epoch = 25 | BEST Corr = 0.5292 | RMSE@BEST = 0.8536\n",
      "----------------------------------------\n",
      "      Processing Trait: FLW\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7433 | Val Loss: 0.6938 | Val Corr: 0.3761 | Val RMSE: 1.0370\n",
      "   Epoch 10/100 | Train Loss: 0.6421 | Val Loss: 0.6815 | Val Corr: 0.4321 | Val RMSE: 0.9638\n",
      "   Epoch 20/100 | Train Loss: 0.6171 | Val Loss: 0.6337 | Val Corr: 0.4681 | Val RMSE: 0.9164\n",
      "   Epoch 30/100 | Train Loss: 0.5931 | Val Loss: 0.6229 | Val Corr: 0.4614 | Val RMSE: 0.9317\n",
      "   Epoch 40/100 | Train Loss: 0.5486 | Val Loss: 0.5982 | Val Corr: 0.4452 | Val RMSE: 0.9288\n",
      "Early stopping at epoch 42 (best epoch 27, best corr 0.4806)\n",
      "BEST epoch = 27 | BEST Corr = 0.4806 | RMSE@BEST = 0.9148\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7432 | Val Loss: 0.7245 | Val Corr: 0.2215 | Val RMSE: 1.1506\n",
      "   Epoch 10/100 | Train Loss: 0.6432 | Val Loss: 0.7441 | Val Corr: 0.3064 | Val RMSE: 1.0971\n",
      "   Epoch 20/100 | Train Loss: 0.5849 | Val Loss: 0.7703 | Val Corr: 0.3233 | Val RMSE: 1.1107\n",
      "   Epoch 30/100 | Train Loss: 0.5299 | Val Loss: 0.7273 | Val Corr: 0.3818 | Val RMSE: 1.0652\n",
      "   Epoch 40/100 | Train Loss: 0.5264 | Val Loss: 0.7210 | Val Corr: 0.4339 | Val RMSE: 1.0489\n",
      "Early stopping at epoch 47 (best epoch 32, best corr 0.4564)\n",
      "BEST epoch = 32 | BEST Corr = 0.4564 | RMSE@BEST = 1.0297\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7332 | Val Loss: 0.7234 | Val Corr: 0.4491 | Val RMSE: 1.0788\n",
      "   Epoch 10/100 | Train Loss: 0.6491 | Val Loss: 0.6572 | Val Corr: 0.3938 | Val RMSE: 0.9921\n",
      "Early stopping at epoch 16 (best epoch 1, best corr 0.4491)\n",
      "BEST epoch = 1 | BEST Corr = 0.4491 | RMSE@BEST = 1.0788\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7527 | Val Loss: 0.6476 | Val Corr: 0.2874 | Val RMSE: 0.8541\n",
      "   Epoch 10/100 | Train Loss: 0.7154 | Val Loss: 0.5972 | Val Corr: 0.2640 | Val RMSE: 0.8341\n",
      "   Epoch 20/100 | Train Loss: 0.5958 | Val Loss: 0.5890 | Val Corr: 0.3218 | Val RMSE: 0.8198\n",
      "   Epoch 30/100 | Train Loss: 0.6051 | Val Loss: 0.5869 | Val Corr: 0.3046 | Val RMSE: 0.8271\n",
      "   Epoch 40/100 | Train Loss: 0.5358 | Val Loss: 0.6254 | Val Corr: 0.3499 | Val RMSE: 0.8400\n",
      "   Epoch 50/100 | Train Loss: 0.5090 | Val Loss: 0.6229 | Val Corr: 0.3382 | Val RMSE: 0.8424\n",
      "   Epoch 60/100 | Train Loss: 0.4961 | Val Loss: 0.6170 | Val Corr: 0.3638 | Val RMSE: 0.8464\n",
      "   Epoch 70/100 | Train Loss: 0.4540 | Val Loss: 0.5982 | Val Corr: 0.3844 | Val RMSE: 0.8136\n",
      "Early stopping at epoch 79 (best epoch 64, best corr 0.3996)\n",
      "BEST epoch = 64 | BEST Corr = 0.3996 | RMSE@BEST = 0.8262\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7397 | Val Loss: 0.5765 | Val Corr: 0.3554 | Val RMSE: 0.8288\n",
      "   Epoch 10/100 | Train Loss: 0.6398 | Val Loss: 0.6105 | Val Corr: 0.3655 | Val RMSE: 0.8214\n",
      "Early stopping at epoch 19 (best epoch 4, best corr 0.3795)\n",
      "BEST epoch = 4 | BEST Corr = 0.3795 | RMSE@BEST = 0.8281\n",
      "----------------------------------------\n",
      "      Processing Trait: AC\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7146 | Val Loss: 0.6004 | Val Corr: 0.1187 | Val RMSE: 0.9417\n",
      "   Epoch 10/100 | Train Loss: 0.6863 | Val Loss: 0.5402 | Val Corr: 0.2303 | Val RMSE: 0.9151\n",
      "   Epoch 20/100 | Train Loss: 0.6198 | Val Loss: 0.5050 | Val Corr: 0.3451 | Val RMSE: 0.9088\n",
      "   Epoch 30/100 | Train Loss: 0.5884 | Val Loss: 0.4807 | Val Corr: 0.3623 | Val RMSE: 0.8737\n",
      "   Epoch 40/100 | Train Loss: 0.5039 | Val Loss: 0.4651 | Val Corr: 0.4299 | Val RMSE: 0.8556\n",
      "   Epoch 50/100 | Train Loss: 0.4858 | Val Loss: 0.4858 | Val Corr: 0.3507 | Val RMSE: 0.9014\n",
      "Early stopping at epoch 51 (best epoch 36, best corr 0.4493)\n",
      "BEST epoch = 36 | BEST Corr = 0.4493 | RMSE@BEST = 0.8507\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7175 | Val Loss: 0.5903 | Val Corr: 0.1718 | Val RMSE: 0.9272\n",
      "   Epoch 10/100 | Train Loss: 0.6203 | Val Loss: 0.5427 | Val Corr: 0.2706 | Val RMSE: 0.9206\n",
      "   Epoch 20/100 | Train Loss: 0.5338 | Val Loss: 0.5239 | Val Corr: 0.3954 | Val RMSE: 0.8767\n",
      "   Epoch 30/100 | Train Loss: 0.4893 | Val Loss: 0.5053 | Val Corr: 0.4170 | Val RMSE: 0.8488\n",
      "   Epoch 40/100 | Train Loss: 0.4452 | Val Loss: 0.5431 | Val Corr: 0.3708 | Val RMSE: 0.8825\n",
      "   Epoch 50/100 | Train Loss: 0.4260 | Val Loss: 0.5228 | Val Corr: 0.4246 | Val RMSE: 0.8574\n",
      "Early stopping at epoch 51 (best epoch 36, best corr 0.5001)\n",
      "BEST epoch = 36 | BEST Corr = 0.5001 | RMSE@BEST = 0.8061\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.6829 | Val Loss: 0.6513 | Val Corr: 0.0126 | Val RMSE: 1.0695\n",
      "   Epoch 10/100 | Train Loss: 0.5973 | Val Loss: 0.6171 | Val Corr: 0.3300 | Val RMSE: 1.0325\n",
      "   Epoch 20/100 | Train Loss: 0.5168 | Val Loss: 0.5907 | Val Corr: 0.3961 | Val RMSE: 1.0242\n",
      "   Epoch 30/100 | Train Loss: 0.4912 | Val Loss: 0.5946 | Val Corr: 0.3885 | Val RMSE: 1.0614\n",
      "Early stopping at epoch 32 (best epoch 17, best corr 0.4251)\n",
      "BEST epoch = 17 | BEST Corr = 0.4251 | RMSE@BEST = 0.9735\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.6472 | Val Loss: 0.7679 | Val Corr: -0.0128 | Val RMSE: 1.1086\n",
      "   Epoch 10/100 | Train Loss: 0.5796 | Val Loss: 0.7262 | Val Corr: 0.2168 | Val RMSE: 1.0932\n",
      "   Epoch 20/100 | Train Loss: 0.5211 | Val Loss: 0.7272 | Val Corr: 0.2058 | Val RMSE: 1.1413\n",
      "   Epoch 30/100 | Train Loss: 0.4912 | Val Loss: 0.6378 | Val Corr: 0.4459 | Val RMSE: 1.0187\n",
      "   Epoch 40/100 | Train Loss: 0.4551 | Val Loss: 0.6600 | Val Corr: 0.3662 | Val RMSE: 1.0624\n",
      "   Epoch 50/100 | Train Loss: 0.4216 | Val Loss: 0.6519 | Val Corr: 0.4056 | Val RMSE: 1.0519\n",
      "Early stopping at epoch 53 (best epoch 38, best corr 0.4736)\n",
      "BEST epoch = 38 | BEST Corr = 0.4736 | RMSE@BEST = 0.9892\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7473 | Val Loss: 0.6423 | Val Corr: 0.1902 | Val RMSE: 0.9462\n",
      "   Epoch 10/100 | Train Loss: 0.6368 | Val Loss: 0.5732 | Val Corr: 0.3362 | Val RMSE: 0.9697\n",
      "   Epoch 20/100 | Train Loss: 0.5527 | Val Loss: 0.5085 | Val Corr: 0.4599 | Val RMSE: 0.8751\n",
      "   Epoch 30/100 | Train Loss: 0.5188 | Val Loss: 0.5200 | Val Corr: 0.4794 | Val RMSE: 0.8662\n",
      "   Epoch 40/100 | Train Loss: 0.4898 | Val Loss: 0.5262 | Val Corr: 0.4797 | Val RMSE: 0.8445\n",
      "Early stopping at epoch 43 (best epoch 28, best corr 0.5394)\n",
      "BEST epoch = 28 | BEST Corr = 0.5394 | RMSE@BEST = 0.8401\n",
      "----------------------------------------\n",
      "      Processing Trait: PH\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7779 | Val Loss: 0.6336 | Val Corr: 0.4426 | Val RMSE: 0.9054\n",
      "   Epoch 10/100 | Train Loss: 0.7007 | Val Loss: 0.5914 | Val Corr: 0.4903 | Val RMSE: 0.8277\n",
      "   Epoch 20/100 | Train Loss: 0.6560 | Val Loss: 0.5655 | Val Corr: 0.5670 | Val RMSE: 0.7623\n",
      "   Epoch 30/100 | Train Loss: 0.5653 | Val Loss: 0.5282 | Val Corr: 0.5978 | Val RMSE: 0.7280\n",
      "   Epoch 40/100 | Train Loss: 0.5660 | Val Loss: 0.5090 | Val Corr: 0.6401 | Val RMSE: 0.7015\n",
      "   Epoch 50/100 | Train Loss: 0.5297 | Val Loss: 0.5365 | Val Corr: 0.6678 | Val RMSE: 0.7186\n",
      "Early stopping at epoch 60 (best epoch 45, best corr 0.7033)\n",
      "BEST epoch = 45 | BEST Corr = 0.7033 | RMSE@BEST = 0.7384\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7694 | Val Loss: 0.5880 | Val Corr: 0.3734 | Val RMSE: 0.8680\n",
      "   Epoch 10/100 | Train Loss: 0.6499 | Val Loss: 0.6324 | Val Corr: 0.4181 | Val RMSE: 0.8302\n",
      "   Epoch 20/100 | Train Loss: 0.5918 | Val Loss: 0.5603 | Val Corr: 0.4421 | Val RMSE: 0.7977\n",
      "Early stopping at epoch 29 (best epoch 14, best corr 0.4860)\n",
      "BEST epoch = 14 | BEST Corr = 0.4860 | RMSE@BEST = 0.8220\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7721 | Val Loss: 0.6827 | Val Corr: 0.0066 | Val RMSE: 0.9946\n",
      "   Epoch 10/100 | Train Loss: 0.6765 | Val Loss: 0.7090 | Val Corr: 0.1883 | Val RMSE: 0.9858\n",
      "   Epoch 20/100 | Train Loss: 0.5893 | Val Loss: 0.6614 | Val Corr: 0.3822 | Val RMSE: 0.9239\n",
      "   Epoch 30/100 | Train Loss: 0.5218 | Val Loss: 0.6500 | Val Corr: 0.3721 | Val RMSE: 0.9330\n",
      "Early stopping at epoch 34 (best epoch 19, best corr 0.4179)\n",
      "BEST epoch = 19 | BEST Corr = 0.4179 | RMSE@BEST = 0.9179\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.6715 | Val Loss: 0.9001 | Val Corr: 0.1901 | Val RMSE: 1.2812\n",
      "   Epoch 10/100 | Train Loss: 0.6190 | Val Loss: 0.8074 | Val Corr: 0.2939 | Val RMSE: 1.2280\n",
      "   Epoch 20/100 | Train Loss: 0.5411 | Val Loss: 0.7747 | Val Corr: 0.3108 | Val RMSE: 1.2105\n",
      "   Epoch 30/100 | Train Loss: 0.5332 | Val Loss: 0.7424 | Val Corr: 0.3428 | Val RMSE: 1.2017\n",
      "   Epoch 40/100 | Train Loss: 0.4581 | Val Loss: 0.7346 | Val Corr: 0.3808 | Val RMSE: 1.1793\n",
      "   Epoch 50/100 | Train Loss: 0.4316 | Val Loss: 0.7192 | Val Corr: 0.4293 | Val RMSE: 1.1563\n",
      "   Epoch 60/100 | Train Loss: 0.3808 | Val Loss: 0.7143 | Val Corr: 0.4519 | Val RMSE: 1.1511\n",
      "   Epoch 70/100 | Train Loss: 0.3942 | Val Loss: 0.7102 | Val Corr: 0.4509 | Val RMSE: 1.1456\n",
      "Early stopping at epoch 73 (best epoch 58, best corr 0.4664)\n",
      "BEST epoch = 58 | BEST Corr = 0.4664 | RMSE@BEST = 1.1311\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7363 | Val Loss: 0.6299 | Val Corr: 0.3742 | Val RMSE: 0.9236\n",
      "   Epoch 10/100 | Train Loss: 0.6544 | Val Loss: 0.6086 | Val Corr: 0.4407 | Val RMSE: 0.8664\n",
      "   Epoch 20/100 | Train Loss: 0.6131 | Val Loss: 0.5949 | Val Corr: 0.5157 | Val RMSE: 0.8030\n",
      "   Epoch 30/100 | Train Loss: 0.5140 | Val Loss: 0.5513 | Val Corr: 0.5895 | Val RMSE: 0.7558\n",
      "   Epoch 40/100 | Train Loss: 0.4646 | Val Loss: 0.5568 | Val Corr: 0.5900 | Val RMSE: 0.7366\n",
      "Early stopping at epoch 43 (best epoch 28, best corr 0.5978)\n",
      "BEST epoch = 28 | BEST Corr = 0.5978 | RMSE@BEST = 0.7384\n",
      "----------------------------------------\n",
      "      Processing Trait: SNPP\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7860 | Val Loss: 0.6606 | Val Corr: 0.2867 | Val RMSE: 0.9145\n",
      "   Epoch 10/100 | Train Loss: 0.6900 | Val Loss: 0.6258 | Val Corr: 0.3070 | Val RMSE: 0.8501\n",
      "   Epoch 20/100 | Train Loss: 0.6404 | Val Loss: 0.5939 | Val Corr: 0.4289 | Val RMSE: 0.8492\n",
      "   Epoch 30/100 | Train Loss: 0.6258 | Val Loss: 0.5831 | Val Corr: 0.3968 | Val RMSE: 0.8282\n",
      "Early stopping at epoch 35 (best epoch 20, best corr 0.4289)\n",
      "BEST epoch = 20 | BEST Corr = 0.4289 | RMSE@BEST = 0.8492\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7469 | Val Loss: 0.6230 | Val Corr: 0.0354 | Val RMSE: 0.9632\n",
      "   Epoch 10/100 | Train Loss: 0.6735 | Val Loss: 0.6197 | Val Corr: 0.1936 | Val RMSE: 0.9491\n",
      "   Epoch 20/100 | Train Loss: 0.6433 | Val Loss: 0.5961 | Val Corr: 0.3093 | Val RMSE: 0.9150\n",
      "   Epoch 30/100 | Train Loss: 0.5638 | Val Loss: 0.5782 | Val Corr: 0.4008 | Val RMSE: 0.8803\n",
      "   Epoch 40/100 | Train Loss: 0.5500 | Val Loss: 0.5965 | Val Corr: 0.3741 | Val RMSE: 0.9010\n",
      "   Epoch 50/100 | Train Loss: 0.5068 | Val Loss: 0.6117 | Val Corr: 0.4044 | Val RMSE: 0.8919\n",
      "Early stopping at epoch 53 (best epoch 38, best corr 0.4485)\n",
      "BEST epoch = 38 | BEST Corr = 0.4485 | RMSE@BEST = 0.9169\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7232 | Val Loss: 0.7229 | Val Corr: 0.1788 | Val RMSE: 1.1183\n",
      "   Epoch 10/100 | Train Loss: 0.6668 | Val Loss: 0.6911 | Val Corr: 0.3105 | Val RMSE: 1.0800\n",
      "   Epoch 20/100 | Train Loss: 0.6343 | Val Loss: 0.7013 | Val Corr: 0.4134 | Val RMSE: 1.0485\n",
      "   Epoch 30/100 | Train Loss: 0.5768 | Val Loss: 0.6505 | Val Corr: 0.5418 | Val RMSE: 0.9629\n",
      "   Epoch 40/100 | Train Loss: 0.5617 | Val Loss: 0.6957 | Val Corr: 0.4132 | Val RMSE: 1.0222\n",
      "Early stopping at epoch 42 (best epoch 27, best corr 0.5781)\n",
      "BEST epoch = 27 | BEST Corr = 0.5781 | RMSE@BEST = 0.9900\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7169 | Val Loss: 0.7189 | Val Corr: -0.0136 | Val RMSE: 1.0926\n",
      "   Epoch 10/100 | Train Loss: 0.6442 | Val Loss: 0.8101 | Val Corr: 0.0133 | Val RMSE: 1.1240\n",
      "   Epoch 20/100 | Train Loss: 0.5777 | Val Loss: 0.7628 | Val Corr: 0.1451 | Val RMSE: 1.1408\n",
      "   Epoch 30/100 | Train Loss: 0.5498 | Val Loss: 0.7664 | Val Corr: 0.1409 | Val RMSE: 1.1529\n",
      "Early stopping at epoch 37 (best epoch 22, best corr 0.1749)\n",
      "BEST epoch = 22 | BEST Corr = 0.1749 | RMSE@BEST = 1.1563\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7518 | Val Loss: 0.5676 | Val Corr: 0.2986 | Val RMSE: 0.9478\n",
      "   Epoch 10/100 | Train Loss: 0.6871 | Val Loss: 0.5199 | Val Corr: 0.3094 | Val RMSE: 0.9029\n",
      "   Epoch 20/100 | Train Loss: 0.5969 | Val Loss: 0.5756 | Val Corr: 0.2911 | Val RMSE: 0.8961\n",
      "   Epoch 30/100 | Train Loss: 0.5695 | Val Loss: 0.5820 | Val Corr: 0.3420 | Val RMSE: 0.9065\n",
      "Early stopping at epoch 33 (best epoch 18, best corr 0.3742)\n",
      "BEST epoch = 18 | BEST Corr = 0.3742 | RMSE@BEST = 0.8846\n",
      "----------------------------------------\n",
      "   [Saved] rice_T5000_LD0.4.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.6 | Species: rice | Feature Count: T5000\n",
      "      Processing Trait: SW\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.8147 | Val Loss: 0.6781 | Val Corr: 0.4868 | Val RMSE: 0.8936\n",
      "   Epoch 10/100 | Train Loss: 0.6932 | Val Loss: 0.5636 | Val Corr: 0.5832 | Val RMSE: 0.7302\n",
      "   Epoch 20/100 | Train Loss: 0.6337 | Val Loss: 0.5926 | Val Corr: 0.6002 | Val RMSE: 0.7689\n",
      "   Epoch 30/100 | Train Loss: 0.5534 | Val Loss: 0.5255 | Val Corr: 0.6163 | Val RMSE: 0.7235\n",
      "Early stopping at epoch 34 (best epoch 19, best corr 0.6471)\n",
      "BEST epoch = 19 | BEST Corr = 0.6471 | RMSE@BEST = 0.6866\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7686 | Val Loss: 0.7077 | Val Corr: 0.3566 | Val RMSE: 1.0498\n",
      "   Epoch 10/100 | Train Loss: 0.6301 | Val Loss: 0.6429 | Val Corr: 0.5680 | Val RMSE: 0.9023\n",
      "   Epoch 20/100 | Train Loss: 0.5204 | Val Loss: 0.6224 | Val Corr: 0.5991 | Val RMSE: 0.8658\n",
      "   Epoch 30/100 | Train Loss: 0.5006 | Val Loss: 0.6147 | Val Corr: 0.5975 | Val RMSE: 0.8656\n",
      "Early stopping at epoch 31 (best epoch 16, best corr 0.6071)\n",
      "BEST epoch = 16 | BEST Corr = 0.6071 | RMSE@BEST = 0.8511\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7800 | Val Loss: 0.8021 | Val Corr: 0.4758 | Val RMSE: 1.1139\n",
      "   Epoch 10/100 | Train Loss: 0.6337 | Val Loss: 0.6228 | Val Corr: 0.6298 | Val RMSE: 0.8875\n",
      "   Epoch 20/100 | Train Loss: 0.5272 | Val Loss: 0.6066 | Val Corr: 0.6308 | Val RMSE: 0.8874\n",
      "   Epoch 30/100 | Train Loss: 0.4793 | Val Loss: 0.5824 | Val Corr: 0.6668 | Val RMSE: 0.8542\n",
      "   Epoch 40/100 | Train Loss: 0.4894 | Val Loss: 0.6020 | Val Corr: 0.6614 | Val RMSE: 0.8711\n",
      "   Epoch 50/100 | Train Loss: 0.4240 | Val Loss: 0.6264 | Val Corr: 0.6644 | Val RMSE: 0.8779\n",
      "Early stopping at epoch 57 (best epoch 42, best corr 0.6717)\n",
      "BEST epoch = 42 | BEST Corr = 0.6717 | RMSE@BEST = 0.8690\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7661 | Val Loss: 0.7156 | Val Corr: 0.3013 | Val RMSE: 0.9388\n",
      "   Epoch 10/100 | Train Loss: 0.6229 | Val Loss: 0.6394 | Val Corr: 0.4241 | Val RMSE: 0.8927\n",
      "   Epoch 20/100 | Train Loss: 0.5163 | Val Loss: 0.6773 | Val Corr: 0.4473 | Val RMSE: 0.9679\n",
      "   Epoch 30/100 | Train Loss: 0.4776 | Val Loss: 0.6681 | Val Corr: 0.4273 | Val RMSE: 0.9604\n",
      "Early stopping at epoch 36 (best epoch 21, best corr 0.4696)\n",
      "BEST epoch = 21 | BEST Corr = 0.4696 | RMSE@BEST = 0.9740\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7578 | Val Loss: 0.6993 | Val Corr: 0.3584 | Val RMSE: 0.9787\n",
      "   Epoch 10/100 | Train Loss: 0.6231 | Val Loss: 0.6770 | Val Corr: 0.4995 | Val RMSE: 0.8994\n",
      "   Epoch 20/100 | Train Loss: 0.5498 | Val Loss: 0.6764 | Val Corr: 0.4833 | Val RMSE: 0.9133\n",
      "Early stopping at epoch 26 (best epoch 11, best corr 0.5121)\n",
      "BEST epoch = 11 | BEST Corr = 0.5121 | RMSE@BEST = 0.8605\n",
      "----------------------------------------\n",
      "      Processing Trait: FLW\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7469 | Val Loss: 0.6895 | Val Corr: 0.3464 | Val RMSE: 1.0362\n",
      "   Epoch 10/100 | Train Loss: 0.6398 | Val Loss: 0.6560 | Val Corr: 0.4534 | Val RMSE: 0.9552\n",
      "   Epoch 20/100 | Train Loss: 0.6009 | Val Loss: 0.6213 | Val Corr: 0.4905 | Val RMSE: 0.9242\n",
      "   Epoch 30/100 | Train Loss: 0.5667 | Val Loss: 0.6094 | Val Corr: 0.4819 | Val RMSE: 0.9199\n",
      "Early stopping at epoch 33 (best epoch 18, best corr 0.5096)\n",
      "BEST epoch = 18 | BEST Corr = 0.5096 | RMSE@BEST = 0.9144\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7458 | Val Loss: 0.7182 | Val Corr: 0.2975 | Val RMSE: 1.1516\n",
      "   Epoch 10/100 | Train Loss: 0.6328 | Val Loss: 0.8060 | Val Corr: 0.3374 | Val RMSE: 1.1345\n",
      "   Epoch 20/100 | Train Loss: 0.5506 | Val Loss: 0.7379 | Val Corr: 0.3861 | Val RMSE: 1.0822\n",
      "   Epoch 30/100 | Train Loss: 0.5122 | Val Loss: 0.7288 | Val Corr: 0.3971 | Val RMSE: 1.0642\n",
      "   Epoch 40/100 | Train Loss: 0.4913 | Val Loss: 0.7131 | Val Corr: 0.4773 | Val RMSE: 1.0346\n",
      "   Epoch 50/100 | Train Loss: 0.4361 | Val Loss: 0.7383 | Val Corr: 0.4179 | Val RMSE: 1.0542\n",
      "   Epoch 60/100 | Train Loss: 0.4280 | Val Loss: 0.7151 | Val Corr: 0.4643 | Val RMSE: 1.0257\n",
      "Early stopping at epoch 61 (best epoch 46, best corr 0.4810)\n",
      "BEST epoch = 46 | BEST Corr = 0.4810 | RMSE@BEST = 1.0143\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7354 | Val Loss: 0.7435 | Val Corr: 0.3599 | Val RMSE: 1.0752\n",
      "   Epoch 10/100 | Train Loss: 0.6446 | Val Loss: 0.6692 | Val Corr: 0.4199 | Val RMSE: 1.0030\n",
      "   Epoch 20/100 | Train Loss: 0.5756 | Val Loss: 0.6905 | Val Corr: 0.3991 | Val RMSE: 1.0543\n",
      "   Epoch 30/100 | Train Loss: 0.5529 | Val Loss: 0.6738 | Val Corr: 0.4881 | Val RMSE: 1.0071\n",
      "   Epoch 40/100 | Train Loss: 0.5106 | Val Loss: 0.6764 | Val Corr: 0.4055 | Val RMSE: 1.0102\n",
      "Early stopping at epoch 49 (best epoch 34, best corr 0.4970)\n",
      "BEST epoch = 34 | BEST Corr = 0.4970 | RMSE@BEST = 0.9425\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7592 | Val Loss: 0.6539 | Val Corr: 0.3196 | Val RMSE: 0.8566\n",
      "   Epoch 10/100 | Train Loss: 0.7024 | Val Loss: 0.5829 | Val Corr: 0.2885 | Val RMSE: 0.8164\n",
      "   Epoch 20/100 | Train Loss: 0.6061 | Val Loss: 0.5863 | Val Corr: 0.3308 | Val RMSE: 0.8391\n",
      "   Epoch 30/100 | Train Loss: 0.5622 | Val Loss: 0.5918 | Val Corr: 0.3223 | Val RMSE: 0.8114\n",
      "   Epoch 40/100 | Train Loss: 0.5251 | Val Loss: 0.6194 | Val Corr: 0.3881 | Val RMSE: 0.8342\n",
      "Early stopping at epoch 47 (best epoch 32, best corr 0.3911)\n",
      "BEST epoch = 32 | BEST Corr = 0.3911 | RMSE@BEST = 0.8218\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7430 | Val Loss: 0.5917 | Val Corr: 0.3252 | Val RMSE: 0.8336\n",
      "   Epoch 10/100 | Train Loss: 0.6179 | Val Loss: 0.5797 | Val Corr: 0.3969 | Val RMSE: 0.7723\n",
      "   Epoch 20/100 | Train Loss: 0.5730 | Val Loss: 0.6060 | Val Corr: 0.3814 | Val RMSE: 0.8109\n",
      "Early stopping at epoch 23 (best epoch 8, best corr 0.4036)\n",
      "BEST epoch = 8 | BEST Corr = 0.4036 | RMSE@BEST = 0.7643\n",
      "----------------------------------------\n",
      "      Processing Trait: AC\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7186 | Val Loss: 0.6030 | Val Corr: 0.1261 | Val RMSE: 0.9422\n",
      "   Epoch 10/100 | Train Loss: 0.7104 | Val Loss: 0.5227 | Val Corr: 0.2579 | Val RMSE: 0.9355\n",
      "   Epoch 20/100 | Train Loss: 0.6323 | Val Loss: 0.5073 | Val Corr: 0.3271 | Val RMSE: 0.8879\n",
      "   Epoch 30/100 | Train Loss: 0.5974 | Val Loss: 0.5416 | Val Corr: 0.2896 | Val RMSE: 0.9036\n",
      "   Epoch 40/100 | Train Loss: 0.5069 | Val Loss: 0.5152 | Val Corr: 0.3186 | Val RMSE: 0.8991\n",
      "   Epoch 50/100 | Train Loss: 0.4708 | Val Loss: 0.5278 | Val Corr: 0.2791 | Val RMSE: 0.9202\n",
      "Early stopping at epoch 53 (best epoch 38, best corr 0.3644)\n",
      "BEST epoch = 38 | BEST Corr = 0.3644 | RMSE@BEST = 0.8835\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7179 | Val Loss: 0.5913 | Val Corr: 0.2355 | Val RMSE: 0.9272\n",
      "   Epoch 10/100 | Train Loss: 0.6010 | Val Loss: 0.5435 | Val Corr: 0.4418 | Val RMSE: 0.8686\n",
      "   Epoch 20/100 | Train Loss: 0.5411 | Val Loss: 0.5483 | Val Corr: 0.3362 | Val RMSE: 0.8960\n",
      "Early stopping at epoch 25 (best epoch 10, best corr 0.4418)\n",
      "BEST epoch = 10 | BEST Corr = 0.4418 | RMSE@BEST = 0.8686\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.6791 | Val Loss: 0.6517 | Val Corr: -0.0044 | Val RMSE: 1.0681\n",
      "   Epoch 10/100 | Train Loss: 0.5917 | Val Loss: 0.5780 | Val Corr: 0.4057 | Val RMSE: 1.0180\n",
      "   Epoch 20/100 | Train Loss: 0.5211 | Val Loss: 0.5725 | Val Corr: 0.4086 | Val RMSE: 0.9937\n",
      "   Epoch 30/100 | Train Loss: 0.4842 | Val Loss: 0.5744 | Val Corr: 0.3871 | Val RMSE: 1.0158\n",
      "Early stopping at epoch 36 (best epoch 21, best corr 0.4449)\n",
      "BEST epoch = 21 | BEST Corr = 0.4449 | RMSE@BEST = 0.9623\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.6621 | Val Loss: 0.7666 | Val Corr: 0.0024 | Val RMSE: 1.1090\n",
      "   Epoch 10/100 | Train Loss: 0.5722 | Val Loss: 0.7123 | Val Corr: 0.2243 | Val RMSE: 1.1009\n",
      "   Epoch 20/100 | Train Loss: 0.5358 | Val Loss: 0.7052 | Val Corr: 0.3100 | Val RMSE: 1.0611\n",
      "   Epoch 30/100 | Train Loss: 0.4777 | Val Loss: 0.6715 | Val Corr: 0.4259 | Val RMSE: 1.0068\n",
      "   Epoch 40/100 | Train Loss: 0.4798 | Val Loss: 0.6646 | Val Corr: 0.3463 | Val RMSE: 1.0822\n",
      "Early stopping at epoch 50 (best epoch 35, best corr 0.4777)\n",
      "BEST epoch = 35 | BEST Corr = 0.4777 | RMSE@BEST = 0.9840\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7715 | Val Loss: 0.6389 | Val Corr: 0.1952 | Val RMSE: 0.9459\n",
      "   Epoch 10/100 | Train Loss: 0.6241 | Val Loss: 0.5640 | Val Corr: 0.3328 | Val RMSE: 0.9031\n",
      "   Epoch 20/100 | Train Loss: 0.5280 | Val Loss: 0.5551 | Val Corr: 0.4691 | Val RMSE: 0.8490\n",
      "   Epoch 30/100 | Train Loss: 0.4912 | Val Loss: 0.5336 | Val Corr: 0.4854 | Val RMSE: 0.8472\n",
      "   Epoch 40/100 | Train Loss: 0.4688 | Val Loss: 0.5238 | Val Corr: 0.5332 | Val RMSE: 0.8100\n",
      "   Epoch 50/100 | Train Loss: 0.4386 | Val Loss: 0.5154 | Val Corr: 0.5136 | Val RMSE: 0.8178\n",
      "   Epoch 60/100 | Train Loss: 0.3772 | Val Loss: 0.5380 | Val Corr: 0.5295 | Val RMSE: 0.8067\n",
      "Early stopping at epoch 67 (best epoch 52, best corr 0.5495)\n",
      "BEST epoch = 52 | BEST Corr = 0.5495 | RMSE@BEST = 0.7911\n",
      "----------------------------------------\n",
      "      Processing Trait: PH\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7780 | Val Loss: 0.6239 | Val Corr: 0.4426 | Val RMSE: 0.9035\n",
      "   Epoch 10/100 | Train Loss: 0.6884 | Val Loss: 0.6004 | Val Corr: 0.5284 | Val RMSE: 0.8017\n",
      "   Epoch 20/100 | Train Loss: 0.6505 | Val Loss: 0.5424 | Val Corr: 0.5983 | Val RMSE: 0.7484\n",
      "Early stopping at epoch 30 (best epoch 15, best corr 0.6641)\n",
      "BEST epoch = 15 | BEST Corr = 0.6641 | RMSE@BEST = 0.8352\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7516 | Val Loss: 0.6171 | Val Corr: 0.3611 | Val RMSE: 0.8780\n",
      "   Epoch 10/100 | Train Loss: 0.6634 | Val Loss: 0.5428 | Val Corr: 0.4427 | Val RMSE: 0.7836\n",
      "   Epoch 20/100 | Train Loss: 0.6193 | Val Loss: 0.5565 | Val Corr: 0.4408 | Val RMSE: 0.7913\n",
      "Early stopping at epoch 28 (best epoch 13, best corr 0.4513)\n",
      "BEST epoch = 13 | BEST Corr = 0.4513 | RMSE@BEST = 0.7973\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7772 | Val Loss: 0.6750 | Val Corr: 0.0211 | Val RMSE: 0.9936\n",
      "   Epoch 10/100 | Train Loss: 0.6796 | Val Loss: 0.6806 | Val Corr: 0.2029 | Val RMSE: 0.9728\n",
      "   Epoch 20/100 | Train Loss: 0.5886 | Val Loss: 0.6718 | Val Corr: 0.2789 | Val RMSE: 0.9602\n",
      "   Epoch 30/100 | Train Loss: 0.5255 | Val Loss: 0.6577 | Val Corr: 0.3882 | Val RMSE: 0.9361\n",
      "   Epoch 40/100 | Train Loss: 0.5095 | Val Loss: 0.6868 | Val Corr: 0.3715 | Val RMSE: 0.9784\n",
      "Early stopping at epoch 42 (best epoch 27, best corr 0.4558)\n",
      "BEST epoch = 27 | BEST Corr = 0.4558 | RMSE@BEST = 0.9197\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.6766 | Val Loss: 0.9061 | Val Corr: 0.2472 | Val RMSE: 1.2840\n",
      "   Epoch 10/100 | Train Loss: 0.6442 | Val Loss: 0.8165 | Val Corr: 0.3177 | Val RMSE: 1.2196\n",
      "   Epoch 20/100 | Train Loss: 0.5268 | Val Loss: 0.7789 | Val Corr: 0.3298 | Val RMSE: 1.2237\n",
      "   Epoch 30/100 | Train Loss: 0.4978 | Val Loss: 0.7397 | Val Corr: 0.3615 | Val RMSE: 1.1854\n",
      "   Epoch 40/100 | Train Loss: 0.4802 | Val Loss: 0.7611 | Val Corr: 0.4004 | Val RMSE: 1.1855\n",
      "   Epoch 50/100 | Train Loss: 0.4413 | Val Loss: 0.7306 | Val Corr: 0.4329 | Val RMSE: 1.1586\n",
      "   Epoch 60/100 | Train Loss: 0.3843 | Val Loss: 0.7249 | Val Corr: 0.4549 | Val RMSE: 1.1511\n",
      "   Epoch 70/100 | Train Loss: 0.3736 | Val Loss: 0.6987 | Val Corr: 0.4496 | Val RMSE: 1.1422\n",
      "Early stopping at epoch 73 (best epoch 58, best corr 0.4624)\n",
      "BEST epoch = 58 | BEST Corr = 0.4624 | RMSE@BEST = 1.1287\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7239 | Val Loss: 0.6191 | Val Corr: 0.3679 | Val RMSE: 0.9184\n",
      "   Epoch 10/100 | Train Loss: 0.6579 | Val Loss: 0.5958 | Val Corr: 0.4544 | Val RMSE: 0.8416\n",
      "   Epoch 20/100 | Train Loss: 0.5921 | Val Loss: 0.5944 | Val Corr: 0.5074 | Val RMSE: 0.8244\n",
      "   Epoch 30/100 | Train Loss: 0.5186 | Val Loss: 0.5853 | Val Corr: 0.5297 | Val RMSE: 0.7698\n",
      "   Epoch 40/100 | Train Loss: 0.4842 | Val Loss: 0.6289 | Val Corr: 0.4699 | Val RMSE: 0.8545\n",
      "Early stopping at epoch 43 (best epoch 28, best corr 0.5627)\n",
      "BEST epoch = 28 | BEST Corr = 0.5627 | RMSE@BEST = 0.7633\n",
      "----------------------------------------\n",
      "      Processing Trait: SNPP\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7751 | Val Loss: 0.6576 | Val Corr: 0.3087 | Val RMSE: 0.9128\n",
      "   Epoch 10/100 | Train Loss: 0.6909 | Val Loss: 0.6191 | Val Corr: 0.3191 | Val RMSE: 0.8717\n",
      "Early stopping at epoch 18 (best epoch 3, best corr 0.3774)\n",
      "BEST epoch = 3 | BEST Corr = 0.3774 | RMSE@BEST = 0.8664\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7509 | Val Loss: 0.6085 | Val Corr: 0.0066 | Val RMSE: 0.9604\n",
      "   Epoch 10/100 | Train Loss: 0.6536 | Val Loss: 0.6681 | Val Corr: 0.1646 | Val RMSE: 0.9869\n",
      "   Epoch 20/100 | Train Loss: 0.6110 | Val Loss: 0.5916 | Val Corr: 0.4034 | Val RMSE: 0.8918\n",
      "   Epoch 30/100 | Train Loss: 0.5594 | Val Loss: 0.6889 | Val Corr: 0.3200 | Val RMSE: 0.9892\n",
      "   Epoch 40/100 | Train Loss: 0.5608 | Val Loss: 0.5611 | Val Corr: 0.4638 | Val RMSE: 0.8655\n",
      "   Epoch 50/100 | Train Loss: 0.4800 | Val Loss: 0.5811 | Val Corr: 0.4753 | Val RMSE: 0.8717\n",
      "   Epoch 60/100 | Train Loss: 0.4805 | Val Loss: 0.5685 | Val Corr: 0.4857 | Val RMSE: 0.8798\n",
      "Early stopping at epoch 64 (best epoch 49, best corr 0.5131)\n",
      "BEST epoch = 49 | BEST Corr = 0.5131 | RMSE@BEST = 0.8661\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7274 | Val Loss: 0.7381 | Val Corr: 0.2717 | Val RMSE: 1.1217\n",
      "   Epoch 10/100 | Train Loss: 0.6609 | Val Loss: 0.7435 | Val Corr: 0.4092 | Val RMSE: 1.0743\n",
      "   Epoch 20/100 | Train Loss: 0.6097 | Val Loss: 0.6731 | Val Corr: 0.5012 | Val RMSE: 1.0187\n",
      "   Epoch 30/100 | Train Loss: 0.5755 | Val Loss: 0.6655 | Val Corr: 0.5653 | Val RMSE: 0.9528\n",
      "   Epoch 40/100 | Train Loss: 0.5605 | Val Loss: 0.7163 | Val Corr: 0.4314 | Val RMSE: 1.0108\n",
      "Early stopping at epoch 46 (best epoch 31, best corr 0.5875)\n",
      "BEST epoch = 31 | BEST Corr = 0.5875 | RMSE@BEST = 0.9307\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7208 | Val Loss: 0.7293 | Val Corr: -0.0296 | Val RMSE: 1.0991\n",
      "   Epoch 10/100 | Train Loss: 0.6675 | Val Loss: 0.7017 | Val Corr: 0.1307 | Val RMSE: 1.0727\n",
      "Early stopping at epoch 17 (best epoch 2, best corr 0.1476)\n",
      "BEST epoch = 2 | BEST Corr = 0.1476 | RMSE@BEST = 1.1206\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7718 | Val Loss: 0.5577 | Val Corr: 0.2963 | Val RMSE: 0.9440\n",
      "   Epoch 10/100 | Train Loss: 0.6966 | Val Loss: 0.5344 | Val Corr: 0.2961 | Val RMSE: 0.9079\n",
      "   Epoch 20/100 | Train Loss: 0.6093 | Val Loss: 0.5583 | Val Corr: 0.3197 | Val RMSE: 0.9109\n",
      "   Epoch 30/100 | Train Loss: 0.5649 | Val Loss: 0.5775 | Val Corr: 0.3047 | Val RMSE: 0.9229\n",
      "Early stopping at epoch 33 (best epoch 18, best corr 0.3490)\n",
      "BEST epoch = 18 | BEST Corr = 0.3490 | RMSE@BEST = 0.8976\n",
      "----------------------------------------\n",
      "   [Saved] rice_T5000_LD0.6.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.8 | Species: rice | Feature Count: T5000\n",
      "      Processing Trait: SW\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.8207 | Val Loss: 0.6766 | Val Corr: 0.4940 | Val RMSE: 0.8944\n",
      "   Epoch 10/100 | Train Loss: 0.6847 | Val Loss: 0.5693 | Val Corr: 0.6312 | Val RMSE: 0.7560\n",
      "   Epoch 20/100 | Train Loss: 0.5951 | Val Loss: 0.5286 | Val Corr: 0.6755 | Val RMSE: 0.6991\n",
      "   Epoch 30/100 | Train Loss: 0.5422 | Val Loss: 0.5370 | Val Corr: 0.5952 | Val RMSE: 0.7214\n",
      "Early stopping at epoch 31 (best epoch 16, best corr 0.6777)\n",
      "BEST epoch = 16 | BEST Corr = 0.6777 | RMSE@BEST = 0.7023\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7672 | Val Loss: 0.7071 | Val Corr: 0.3913 | Val RMSE: 1.0495\n",
      "   Epoch 10/100 | Train Loss: 0.6213 | Val Loss: 0.7167 | Val Corr: 0.5454 | Val RMSE: 0.9271\n",
      "   Epoch 20/100 | Train Loss: 0.5323 | Val Loss: 0.6009 | Val Corr: 0.5779 | Val RMSE: 0.8603\n",
      "   Epoch 30/100 | Train Loss: 0.4905 | Val Loss: 0.5840 | Val Corr: 0.5698 | Val RMSE: 0.8743\n",
      "   Epoch 40/100 | Train Loss: 0.4499 | Val Loss: 0.6287 | Val Corr: 0.5870 | Val RMSE: 0.8831\n",
      "   Epoch 50/100 | Train Loss: 0.4267 | Val Loss: 0.6002 | Val Corr: 0.6027 | Val RMSE: 0.8784\n",
      "   Epoch 60/100 | Train Loss: 0.3673 | Val Loss: 0.6270 | Val Corr: 0.5907 | Val RMSE: 0.8730\n",
      "Early stopping at epoch 68 (best epoch 53, best corr 0.6042)\n",
      "BEST epoch = 53 | BEST Corr = 0.6042 | RMSE@BEST = 0.8726\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7749 | Val Loss: 0.8030 | Val Corr: 0.4854 | Val RMSE: 1.1143\n",
      "   Epoch 10/100 | Train Loss: 0.6516 | Val Loss: 0.6406 | Val Corr: 0.6011 | Val RMSE: 0.9275\n",
      "   Epoch 20/100 | Train Loss: 0.5361 | Val Loss: 0.5776 | Val Corr: 0.6607 | Val RMSE: 0.8443\n",
      "   Epoch 30/100 | Train Loss: 0.4747 | Val Loss: 0.6168 | Val Corr: 0.6610 | Val RMSE: 0.8777\n",
      "   Epoch 40/100 | Train Loss: 0.4620 | Val Loss: 0.6253 | Val Corr: 0.6530 | Val RMSE: 0.9154\n",
      "   Epoch 50/100 | Train Loss: 0.4126 | Val Loss: 0.5712 | Val Corr: 0.6808 | Val RMSE: 0.8588\n",
      "   Epoch 60/100 | Train Loss: 0.3584 | Val Loss: 0.6145 | Val Corr: 0.6850 | Val RMSE: 0.8865\n",
      "   Epoch 70/100 | Train Loss: 0.3662 | Val Loss: 0.5643 | Val Corr: 0.6912 | Val RMSE: 0.8333\n",
      "   Epoch 80/100 | Train Loss: 0.3629 | Val Loss: 0.5756 | Val Corr: 0.6934 | Val RMSE: 0.8466\n",
      "Early stopping at epoch 83 (best epoch 68, best corr 0.7048)\n",
      "BEST epoch = 68 | BEST Corr = 0.7048 | RMSE@BEST = 0.8279\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7685 | Val Loss: 0.7150 | Val Corr: 0.3200 | Val RMSE: 0.9402\n",
      "   Epoch 10/100 | Train Loss: 0.6368 | Val Loss: 0.6420 | Val Corr: 0.4268 | Val RMSE: 0.8586\n",
      "   Epoch 20/100 | Train Loss: 0.5059 | Val Loss: 0.6362 | Val Corr: 0.4710 | Val RMSE: 0.9245\n",
      "Early stopping at epoch 30 (best epoch 15, best corr 0.4810)\n",
      "BEST epoch = 15 | BEST Corr = 0.4810 | RMSE@BEST = 0.8325\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7662 | Val Loss: 0.6954 | Val Corr: 0.3608 | Val RMSE: 0.9773\n",
      "   Epoch 10/100 | Train Loss: 0.5943 | Val Loss: 0.6469 | Val Corr: 0.4951 | Val RMSE: 0.8642\n",
      "   Epoch 20/100 | Train Loss: 0.5599 | Val Loss: 0.6732 | Val Corr: 0.4846 | Val RMSE: 0.8858\n",
      "   Epoch 30/100 | Train Loss: 0.4627 | Val Loss: 0.6152 | Val Corr: 0.5270 | Val RMSE: 0.8569\n",
      "   Epoch 40/100 | Train Loss: 0.3747 | Val Loss: 0.6356 | Val Corr: 0.5143 | Val RMSE: 0.9109\n",
      "Early stopping at epoch 47 (best epoch 32, best corr 0.5312)\n",
      "BEST epoch = 32 | BEST Corr = 0.5312 | RMSE@BEST = 0.8651\n",
      "----------------------------------------\n",
      "      Processing Trait: FLW\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7253 | Val Loss: 0.6850 | Val Corr: 0.3483 | Val RMSE: 1.0358\n",
      "   Epoch 10/100 | Train Loss: 0.6459 | Val Loss: 0.6230 | Val Corr: 0.4513 | Val RMSE: 0.9387\n",
      "   Epoch 20/100 | Train Loss: 0.6022 | Val Loss: 0.6116 | Val Corr: 0.4252 | Val RMSE: 0.9527\n",
      "   Epoch 30/100 | Train Loss: 0.5799 | Val Loss: 0.6066 | Val Corr: 0.4842 | Val RMSE: 0.9144\n",
      "Early stopping at epoch 37 (best epoch 22, best corr 0.4964)\n",
      "BEST epoch = 22 | BEST Corr = 0.4964 | RMSE@BEST = 0.9014\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7501 | Val Loss: 0.7190 | Val Corr: 0.2704 | Val RMSE: 1.1516\n",
      "   Epoch 10/100 | Train Loss: 0.6248 | Val Loss: 0.7229 | Val Corr: 0.3531 | Val RMSE: 1.0888\n",
      "   Epoch 20/100 | Train Loss: 0.5628 | Val Loss: 0.7130 | Val Corr: 0.4390 | Val RMSE: 1.0614\n",
      "   Epoch 30/100 | Train Loss: 0.5263 | Val Loss: 0.7290 | Val Corr: 0.4024 | Val RMSE: 1.0581\n",
      "   Epoch 40/100 | Train Loss: 0.5078 | Val Loss: 0.6993 | Val Corr: 0.4467 | Val RMSE: 1.0310\n",
      "Early stopping at epoch 49 (best epoch 34, best corr 0.4950)\n",
      "BEST epoch = 34 | BEST Corr = 0.4950 | RMSE@BEST = 1.0487\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7271 | Val Loss: 0.7236 | Val Corr: 0.3380 | Val RMSE: 1.0792\n",
      "   Epoch 10/100 | Train Loss: 0.6455 | Val Loss: 0.6782 | Val Corr: 0.3623 | Val RMSE: 1.0089\n",
      "   Epoch 20/100 | Train Loss: 0.5927 | Val Loss: 0.6502 | Val Corr: 0.4605 | Val RMSE: 0.9693\n",
      "   Epoch 30/100 | Train Loss: 0.5660 | Val Loss: 0.6749 | Val Corr: 0.4888 | Val RMSE: 0.9901\n",
      "   Epoch 40/100 | Train Loss: 0.4914 | Val Loss: 0.6971 | Val Corr: 0.4989 | Val RMSE: 1.0079\n",
      "   Epoch 50/100 | Train Loss: 0.5184 | Val Loss: 0.6646 | Val Corr: 0.4532 | Val RMSE: 0.9648\n",
      "Early stopping at epoch 56 (best epoch 41, best corr 0.5134)\n",
      "BEST epoch = 41 | BEST Corr = 0.5134 | RMSE@BEST = 0.9588\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7630 | Val Loss: 0.6522 | Val Corr: 0.3217 | Val RMSE: 0.8560\n",
      "   Epoch 10/100 | Train Loss: 0.7344 | Val Loss: 0.6027 | Val Corr: 0.2691 | Val RMSE: 0.8173\n",
      "   Epoch 20/100 | Train Loss: 0.5913 | Val Loss: 0.5935 | Val Corr: 0.2842 | Val RMSE: 0.8166\n",
      "   Epoch 30/100 | Train Loss: 0.5740 | Val Loss: 0.5998 | Val Corr: 0.3221 | Val RMSE: 0.8239\n",
      "   Epoch 40/100 | Train Loss: 0.5143 | Val Loss: 0.6326 | Val Corr: 0.3990 | Val RMSE: 0.8406\n",
      "   Epoch 50/100 | Train Loss: 0.4947 | Val Loss: 0.5761 | Val Corr: 0.4044 | Val RMSE: 0.7964\n",
      "   Epoch 60/100 | Train Loss: 0.4902 | Val Loss: 0.5833 | Val Corr: 0.4114 | Val RMSE: 0.8067\n",
      "   Epoch 70/100 | Train Loss: 0.4545 | Val Loss: 0.5854 | Val Corr: 0.4108 | Val RMSE: 0.7988\n",
      "   Epoch 80/100 | Train Loss: 0.4336 | Val Loss: 0.5834 | Val Corr: 0.4019 | Val RMSE: 0.7950\n",
      "Early stopping at epoch 88 (best epoch 73, best corr 0.4194)\n",
      "BEST epoch = 73 | BEST Corr = 0.4194 | RMSE@BEST = 0.7923\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7230 | Val Loss: 0.5802 | Val Corr: 0.3148 | Val RMSE: 0.8298\n",
      "   Epoch 10/100 | Train Loss: 0.6350 | Val Loss: 0.5675 | Val Corr: 0.4031 | Val RMSE: 0.7668\n",
      "   Epoch 20/100 | Train Loss: 0.5551 | Val Loss: 0.6240 | Val Corr: 0.3618 | Val RMSE: 0.8388\n",
      "Early stopping at epoch 25 (best epoch 10, best corr 0.4031)\n",
      "BEST epoch = 10 | BEST Corr = 0.4031 | RMSE@BEST = 0.7668\n",
      "----------------------------------------\n",
      "      Processing Trait: AC\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7267 | Val Loss: 0.6045 | Val Corr: 0.1523 | Val RMSE: 0.9425\n",
      "   Epoch 10/100 | Train Loss: 0.6948 | Val Loss: 0.5488 | Val Corr: 0.2481 | Val RMSE: 0.9103\n",
      "   Epoch 20/100 | Train Loss: 0.6291 | Val Loss: 0.4901 | Val Corr: 0.3452 | Val RMSE: 0.8997\n",
      "   Epoch 30/100 | Train Loss: 0.5909 | Val Loss: 0.5220 | Val Corr: 0.2598 | Val RMSE: 0.9083\n",
      "Early stopping at epoch 36 (best epoch 21, best corr 0.3513)\n",
      "BEST epoch = 21 | BEST Corr = 0.3513 | RMSE@BEST = 0.8893\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7181 | Val Loss: 0.5968 | Val Corr: 0.1167 | Val RMSE: 0.9282\n",
      "   Epoch 10/100 | Train Loss: 0.6276 | Val Loss: 0.5214 | Val Corr: 0.3605 | Val RMSE: 0.8904\n",
      "   Epoch 20/100 | Train Loss: 0.5608 | Val Loss: 0.5173 | Val Corr: 0.3899 | Val RMSE: 0.8648\n",
      "Early stopping at epoch 27 (best epoch 12, best corr 0.4549)\n",
      "BEST epoch = 12 | BEST Corr = 0.4549 | RMSE@BEST = 0.8515\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.6841 | Val Loss: 0.6512 | Val Corr: 0.0418 | Val RMSE: 1.0691\n",
      "   Epoch 10/100 | Train Loss: 0.6080 | Val Loss: 0.6179 | Val Corr: 0.3092 | Val RMSE: 1.0252\n",
      "   Epoch 20/100 | Train Loss: 0.5291 | Val Loss: 0.6034 | Val Corr: 0.3702 | Val RMSE: 1.0171\n",
      "   Epoch 30/100 | Train Loss: 0.4696 | Val Loss: 0.6062 | Val Corr: 0.3701 | Val RMSE: 1.0203\n",
      "Early stopping at epoch 32 (best epoch 17, best corr 0.4554)\n",
      "BEST epoch = 17 | BEST Corr = 0.4554 | RMSE@BEST = 0.9811\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.6511 | Val Loss: 0.7663 | Val Corr: 0.1243 | Val RMSE: 1.1087\n",
      "   Epoch 10/100 | Train Loss: 0.5994 | Val Loss: 0.7311 | Val Corr: 0.1345 | Val RMSE: 1.1240\n",
      "   Epoch 20/100 | Train Loss: 0.5295 | Val Loss: 0.7011 | Val Corr: 0.3223 | Val RMSE: 1.0511\n",
      "   Epoch 30/100 | Train Loss: 0.4771 | Val Loss: 0.6542 | Val Corr: 0.4356 | Val RMSE: 1.0143\n",
      "   Epoch 40/100 | Train Loss: 0.4387 | Val Loss: 0.6590 | Val Corr: 0.4019 | Val RMSE: 1.0346\n",
      "   Epoch 50/100 | Train Loss: 0.4222 | Val Loss: 0.6619 | Val Corr: 0.3283 | Val RMSE: 1.1064\n",
      "Early stopping at epoch 54 (best epoch 39, best corr 0.4803)\n",
      "BEST epoch = 39 | BEST Corr = 0.4803 | RMSE@BEST = 0.9814\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7562 | Val Loss: 0.6490 | Val Corr: 0.2197 | Val RMSE: 0.9466\n",
      "   Epoch 10/100 | Train Loss: 0.6135 | Val Loss: 0.5429 | Val Corr: 0.3979 | Val RMSE: 0.9036\n",
      "   Epoch 20/100 | Train Loss: 0.5639 | Val Loss: 0.5858 | Val Corr: 0.4735 | Val RMSE: 0.8498\n",
      "   Epoch 30/100 | Train Loss: 0.5036 | Val Loss: 0.5308 | Val Corr: 0.5285 | Val RMSE: 0.8255\n",
      "   Epoch 40/100 | Train Loss: 0.4478 | Val Loss: 0.5485 | Val Corr: 0.5567 | Val RMSE: 0.7900\n",
      "   Epoch 50/100 | Train Loss: 0.4251 | Val Loss: 0.5502 | Val Corr: 0.5087 | Val RMSE: 0.8148\n",
      "Early stopping at epoch 56 (best epoch 41, best corr 0.5700)\n",
      "BEST epoch = 41 | BEST Corr = 0.5700 | RMSE@BEST = 0.7808\n",
      "----------------------------------------\n",
      "      Processing Trait: PH\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7774 | Val Loss: 0.6305 | Val Corr: 0.4638 | Val RMSE: 0.9047\n",
      "   Epoch 10/100 | Train Loss: 0.6808 | Val Loss: 0.6219 | Val Corr: 0.5131 | Val RMSE: 0.8233\n",
      "   Epoch 20/100 | Train Loss: 0.6278 | Val Loss: 0.5383 | Val Corr: 0.5439 | Val RMSE: 0.7709\n",
      "   Epoch 30/100 | Train Loss: 0.5725 | Val Loss: 0.5261 | Val Corr: 0.5756 | Val RMSE: 0.7439\n",
      "   Epoch 40/100 | Train Loss: 0.5541 | Val Loss: 0.5255 | Val Corr: 0.5958 | Val RMSE: 0.7441\n",
      "   Epoch 50/100 | Train Loss: 0.5117 | Val Loss: 0.5085 | Val Corr: 0.6528 | Val RMSE: 0.6892\n",
      "   Epoch 60/100 | Train Loss: 0.4524 | Val Loss: 0.5248 | Val Corr: 0.6324 | Val RMSE: 0.7100\n",
      "   Epoch 70/100 | Train Loss: 0.4274 | Val Loss: 0.5162 | Val Corr: 0.6300 | Val RMSE: 0.7038\n",
      "Early stopping at epoch 78 (best epoch 63, best corr 0.6836)\n",
      "BEST epoch = 63 | BEST Corr = 0.6836 | RMSE@BEST = 0.6940\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7442 | Val Loss: 0.5886 | Val Corr: 0.3501 | Val RMSE: 0.8681\n",
      "   Epoch 10/100 | Train Loss: 0.6482 | Val Loss: 0.5422 | Val Corr: 0.4255 | Val RMSE: 0.7893\n",
      "   Epoch 20/100 | Train Loss: 0.5985 | Val Loss: 0.5950 | Val Corr: 0.4365 | Val RMSE: 0.8232\n",
      "Early stopping at epoch 28 (best epoch 13, best corr 0.4720)\n",
      "BEST epoch = 13 | BEST Corr = 0.4720 | RMSE@BEST = 0.7954\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7512 | Val Loss: 0.6941 | Val Corr: -0.0007 | Val RMSE: 0.9970\n",
      "   Epoch 10/100 | Train Loss: 0.6514 | Val Loss: 0.6950 | Val Corr: 0.1650 | Val RMSE: 0.9910\n",
      "   Epoch 20/100 | Train Loss: 0.6065 | Val Loss: 0.7051 | Val Corr: 0.2703 | Val RMSE: 0.9765\n",
      "   Epoch 30/100 | Train Loss: 0.5043 | Val Loss: 0.6357 | Val Corr: 0.4039 | Val RMSE: 0.9249\n",
      "   Epoch 40/100 | Train Loss: 0.4774 | Val Loss: 0.6901 | Val Corr: 0.4005 | Val RMSE: 0.9591\n",
      "   Epoch 50/100 | Train Loss: 0.4789 | Val Loss: 0.7192 | Val Corr: 0.3498 | Val RMSE: 0.9810\n",
      "Early stopping at epoch 54 (best epoch 39, best corr 0.4181)\n",
      "BEST epoch = 39 | BEST Corr = 0.4181 | RMSE@BEST = 0.9350\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.6845 | Val Loss: 0.9108 | Val Corr: 0.2225 | Val RMSE: 1.2866\n",
      "   Epoch 10/100 | Train Loss: 0.6195 | Val Loss: 0.7993 | Val Corr: 0.2721 | Val RMSE: 1.2261\n",
      "   Epoch 20/100 | Train Loss: 0.5274 | Val Loss: 0.7980 | Val Corr: 0.2474 | Val RMSE: 1.2459\n",
      "   Epoch 30/100 | Train Loss: 0.5109 | Val Loss: 0.7422 | Val Corr: 0.3587 | Val RMSE: 1.1943\n",
      "   Epoch 40/100 | Train Loss: 0.4475 | Val Loss: 0.7541 | Val Corr: 0.3505 | Val RMSE: 1.2052\n",
      "   Epoch 50/100 | Train Loss: 0.4116 | Val Loss: 0.7198 | Val Corr: 0.4048 | Val RMSE: 1.1672\n",
      "   Epoch 60/100 | Train Loss: 0.3798 | Val Loss: 0.7687 | Val Corr: 0.4076 | Val RMSE: 1.1927\n",
      "   Epoch 70/100 | Train Loss: 0.3734 | Val Loss: 0.7148 | Val Corr: 0.4270 | Val RMSE: 1.1500\n",
      "   Epoch 80/100 | Train Loss: 0.3772 | Val Loss: 0.7168 | Val Corr: 0.4268 | Val RMSE: 1.1520\n",
      "Early stopping at epoch 90 (best epoch 75, best corr 0.4475)\n",
      "BEST epoch = 75 | BEST Corr = 0.4475 | RMSE@BEST = 1.1381\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7144 | Val Loss: 0.6125 | Val Corr: 0.3724 | Val RMSE: 0.9151\n",
      "   Epoch 10/100 | Train Loss: 0.6652 | Val Loss: 0.5901 | Val Corr: 0.4456 | Val RMSE: 0.8249\n",
      "   Epoch 20/100 | Train Loss: 0.5798 | Val Loss: 0.5855 | Val Corr: 0.4694 | Val RMSE: 0.8079\n",
      "   Epoch 30/100 | Train Loss: 0.5184 | Val Loss: 0.5669 | Val Corr: 0.5405 | Val RMSE: 0.7736\n",
      "   Epoch 40/100 | Train Loss: 0.4611 | Val Loss: 0.5758 | Val Corr: 0.5300 | Val RMSE: 0.7934\n",
      "Early stopping at epoch 43 (best epoch 28, best corr 0.5639)\n",
      "BEST epoch = 28 | BEST Corr = 0.5639 | RMSE@BEST = 0.7504\n",
      "----------------------------------------\n",
      "      Processing Trait: SNPP\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7620 | Val Loss: 0.6471 | Val Corr: 0.2919 | Val RMSE: 0.9072\n",
      "   Epoch 10/100 | Train Loss: 0.6980 | Val Loss: 0.6180 | Val Corr: 0.3825 | Val RMSE: 0.8457\n",
      "   Epoch 20/100 | Train Loss: 0.6563 | Val Loss: 0.5988 | Val Corr: 0.4258 | Val RMSE: 0.8236\n",
      "   Epoch 30/100 | Train Loss: 0.6143 | Val Loss: 0.5779 | Val Corr: 0.4540 | Val RMSE: 0.8080\n",
      "   Epoch 40/100 | Train Loss: 0.5948 | Val Loss: 0.5655 | Val Corr: 0.5073 | Val RMSE: 0.7916\n",
      "   Epoch 50/100 | Train Loss: 0.5498 | Val Loss: 0.5780 | Val Corr: 0.4475 | Val RMSE: 0.8185\n",
      "Early stopping at epoch 55 (best epoch 40, best corr 0.5073)\n",
      "BEST epoch = 40 | BEST Corr = 0.5073 | RMSE@BEST = 0.7916\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7434 | Val Loss: 0.6079 | Val Corr: 0.0833 | Val RMSE: 0.9600\n",
      "   Epoch 10/100 | Train Loss: 0.6848 | Val Loss: 0.6000 | Val Corr: 0.2296 | Val RMSE: 0.9336\n",
      "   Epoch 20/100 | Train Loss: 0.6187 | Val Loss: 0.5812 | Val Corr: 0.4039 | Val RMSE: 0.8822\n",
      "   Epoch 30/100 | Train Loss: 0.5511 | Val Loss: 0.5845 | Val Corr: 0.4024 | Val RMSE: 0.8864\n",
      "   Epoch 40/100 | Train Loss: 0.5798 | Val Loss: 0.6266 | Val Corr: 0.4028 | Val RMSE: 0.9202\n",
      "   Epoch 50/100 | Train Loss: 0.4918 | Val Loss: 0.5928 | Val Corr: 0.4171 | Val RMSE: 0.9039\n",
      "   Epoch 60/100 | Train Loss: 0.4730 | Val Loss: 0.5781 | Val Corr: 0.4684 | Val RMSE: 0.8775\n",
      "   Epoch 70/100 | Train Loss: 0.4223 | Val Loss: 0.5769 | Val Corr: 0.4543 | Val RMSE: 0.8793\n",
      "Early stopping at epoch 72 (best epoch 57, best corr 0.4759)\n",
      "BEST epoch = 57 | BEST Corr = 0.4759 | RMSE@BEST = 0.8698\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7239 | Val Loss: 0.7354 | Val Corr: 0.1415 | Val RMSE: 1.1211\n",
      "   Epoch 10/100 | Train Loss: 0.6526 | Val Loss: 0.6891 | Val Corr: 0.4519 | Val RMSE: 1.0554\n",
      "   Epoch 20/100 | Train Loss: 0.6124 | Val Loss: 0.6836 | Val Corr: 0.4817 | Val RMSE: 1.0338\n",
      "   Epoch 30/100 | Train Loss: 0.5632 | Val Loss: 0.6825 | Val Corr: 0.5270 | Val RMSE: 0.9629\n",
      "   Epoch 40/100 | Train Loss: 0.5499 | Val Loss: 0.6855 | Val Corr: 0.4825 | Val RMSE: 0.9767\n",
      "   Epoch 50/100 | Train Loss: 0.5058 | Val Loss: 0.7305 | Val Corr: 0.4814 | Val RMSE: 0.9871\n",
      "Early stopping at epoch 51 (best epoch 36, best corr 0.5611)\n",
      "BEST epoch = 36 | BEST Corr = 0.5611 | RMSE@BEST = 0.9301\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7268 | Val Loss: 0.7316 | Val Corr: 0.0477 | Val RMSE: 1.1004\n",
      "   Epoch 10/100 | Train Loss: 0.6351 | Val Loss: 0.7487 | Val Corr: 0.0904 | Val RMSE: 1.0724\n",
      "   Epoch 20/100 | Train Loss: 0.5959 | Val Loss: 0.7254 | Val Corr: 0.1819 | Val RMSE: 1.0993\n",
      "Early stopping at epoch 30 (best epoch 15, best corr 0.1933)\n",
      "BEST epoch = 15 | BEST Corr = 0.1933 | RMSE@BEST = 1.0913\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7564 | Val Loss: 0.5732 | Val Corr: 0.1888 | Val RMSE: 0.9504\n",
      "   Epoch 10/100 | Train Loss: 0.6892 | Val Loss: 0.5379 | Val Corr: 0.2837 | Val RMSE: 0.8947\n",
      "   Epoch 20/100 | Train Loss: 0.6096 | Val Loss: 0.5732 | Val Corr: 0.2888 | Val RMSE: 0.8924\n",
      "   Epoch 30/100 | Train Loss: 0.5683 | Val Loss: 0.5911 | Val Corr: 0.3202 | Val RMSE: 0.9416\n",
      "Early stopping at epoch 33 (best epoch 18, best corr 0.3669)\n",
      "BEST epoch = 18 | BEST Corr = 0.3669 | RMSE@BEST = 0.8853\n",
      "----------------------------------------\n",
      "   [Saved] rice_T5000_LD0.8.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.2 | Species: sorghum | Feature Count: T5000\n",
      "      Processing Trait: HT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.7536 | Val Loss: 0.7654 | Val Corr: 0.4017 | Val RMSE: 0.9386\n",
      "   Epoch 10/100 | Train Loss: 0.6167 | Val Loss: 0.6478 | Val Corr: 0.4994 | Val RMSE: 0.8646\n",
      "   Epoch 20/100 | Train Loss: 0.4668 | Val Loss: 0.6170 | Val Corr: 0.5157 | Val RMSE: 0.8095\n",
      "   Epoch 30/100 | Train Loss: 0.4076 | Val Loss: 0.6601 | Val Corr: 0.4609 | Val RMSE: 0.8651\n",
      "Early stopping at epoch 37 (best epoch 22, best corr 0.5178)\n",
      "BEST epoch = 22 | BEST Corr = 0.5178 | RMSE@BEST = 0.8097\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7872 | Val Loss: 0.7837 | Val Corr: 0.3695 | Val RMSE: 1.0806\n",
      "   Epoch 10/100 | Train Loss: 0.5750 | Val Loss: 0.7183 | Val Corr: 0.5156 | Val RMSE: 1.0251\n",
      "   Epoch 20/100 | Train Loss: 0.4355 | Val Loss: 0.7246 | Val Corr: 0.4841 | Val RMSE: 1.0382\n",
      "Early stopping at epoch 22 (best epoch 7, best corr 0.5240)\n",
      "BEST epoch = 7 | BEST Corr = 0.5240 | RMSE@BEST = 1.0675\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7653 | Val Loss: 0.9049 | Val Corr: 0.3084 | Val RMSE: 1.0754\n",
      "   Epoch 10/100 | Train Loss: 0.5852 | Val Loss: 0.6746 | Val Corr: 0.5594 | Val RMSE: 0.8576\n",
      "   Epoch 20/100 | Train Loss: 0.4770 | Val Loss: 0.6206 | Val Corr: 0.5859 | Val RMSE: 0.8297\n",
      "   Epoch 30/100 | Train Loss: 0.4163 | Val Loss: 0.6885 | Val Corr: 0.5902 | Val RMSE: 0.8517\n",
      "   Epoch 40/100 | Train Loss: 0.3397 | Val Loss: 0.6314 | Val Corr: 0.5849 | Val RMSE: 0.8151\n",
      "Early stopping at epoch 44 (best epoch 29, best corr 0.6083)\n",
      "BEST epoch = 29 | BEST Corr = 0.6083 | RMSE@BEST = 0.7956\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7630 | Val Loss: 0.8188 | Val Corr: 0.3559 | Val RMSE: 1.0060\n",
      "   Epoch 10/100 | Train Loss: 0.5609 | Val Loss: 0.7328 | Val Corr: 0.4800 | Val RMSE: 0.8982\n",
      "   Epoch 20/100 | Train Loss: 0.4755 | Val Loss: 0.7614 | Val Corr: 0.4697 | Val RMSE: 0.9734\n",
      "Early stopping at epoch 27 (best epoch 12, best corr 0.5290)\n",
      "BEST epoch = 12 | BEST Corr = 0.5290 | RMSE@BEST = 0.8696\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7825 | Val Loss: 0.7516 | Val Corr: 0.4656 | Val RMSE: 0.9682\n",
      "   Epoch 10/100 | Train Loss: 0.5730 | Val Loss: 0.6917 | Val Corr: 0.5480 | Val RMSE: 0.8649\n",
      "   Epoch 20/100 | Train Loss: 0.4518 | Val Loss: 0.6151 | Val Corr: 0.5905 | Val RMSE: 0.7866\n",
      "   Epoch 30/100 | Train Loss: 0.3932 | Val Loss: 0.6693 | Val Corr: 0.5402 | Val RMSE: 0.8251\n",
      "Early stopping at epoch 35 (best epoch 20, best corr 0.5905)\n",
      "BEST epoch = 20 | BEST Corr = 0.5905 | RMSE@BEST = 0.7866\n",
      "----------------------------------------\n",
      "      Processing Trait: MO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.7714 | Val Loss: 0.8600 | Val Corr: 0.1313 | Val RMSE: 1.0677\n",
      "   Epoch 10/100 | Train Loss: 0.6322 | Val Loss: 0.8356 | Val Corr: 0.2253 | Val RMSE: 1.0477\n",
      "   Epoch 20/100 | Train Loss: 0.5227 | Val Loss: 0.9146 | Val Corr: 0.2741 | Val RMSE: 1.1323\n",
      "   Epoch 30/100 | Train Loss: 0.4318 | Val Loss: 0.8882 | Val Corr: 0.2024 | Val RMSE: 1.1271\n",
      "Early stopping at epoch 31 (best epoch 16, best corr 0.2950)\n",
      "BEST epoch = 16 | BEST Corr = 0.2950 | RMSE@BEST = 1.0162\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7980 | Val Loss: 0.6748 | Val Corr: 0.4583 | Val RMSE: 0.8433\n",
      "   Epoch 10/100 | Train Loss: 0.5997 | Val Loss: 0.7620 | Val Corr: 0.4150 | Val RMSE: 0.8823\n",
      "Early stopping at epoch 16 (best epoch 1, best corr 0.4583)\n",
      "BEST epoch = 1 | BEST Corr = 0.4583 | RMSE@BEST = 0.8433\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7837 | Val Loss: 0.7285 | Val Corr: 0.3696 | Val RMSE: 0.9106\n",
      "   Epoch 10/100 | Train Loss: 0.6441 | Val Loss: 0.6499 | Val Corr: 0.4597 | Val RMSE: 0.8185\n",
      "Early stopping at epoch 20 (best epoch 5, best corr 0.4787)\n",
      "BEST epoch = 5 | BEST Corr = 0.4787 | RMSE@BEST = 0.7991\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7742 | Val Loss: 0.7530 | Val Corr: 0.1085 | Val RMSE: 0.9497\n",
      "   Epoch 10/100 | Train Loss: 0.6031 | Val Loss: 0.7230 | Val Corr: 0.2678 | Val RMSE: 0.9751\n",
      "   Epoch 20/100 | Train Loss: 0.4881 | Val Loss: 0.7499 | Val Corr: 0.2771 | Val RMSE: 0.9752\n",
      "   Epoch 30/100 | Train Loss: 0.4721 | Val Loss: 0.7130 | Val Corr: 0.3066 | Val RMSE: 0.9319\n",
      "   Epoch 40/100 | Train Loss: 0.3781 | Val Loss: 0.7859 | Val Corr: 0.2904 | Val RMSE: 0.9791\n",
      "Early stopping at epoch 45 (best epoch 30, best corr 0.3066)\n",
      "BEST epoch = 30 | BEST Corr = 0.3066 | RMSE@BEST = 0.9319\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7586 | Val Loss: 0.9172 | Val Corr: 0.3506 | Val RMSE: 1.1894\n",
      "   Epoch 10/100 | Train Loss: 0.5696 | Val Loss: 0.7766 | Val Corr: 0.4491 | Val RMSE: 1.0630\n",
      "   Epoch 20/100 | Train Loss: 0.4471 | Val Loss: 0.8168 | Val Corr: 0.4198 | Val RMSE: 1.0859\n",
      "   Epoch 30/100 | Train Loss: 0.3916 | Val Loss: 0.7416 | Val Corr: 0.5351 | Val RMSE: 1.0169\n",
      "   Epoch 40/100 | Train Loss: 0.3747 | Val Loss: 0.7869 | Val Corr: 0.4689 | Val RMSE: 1.0644\n",
      "Early stopping at epoch 49 (best epoch 34, best corr 0.5545)\n",
      "BEST epoch = 34 | BEST Corr = 0.5545 | RMSE@BEST = 1.0009\n",
      "----------------------------------------\n",
      "      Processing Trait: YLD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.7706 | Val Loss: 0.8553 | Val Corr: 0.4346 | Val RMSE: 1.0353\n",
      "   Epoch 10/100 | Train Loss: 0.5532 | Val Loss: 0.7013 | Val Corr: 0.5771 | Val RMSE: 0.9188\n",
      "   Epoch 20/100 | Train Loss: 0.4424 | Val Loss: 0.8020 | Val Corr: 0.5991 | Val RMSE: 1.0439\n",
      "   Epoch 30/100 | Train Loss: 0.3724 | Val Loss: 0.6533 | Val Corr: 0.5899 | Val RMSE: 0.8487\n",
      "   Epoch 40/100 | Train Loss: 0.3152 | Val Loss: 0.6640 | Val Corr: 0.6086 | Val RMSE: 0.8729\n",
      "Early stopping at epoch 50 (best epoch 35, best corr 0.6149)\n",
      "BEST epoch = 35 | BEST Corr = 0.6149 | RMSE@BEST = 0.9112\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7794 | Val Loss: 0.8910 | Val Corr: 0.4894 | Val RMSE: 1.0401\n",
      "   Epoch 10/100 | Train Loss: 0.5590 | Val Loss: 0.7086 | Val Corr: 0.5528 | Val RMSE: 0.8715\n",
      "   Epoch 20/100 | Train Loss: 0.4339 | Val Loss: 0.7380 | Val Corr: 0.4868 | Val RMSE: 0.9174\n",
      "Early stopping at epoch 22 (best epoch 7, best corr 0.5747)\n",
      "BEST epoch = 7 | BEST Corr = 0.5747 | RMSE@BEST = 0.8727\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8357 | Val Loss: 0.7576 | Val Corr: 0.4497 | Val RMSE: 0.9661\n",
      "   Epoch 10/100 | Train Loss: 0.5678 | Val Loss: 0.7094 | Val Corr: 0.5018 | Val RMSE: 0.8904\n",
      "   Epoch 20/100 | Train Loss: 0.4414 | Val Loss: 0.7401 | Val Corr: 0.4909 | Val RMSE: 0.9215\n",
      "   Epoch 30/100 | Train Loss: 0.3848 | Val Loss: 0.7054 | Val Corr: 0.5182 | Val RMSE: 0.8751\n",
      "Early stopping at epoch 39 (best epoch 24, best corr 0.5715)\n",
      "BEST epoch = 24 | BEST Corr = 0.5715 | RMSE@BEST = 0.8499\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8181 | Val Loss: 0.7393 | Val Corr: 0.4336 | Val RMSE: 0.9405\n",
      "   Epoch 10/100 | Train Loss: 0.5341 | Val Loss: 0.6332 | Val Corr: 0.5268 | Val RMSE: 0.8384\n",
      "   Epoch 20/100 | Train Loss: 0.4216 | Val Loss: 0.7492 | Val Corr: 0.5562 | Val RMSE: 0.9521\n",
      "   Epoch 30/100 | Train Loss: 0.4080 | Val Loss: 0.6445 | Val Corr: 0.5794 | Val RMSE: 0.8544\n",
      "   Epoch 40/100 | Train Loss: 0.3078 | Val Loss: 0.5901 | Val Corr: 0.5881 | Val RMSE: 0.7744\n",
      "   Epoch 50/100 | Train Loss: 0.2836 | Val Loss: 0.6076 | Val Corr: 0.5760 | Val RMSE: 0.7934\n",
      "Early stopping at epoch 58 (best epoch 43, best corr 0.5973)\n",
      "BEST epoch = 43 | BEST Corr = 0.5973 | RMSE@BEST = 0.7858\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7874 | Val Loss: 0.8202 | Val Corr: 0.3611 | Val RMSE: 1.0062\n",
      "   Epoch 10/100 | Train Loss: 0.5297 | Val Loss: 0.6731 | Val Corr: 0.5310 | Val RMSE: 0.8723\n",
      "   Epoch 20/100 | Train Loss: 0.4102 | Val Loss: 0.6333 | Val Corr: 0.5977 | Val RMSE: 0.8382\n",
      "   Epoch 30/100 | Train Loss: 0.3226 | Val Loss: 0.6761 | Val Corr: 0.5526 | Val RMSE: 0.8678\n",
      "Early stopping at epoch 35 (best epoch 20, best corr 0.5977)\n",
      "BEST epoch = 20 | BEST Corr = 0.5977 | RMSE@BEST = 0.8382\n",
      "----------------------------------------\n",
      "   [Saved] sorghum_T5000_LD0.2.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.4 | Species: sorghum | Feature Count: T5000\n",
      "      Processing Trait: HT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.7516 | Val Loss: 0.7749 | Val Corr: 0.3633 | Val RMSE: 0.9440\n",
      "   Epoch 10/100 | Train Loss: 0.5877 | Val Loss: 0.6324 | Val Corr: 0.5269 | Val RMSE: 0.8007\n",
      "   Epoch 20/100 | Train Loss: 0.4695 | Val Loss: 0.6292 | Val Corr: 0.5278 | Val RMSE: 0.8107\n",
      "Early stopping at epoch 26 (best epoch 11, best corr 0.5411)\n",
      "BEST epoch = 11 | BEST Corr = 0.5411 | RMSE@BEST = 0.8073\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7827 | Val Loss: 0.7848 | Val Corr: 0.3758 | Val RMSE: 1.0828\n",
      "   Epoch 10/100 | Train Loss: 0.5656 | Val Loss: 0.7297 | Val Corr: 0.4529 | Val RMSE: 0.9722\n",
      "   Epoch 20/100 | Train Loss: 0.4432 | Val Loss: 0.6528 | Val Corr: 0.5204 | Val RMSE: 0.9460\n",
      "   Epoch 30/100 | Train Loss: 0.3554 | Val Loss: 0.6683 | Val Corr: 0.4987 | Val RMSE: 0.9494\n",
      "Early stopping at epoch 35 (best epoch 20, best corr 0.5204)\n",
      "BEST epoch = 20 | BEST Corr = 0.5204 | RMSE@BEST = 0.9460\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7671 | Val Loss: 0.9204 | Val Corr: 0.3084 | Val RMSE: 1.0924\n",
      "   Epoch 10/100 | Train Loss: 0.5802 | Val Loss: 0.6885 | Val Corr: 0.5458 | Val RMSE: 0.8643\n",
      "   Epoch 20/100 | Train Loss: 0.4597 | Val Loss: 0.6595 | Val Corr: 0.5508 | Val RMSE: 0.8479\n",
      "   Epoch 30/100 | Train Loss: 0.3973 | Val Loss: 0.7088 | Val Corr: 0.5749 | Val RMSE: 0.8783\n",
      "   Epoch 40/100 | Train Loss: 0.3331 | Val Loss: 0.6686 | Val Corr: 0.5625 | Val RMSE: 0.8531\n",
      "Early stopping at epoch 46 (best epoch 31, best corr 0.5786)\n",
      "BEST epoch = 31 | BEST Corr = 0.5786 | RMSE@BEST = 0.9291\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7419 | Val Loss: 0.8039 | Val Corr: 0.3407 | Val RMSE: 1.0004\n",
      "   Epoch 10/100 | Train Loss: 0.5712 | Val Loss: 0.6915 | Val Corr: 0.5069 | Val RMSE: 0.8659\n",
      "   Epoch 20/100 | Train Loss: 0.4671 | Val Loss: 0.7521 | Val Corr: 0.5183 | Val RMSE: 0.9183\n",
      "   Epoch 30/100 | Train Loss: 0.3988 | Val Loss: 0.7027 | Val Corr: 0.5509 | Val RMSE: 0.8720\n",
      "   Epoch 40/100 | Train Loss: 0.3224 | Val Loss: 0.6802 | Val Corr: 0.5673 | Val RMSE: 0.8449\n",
      "   Epoch 50/100 | Train Loss: 0.3146 | Val Loss: 0.6791 | Val Corr: 0.5551 | Val RMSE: 0.8669\n",
      "Early stopping at epoch 55 (best epoch 40, best corr 0.5673)\n",
      "BEST epoch = 40 | BEST Corr = 0.5673 | RMSE@BEST = 0.8449\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7806 | Val Loss: 0.7637 | Val Corr: 0.4350 | Val RMSE: 0.9717\n",
      "   Epoch 10/100 | Train Loss: 0.5519 | Val Loss: 0.6427 | Val Corr: 0.5761 | Val RMSE: 0.8180\n",
      "   Epoch 20/100 | Train Loss: 0.4221 | Val Loss: 0.6471 | Val Corr: 0.5472 | Val RMSE: 0.8322\n",
      "Early stopping at epoch 25 (best epoch 10, best corr 0.5761)\n",
      "BEST epoch = 10 | BEST Corr = 0.5761 | RMSE@BEST = 0.8180\n",
      "----------------------------------------\n",
      "      Processing Trait: MO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.7733 | Val Loss: 0.8588 | Val Corr: 0.1707 | Val RMSE: 1.0669\n",
      "   Epoch 10/100 | Train Loss: 0.6317 | Val Loss: 0.8615 | Val Corr: 0.1712 | Val RMSE: 1.0674\n",
      "   Epoch 20/100 | Train Loss: 0.4876 | Val Loss: 0.8581 | Val Corr: 0.2468 | Val RMSE: 1.0690\n",
      "   Epoch 30/100 | Train Loss: 0.4348 | Val Loss: 0.8791 | Val Corr: 0.2092 | Val RMSE: 1.0841\n",
      "   Epoch 40/100 | Train Loss: 0.3744 | Val Loss: 0.8090 | Val Corr: 0.2528 | Val RMSE: 1.0324\n",
      "   Epoch 50/100 | Train Loss: 0.3683 | Val Loss: 0.8244 | Val Corr: 0.2343 | Val RMSE: 1.0587\n",
      "Early stopping at epoch 54 (best epoch 39, best corr 0.2531)\n",
      "BEST epoch = 39 | BEST Corr = 0.2531 | RMSE@BEST = 1.0364\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8004 | Val Loss: 0.6752 | Val Corr: 0.4539 | Val RMSE: 0.8431\n",
      "   Epoch 10/100 | Train Loss: 0.6238 | Val Loss: 0.9319 | Val Corr: 0.4169 | Val RMSE: 1.0736\n",
      "Early stopping at epoch 17 (best epoch 2, best corr 0.4694)\n",
      "BEST epoch = 2 | BEST Corr = 0.4694 | RMSE@BEST = 0.8250\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7841 | Val Loss: 0.7277 | Val Corr: 0.3745 | Val RMSE: 0.9097\n",
      "   Epoch 10/100 | Train Loss: 0.6257 | Val Loss: 0.6535 | Val Corr: 0.4741 | Val RMSE: 0.8273\n",
      "   Epoch 20/100 | Train Loss: 0.4927 | Val Loss: 0.6707 | Val Corr: 0.4191 | Val RMSE: 0.8481\n",
      "Early stopping at epoch 27 (best epoch 12, best corr 0.5037)\n",
      "BEST epoch = 12 | BEST Corr = 0.5037 | RMSE@BEST = 0.8322\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7748 | Val Loss: 0.7506 | Val Corr: 0.1363 | Val RMSE: 0.9503\n",
      "   Epoch 10/100 | Train Loss: 0.6211 | Val Loss: 0.7541 | Val Corr: 0.2200 | Val RMSE: 0.9737\n",
      "   Epoch 20/100 | Train Loss: 0.4958 | Val Loss: 0.8625 | Val Corr: 0.2566 | Val RMSE: 1.0717\n",
      "   Epoch 30/100 | Train Loss: 0.4536 | Val Loss: 0.7584 | Val Corr: 0.3114 | Val RMSE: 0.9761\n",
      "   Epoch 40/100 | Train Loss: 0.3857 | Val Loss: 0.7206 | Val Corr: 0.3219 | Val RMSE: 0.9457\n",
      "   Epoch 50/100 | Train Loss: 0.3394 | Val Loss: 0.7855 | Val Corr: 0.3340 | Val RMSE: 1.0018\n",
      "   Epoch 60/100 | Train Loss: 0.3136 | Val Loss: 0.7654 | Val Corr: 0.3179 | Val RMSE: 0.9809\n",
      "Early stopping at epoch 63 (best epoch 48, best corr 0.3482)\n",
      "BEST epoch = 48 | BEST Corr = 0.3482 | RMSE@BEST = 0.9596\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7621 | Val Loss: 0.9209 | Val Corr: 0.3251 | Val RMSE: 1.1911\n",
      "   Epoch 10/100 | Train Loss: 0.6003 | Val Loss: 0.8106 | Val Corr: 0.4032 | Val RMSE: 1.0894\n",
      "   Epoch 20/100 | Train Loss: 0.4423 | Val Loss: 0.7804 | Val Corr: 0.4746 | Val RMSE: 1.0494\n",
      "Early stopping at epoch 23 (best epoch 8, best corr 0.4779)\n",
      "BEST epoch = 8 | BEST Corr = 0.4779 | RMSE@BEST = 1.0576\n",
      "----------------------------------------\n",
      "      Processing Trait: YLD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.7780 | Val Loss: 0.8549 | Val Corr: 0.3990 | Val RMSE: 1.0270\n",
      "   Epoch 10/100 | Train Loss: 0.5525 | Val Loss: 0.7084 | Val Corr: 0.5396 | Val RMSE: 0.9357\n",
      "   Epoch 20/100 | Train Loss: 0.4655 | Val Loss: 0.7132 | Val Corr: 0.5471 | Val RMSE: 0.9389\n",
      "Early stopping at epoch 27 (best epoch 12, best corr 0.5752)\n",
      "BEST epoch = 12 | BEST Corr = 0.5752 | RMSE@BEST = 0.8472\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7808 | Val Loss: 0.8910 | Val Corr: 0.4990 | Val RMSE: 1.0373\n",
      "   Epoch 10/100 | Train Loss: 0.5494 | Val Loss: 0.6903 | Val Corr: 0.5768 | Val RMSE: 0.8513\n",
      "   Epoch 20/100 | Train Loss: 0.4222 | Val Loss: 0.7327 | Val Corr: 0.5338 | Val RMSE: 0.9047\n",
      "   Epoch 30/100 | Train Loss: 0.3664 | Val Loss: 0.6932 | Val Corr: 0.5851 | Val RMSE: 0.8452\n",
      "   Epoch 40/100 | Train Loss: 0.3300 | Val Loss: 0.6868 | Val Corr: 0.5914 | Val RMSE: 0.8546\n",
      "   Epoch 50/100 | Train Loss: 0.3003 | Val Loss: 0.6985 | Val Corr: 0.5770 | Val RMSE: 0.8672\n",
      "Early stopping at epoch 57 (best epoch 42, best corr 0.6064)\n",
      "BEST epoch = 42 | BEST Corr = 0.6064 | RMSE@BEST = 0.8302\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8374 | Val Loss: 0.7580 | Val Corr: 0.4838 | Val RMSE: 0.9679\n",
      "   Epoch 10/100 | Train Loss: 0.5715 | Val Loss: 0.6328 | Val Corr: 0.5378 | Val RMSE: 0.8178\n",
      "   Epoch 20/100 | Train Loss: 0.4411 | Val Loss: 0.6703 | Val Corr: 0.5506 | Val RMSE: 0.8373\n",
      "Early stopping at epoch 23 (best epoch 8, best corr 0.5915)\n",
      "BEST epoch = 8 | BEST Corr = 0.5915 | RMSE@BEST = 0.9213\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8108 | Val Loss: 0.7397 | Val Corr: 0.4353 | Val RMSE: 0.9435\n",
      "   Epoch 10/100 | Train Loss: 0.5470 | Val Loss: 0.6570 | Val Corr: 0.5449 | Val RMSE: 0.8587\n",
      "   Epoch 20/100 | Train Loss: 0.4615 | Val Loss: 0.8435 | Val Corr: 0.5871 | Val RMSE: 1.0510\n",
      "   Epoch 30/100 | Train Loss: 0.4082 | Val Loss: 0.9612 | Val Corr: 0.5941 | Val RMSE: 1.1560\n",
      "   Epoch 40/100 | Train Loss: 0.3110 | Val Loss: 0.6898 | Val Corr: 0.6119 | Val RMSE: 0.9035\n",
      "Early stopping at epoch 43 (best epoch 28, best corr 0.6270)\n",
      "BEST epoch = 28 | BEST Corr = 0.6270 | RMSE@BEST = 0.9808\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7902 | Val Loss: 0.8203 | Val Corr: 0.3868 | Val RMSE: 1.0004\n",
      "   Epoch 10/100 | Train Loss: 0.5395 | Val Loss: 0.6686 | Val Corr: 0.5657 | Val RMSE: 0.8621\n",
      "   Epoch 20/100 | Train Loss: 0.4174 | Val Loss: 0.6653 | Val Corr: 0.5388 | Val RMSE: 0.8641\n",
      "   Epoch 30/100 | Train Loss: 0.3261 | Val Loss: 0.7108 | Val Corr: 0.5464 | Val RMSE: 0.9305\n",
      "   Epoch 40/100 | Train Loss: 0.3144 | Val Loss: 0.6688 | Val Corr: 0.5600 | Val RMSE: 0.8828\n",
      "   Epoch 50/100 | Train Loss: 0.2871 | Val Loss: 0.6692 | Val Corr: 0.5692 | Val RMSE: 0.8810\n",
      "   Epoch 60/100 | Train Loss: 0.2858 | Val Loss: 0.6355 | Val Corr: 0.5743 | Val RMSE: 0.8430\n",
      "   Epoch 70/100 | Train Loss: 0.2511 | Val Loss: 0.6475 | Val Corr: 0.5709 | Val RMSE: 0.8548\n",
      "Early stopping at epoch 71 (best epoch 56, best corr 0.5904)\n",
      "BEST epoch = 56 | BEST Corr = 0.5904 | RMSE@BEST = 0.8358\n",
      "----------------------------------------\n",
      "   [Saved] sorghum_T5000_LD0.4.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.6 | Species: sorghum | Feature Count: T5000\n",
      "      Processing Trait: HT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.7487 | Val Loss: 0.7630 | Val Corr: 0.3581 | Val RMSE: 0.9380\n",
      "   Epoch 10/100 | Train Loss: 0.5886 | Val Loss: 0.6597 | Val Corr: 0.5100 | Val RMSE: 0.8296\n",
      "   Epoch 20/100 | Train Loss: 0.4513 | Val Loss: 0.6688 | Val Corr: 0.5110 | Val RMSE: 0.8283\n",
      "Early stopping at epoch 26 (best epoch 11, best corr 0.5365)\n",
      "BEST epoch = 11 | BEST Corr = 0.5365 | RMSE@BEST = 0.8921\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7871 | Val Loss: 0.7810 | Val Corr: 0.4222 | Val RMSE: 1.0748\n",
      "   Epoch 10/100 | Train Loss: 0.5375 | Val Loss: 0.6602 | Val Corr: 0.4585 | Val RMSE: 0.9729\n",
      "   Epoch 20/100 | Train Loss: 0.4143 | Val Loss: 0.6948 | Val Corr: 0.4910 | Val RMSE: 0.9366\n",
      "   Epoch 30/100 | Train Loss: 0.3794 | Val Loss: 0.6814 | Val Corr: 0.5127 | Val RMSE: 0.9210\n",
      "Early stopping at epoch 33 (best epoch 18, best corr 0.5291)\n",
      "BEST epoch = 18 | BEST Corr = 0.5291 | RMSE@BEST = 0.9285\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7713 | Val Loss: 0.9427 | Val Corr: 0.2580 | Val RMSE: 1.1192\n",
      "   Epoch 10/100 | Train Loss: 0.5854 | Val Loss: 0.7624 | Val Corr: 0.5281 | Val RMSE: 0.9399\n",
      "   Epoch 20/100 | Train Loss: 0.4671 | Val Loss: 0.6156 | Val Corr: 0.5532 | Val RMSE: 0.8357\n",
      "   Epoch 30/100 | Train Loss: 0.3808 | Val Loss: 0.6919 | Val Corr: 0.5637 | Val RMSE: 0.8695\n",
      "Early stopping at epoch 36 (best epoch 21, best corr 0.5677)\n",
      "BEST epoch = 21 | BEST Corr = 0.5677 | RMSE@BEST = 0.8619\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7464 | Val Loss: 0.8111 | Val Corr: 0.3528 | Val RMSE: 1.0024\n",
      "   Epoch 10/100 | Train Loss: 0.5862 | Val Loss: 0.7102 | Val Corr: 0.4902 | Val RMSE: 0.8844\n",
      "   Epoch 20/100 | Train Loss: 0.4643 | Val Loss: 0.7109 | Val Corr: 0.4826 | Val RMSE: 0.8887\n",
      "   Epoch 30/100 | Train Loss: 0.3771 | Val Loss: 0.7067 | Val Corr: 0.5419 | Val RMSE: 0.8601\n",
      "   Epoch 40/100 | Train Loss: 0.3613 | Val Loss: 0.6893 | Val Corr: 0.5519 | Val RMSE: 0.8746\n",
      "   Epoch 50/100 | Train Loss: 0.3137 | Val Loss: 0.6923 | Val Corr: 0.5091 | Val RMSE: 0.8902\n",
      "   Epoch 60/100 | Train Loss: 0.2855 | Val Loss: 0.6919 | Val Corr: 0.5275 | Val RMSE: 0.8847\n",
      "Early stopping at epoch 61 (best epoch 46, best corr 0.5665)\n",
      "BEST epoch = 46 | BEST Corr = 0.5665 | RMSE@BEST = 0.8459\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7815 | Val Loss: 0.7649 | Val Corr: 0.4355 | Val RMSE: 0.9724\n",
      "   Epoch 10/100 | Train Loss: 0.5633 | Val Loss: 0.7423 | Val Corr: 0.5656 | Val RMSE: 0.9330\n",
      "   Epoch 20/100 | Train Loss: 0.4307 | Val Loss: 0.6726 | Val Corr: 0.5608 | Val RMSE: 0.8428\n",
      "   Epoch 30/100 | Train Loss: 0.3851 | Val Loss: 0.6451 | Val Corr: 0.5840 | Val RMSE: 0.8113\n",
      "   Epoch 40/100 | Train Loss: 0.3234 | Val Loss: 0.6426 | Val Corr: 0.5905 | Val RMSE: 0.8088\n",
      "Early stopping at epoch 43 (best epoch 28, best corr 0.6010)\n",
      "BEST epoch = 28 | BEST Corr = 0.6010 | RMSE@BEST = 0.8029\n",
      "----------------------------------------\n",
      "      Processing Trait: MO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.7716 | Val Loss: 0.8589 | Val Corr: 0.1835 | Val RMSE: 1.0669\n",
      "   Epoch 10/100 | Train Loss: 0.6348 | Val Loss: 0.9032 | Val Corr: 0.2131 | Val RMSE: 1.1106\n",
      "   Epoch 20/100 | Train Loss: 0.4801 | Val Loss: 0.8429 | Val Corr: 0.1856 | Val RMSE: 1.0668\n",
      "Early stopping at epoch 21 (best epoch 6, best corr 0.2840)\n",
      "BEST epoch = 6 | BEST Corr = 0.2840 | RMSE@BEST = 1.0413\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7989 | Val Loss: 0.6819 | Val Corr: 0.4119 | Val RMSE: 0.8445\n",
      "   Epoch 10/100 | Train Loss: 0.5938 | Val Loss: 0.7169 | Val Corr: 0.3465 | Val RMSE: 0.8445\n",
      "Early stopping at epoch 18 (best epoch 3, best corr 0.4542)\n",
      "BEST epoch = 3 | BEST Corr = 0.4542 | RMSE@BEST = 0.7718\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7832 | Val Loss: 0.7318 | Val Corr: 0.3419 | Val RMSE: 0.9140\n",
      "   Epoch 10/100 | Train Loss: 0.6301 | Val Loss: 0.6818 | Val Corr: 0.4589 | Val RMSE: 0.8696\n",
      "   Epoch 20/100 | Train Loss: 0.5061 | Val Loss: 0.6402 | Val Corr: 0.4264 | Val RMSE: 0.8426\n",
      "Early stopping at epoch 22 (best epoch 7, best corr 0.4997)\n",
      "BEST epoch = 7 | BEST Corr = 0.4997 | RMSE@BEST = 0.8308\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7781 | Val Loss: 0.7501 | Val Corr: 0.1595 | Val RMSE: 0.9506\n",
      "   Epoch 10/100 | Train Loss: 0.6052 | Val Loss: 0.8207 | Val Corr: 0.2661 | Val RMSE: 1.0288\n",
      "   Epoch 20/100 | Train Loss: 0.4995 | Val Loss: 0.8097 | Val Corr: 0.3497 | Val RMSE: 1.0504\n",
      "   Epoch 30/100 | Train Loss: 0.4512 | Val Loss: 0.8081 | Val Corr: 0.3375 | Val RMSE: 1.0130\n",
      "Early stopping at epoch 36 (best epoch 21, best corr 0.3664)\n",
      "BEST epoch = 21 | BEST Corr = 0.3664 | RMSE@BEST = 0.9815\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7585 | Val Loss: 0.9158 | Val Corr: 0.3116 | Val RMSE: 1.1888\n",
      "   Epoch 10/100 | Train Loss: 0.5677 | Val Loss: 0.8290 | Val Corr: 0.3680 | Val RMSE: 1.1280\n",
      "   Epoch 20/100 | Train Loss: 0.4923 | Val Loss: 0.7650 | Val Corr: 0.4314 | Val RMSE: 1.0773\n",
      "   Epoch 30/100 | Train Loss: 0.3712 | Val Loss: 0.7964 | Val Corr: 0.4616 | Val RMSE: 1.0799\n",
      "   Epoch 40/100 | Train Loss: 0.3839 | Val Loss: 0.7801 | Val Corr: 0.4386 | Val RMSE: 1.0695\n",
      "Early stopping at epoch 41 (best epoch 26, best corr 0.4813)\n",
      "BEST epoch = 26 | BEST Corr = 0.4813 | RMSE@BEST = 1.0522\n",
      "----------------------------------------\n",
      "      Processing Trait: YLD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.7831 | Val Loss: 0.8556 | Val Corr: 0.4391 | Val RMSE: 1.0261\n",
      "   Epoch 10/100 | Train Loss: 0.5694 | Val Loss: 0.7047 | Val Corr: 0.5399 | Val RMSE: 0.9068\n",
      "   Epoch 20/100 | Train Loss: 0.4328 | Val Loss: 0.7006 | Val Corr: 0.5594 | Val RMSE: 0.9045\n",
      "   Epoch 30/100 | Train Loss: 0.3695 | Val Loss: 0.7714 | Val Corr: 0.5315 | Val RMSE: 1.0047\n",
      "Early stopping at epoch 38 (best epoch 23, best corr 0.5752)\n",
      "BEST epoch = 23 | BEST Corr = 0.5752 | RMSE@BEST = 0.9583\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7772 | Val Loss: 0.8908 | Val Corr: 0.4924 | Val RMSE: 1.0372\n",
      "   Epoch 10/100 | Train Loss: 0.5439 | Val Loss: 0.7443 | Val Corr: 0.5200 | Val RMSE: 0.9105\n",
      "   Epoch 20/100 | Train Loss: 0.4310 | Val Loss: 0.7574 | Val Corr: 0.5461 | Val RMSE: 0.9268\n",
      "   Epoch 30/100 | Train Loss: 0.3643 | Val Loss: 0.7284 | Val Corr: 0.5559 | Val RMSE: 0.9062\n",
      "Early stopping at epoch 40 (best epoch 25, best corr 0.5804)\n",
      "BEST epoch = 25 | BEST Corr = 0.5804 | RMSE@BEST = 0.9104\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8428 | Val Loss: 0.7588 | Val Corr: 0.4596 | Val RMSE: 0.9695\n",
      "   Epoch 10/100 | Train Loss: 0.5556 | Val Loss: 0.7225 | Val Corr: 0.5013 | Val RMSE: 0.8997\n",
      "   Epoch 20/100 | Train Loss: 0.4466 | Val Loss: 0.7640 | Val Corr: 0.5556 | Val RMSE: 0.9511\n",
      "   Epoch 30/100 | Train Loss: 0.3671 | Val Loss: 0.7106 | Val Corr: 0.5582 | Val RMSE: 0.8814\n",
      "Early stopping at epoch 37 (best epoch 22, best corr 0.5793)\n",
      "BEST epoch = 22 | BEST Corr = 0.5793 | RMSE@BEST = 0.8135\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8148 | Val Loss: 0.7428 | Val Corr: 0.4369 | Val RMSE: 0.9494\n",
      "   Epoch 10/100 | Train Loss: 0.5253 | Val Loss: 0.7827 | Val Corr: 0.5613 | Val RMSE: 0.9985\n",
      "   Epoch 20/100 | Train Loss: 0.4317 | Val Loss: 0.8096 | Val Corr: 0.5744 | Val RMSE: 1.0100\n",
      "   Epoch 30/100 | Train Loss: 0.3887 | Val Loss: 0.7975 | Val Corr: 0.5896 | Val RMSE: 0.9914\n",
      "   Epoch 40/100 | Train Loss: 0.3214 | Val Loss: 0.6780 | Val Corr: 0.5887 | Val RMSE: 0.8788\n",
      "Early stopping at epoch 46 (best epoch 31, best corr 0.6095)\n",
      "BEST epoch = 31 | BEST Corr = 0.6095 | RMSE@BEST = 0.9362\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7834 | Val Loss: 0.8201 | Val Corr: 0.4056 | Val RMSE: 1.0014\n",
      "   Epoch 10/100 | Train Loss: 0.5346 | Val Loss: 0.6728 | Val Corr: 0.5726 | Val RMSE: 0.8487\n",
      "   Epoch 20/100 | Train Loss: 0.4263 | Val Loss: 0.6700 | Val Corr: 0.6009 | Val RMSE: 0.8739\n",
      "   Epoch 30/100 | Train Loss: 0.3385 | Val Loss: 0.6871 | Val Corr: 0.5783 | Val RMSE: 0.8745\n",
      "Early stopping at epoch 37 (best epoch 22, best corr 0.6170)\n",
      "BEST epoch = 22 | BEST Corr = 0.6170 | RMSE@BEST = 0.9125\n",
      "----------------------------------------\n",
      "   [Saved] sorghum_T5000_LD0.6.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.8 | Species: sorghum | Feature Count: T5000\n",
      "      Processing Trait: HT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.7587 | Val Loss: 0.7654 | Val Corr: 0.3631 | Val RMSE: 0.9385\n",
      "   Epoch 10/100 | Train Loss: 0.5969 | Val Loss: 0.6807 | Val Corr: 0.5225 | Val RMSE: 0.8542\n",
      "   Epoch 20/100 | Train Loss: 0.4597 | Val Loss: 0.6476 | Val Corr: 0.4963 | Val RMSE: 0.8227\n",
      "Early stopping at epoch 21 (best epoch 6, best corr 0.5263)\n",
      "BEST epoch = 6 | BEST Corr = 0.5263 | RMSE@BEST = 0.8045\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7769 | Val Loss: 0.7801 | Val Corr: 0.4261 | Val RMSE: 1.0718\n",
      "   Epoch 10/100 | Train Loss: 0.5500 | Val Loss: 0.6806 | Val Corr: 0.4634 | Val RMSE: 0.9967\n",
      "   Epoch 20/100 | Train Loss: 0.4175 | Val Loss: 0.7134 | Val Corr: 0.5081 | Val RMSE: 1.0171\n",
      "   Epoch 30/100 | Train Loss: 0.3506 | Val Loss: 0.6545 | Val Corr: 0.5417 | Val RMSE: 0.9132\n",
      "   Epoch 40/100 | Train Loss: 0.3331 | Val Loss: 0.6730 | Val Corr: 0.5217 | Val RMSE: 0.9499\n",
      "   Epoch 50/100 | Train Loss: 0.3117 | Val Loss: 0.6752 | Val Corr: 0.5133 | Val RMSE: 0.9324\n",
      "Early stopping at epoch 57 (best epoch 42, best corr 0.5475)\n",
      "BEST epoch = 42 | BEST Corr = 0.5475 | RMSE@BEST = 0.9203\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7765 | Val Loss: 0.8996 | Val Corr: 0.2745 | Val RMSE: 1.0701\n",
      "   Epoch 10/100 | Train Loss: 0.5661 | Val Loss: 0.6401 | Val Corr: 0.5442 | Val RMSE: 0.8521\n",
      "   Epoch 20/100 | Train Loss: 0.4585 | Val Loss: 0.6355 | Val Corr: 0.5754 | Val RMSE: 0.8487\n",
      "   Epoch 30/100 | Train Loss: 0.3850 | Val Loss: 0.7276 | Val Corr: 0.5552 | Val RMSE: 0.8965\n",
      "Early stopping at epoch 36 (best epoch 21, best corr 0.5809)\n",
      "BEST epoch = 21 | BEST Corr = 0.5809 | RMSE@BEST = 0.8192\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7570 | Val Loss: 0.8076 | Val Corr: 0.3428 | Val RMSE: 1.0015\n",
      "   Epoch 10/100 | Train Loss: 0.5567 | Val Loss: 0.7209 | Val Corr: 0.5013 | Val RMSE: 0.8961\n",
      "   Epoch 20/100 | Train Loss: 0.4604 | Val Loss: 0.7436 | Val Corr: 0.5143 | Val RMSE: 0.9332\n",
      "   Epoch 30/100 | Train Loss: 0.3790 | Val Loss: 0.6876 | Val Corr: 0.5561 | Val RMSE: 0.8584\n",
      "   Epoch 40/100 | Train Loss: 0.3441 | Val Loss: 0.7372 | Val Corr: 0.5215 | Val RMSE: 0.8950\n",
      "Early stopping at epoch 43 (best epoch 28, best corr 0.5791)\n",
      "BEST epoch = 28 | BEST Corr = 0.5791 | RMSE@BEST = 0.8339\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7775 | Val Loss: 0.7716 | Val Corr: 0.4145 | Val RMSE: 0.9749\n",
      "   Epoch 10/100 | Train Loss: 0.5631 | Val Loss: 0.6351 | Val Corr: 0.5851 | Val RMSE: 0.7926\n",
      "   Epoch 20/100 | Train Loss: 0.4304 | Val Loss: 0.6700 | Val Corr: 0.5253 | Val RMSE: 0.8396\n",
      "   Epoch 30/100 | Train Loss: 0.3835 | Val Loss: 0.6813 | Val Corr: 0.5147 | Val RMSE: 0.8498\n",
      "Early stopping at epoch 32 (best epoch 17, best corr 0.5887)\n",
      "BEST epoch = 17 | BEST Corr = 0.5887 | RMSE@BEST = 0.8261\n",
      "----------------------------------------\n",
      "      Processing Trait: MO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.7679 | Val Loss: 0.8459 | Val Corr: 0.1492 | Val RMSE: 1.0589\n",
      "   Epoch 10/100 | Train Loss: 0.6178 | Val Loss: 0.8470 | Val Corr: 0.2188 | Val RMSE: 1.0608\n",
      "   Epoch 20/100 | Train Loss: 0.4776 | Val Loss: 0.8292 | Val Corr: 0.2276 | Val RMSE: 1.0467\n",
      "Early stopping at epoch 21 (best epoch 6, best corr 0.2783)\n",
      "BEST epoch = 6 | BEST Corr = 0.2783 | RMSE@BEST = 1.0994\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7981 | Val Loss: 0.6824 | Val Corr: 0.4106 | Val RMSE: 0.8446\n",
      "   Epoch 10/100 | Train Loss: 0.6019 | Val Loss: 0.6747 | Val Corr: 0.4387 | Val RMSE: 0.8171\n",
      "   Epoch 20/100 | Train Loss: 0.4813 | Val Loss: 0.7190 | Val Corr: 0.4084 | Val RMSE: 0.8767\n",
      "Early stopping at epoch 24 (best epoch 9, best corr 0.4507)\n",
      "BEST epoch = 9 | BEST Corr = 0.4507 | RMSE@BEST = 0.8118\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7820 | Val Loss: 0.7526 | Val Corr: 0.3653 | Val RMSE: 0.9482\n",
      "   Epoch 10/100 | Train Loss: 0.6203 | Val Loss: 0.6416 | Val Corr: 0.4350 | Val RMSE: 0.8211\n",
      "   Epoch 20/100 | Train Loss: 0.4714 | Val Loss: 0.6996 | Val Corr: 0.3912 | Val RMSE: 0.8824\n",
      "Early stopping at epoch 27 (best epoch 12, best corr 0.4912)\n",
      "BEST epoch = 12 | BEST Corr = 0.4912 | RMSE@BEST = 0.8099\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7762 | Val Loss: 0.7489 | Val Corr: 0.1002 | Val RMSE: 0.9524\n",
      "   Epoch 10/100 | Train Loss: 0.6046 | Val Loss: 0.7069 | Val Corr: 0.2373 | Val RMSE: 0.9723\n",
      "   Epoch 20/100 | Train Loss: 0.4973 | Val Loss: 0.6754 | Val Corr: 0.3706 | Val RMSE: 0.9072\n",
      "   Epoch 30/100 | Train Loss: 0.4496 | Val Loss: 0.6718 | Val Corr: 0.3678 | Val RMSE: 0.9063\n",
      "Early stopping at epoch 35 (best epoch 20, best corr 0.3706)\n",
      "BEST epoch = 20 | BEST Corr = 0.3706 | RMSE@BEST = 0.9072\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7630 | Val Loss: 0.9151 | Val Corr: 0.4022 | Val RMSE: 1.1881\n",
      "   Epoch 10/100 | Train Loss: 0.5505 | Val Loss: 0.8004 | Val Corr: 0.4283 | Val RMSE: 1.0763\n",
      "   Epoch 20/100 | Train Loss: 0.4446 | Val Loss: 0.7960 | Val Corr: 0.4602 | Val RMSE: 1.0615\n",
      "   Epoch 30/100 | Train Loss: 0.3553 | Val Loss: 0.7671 | Val Corr: 0.4986 | Val RMSE: 1.0384\n",
      "   Epoch 40/100 | Train Loss: 0.3616 | Val Loss: 0.7606 | Val Corr: 0.5058 | Val RMSE: 1.0274\n",
      "   Epoch 50/100 | Train Loss: 0.3262 | Val Loss: 0.7818 | Val Corr: 0.4992 | Val RMSE: 1.0423\n",
      "Early stopping at epoch 57 (best epoch 42, best corr 0.5324)\n",
      "BEST epoch = 42 | BEST Corr = 0.5324 | RMSE@BEST = 1.0166\n",
      "----------------------------------------\n",
      "      Processing Trait: YLD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.7934 | Val Loss: 0.8573 | Val Corr: 0.4363 | Val RMSE: 1.0268\n",
      "   Epoch 10/100 | Train Loss: 0.5635 | Val Loss: 0.9385 | Val Corr: 0.5499 | Val RMSE: 1.1682\n",
      "   Epoch 20/100 | Train Loss: 0.4721 | Val Loss: 0.7158 | Val Corr: 0.5822 | Val RMSE: 0.9173\n",
      "Early stopping at epoch 28 (best epoch 13, best corr 0.6254)\n",
      "BEST epoch = 13 | BEST Corr = 0.6254 | RMSE@BEST = 0.9097\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7721 | Val Loss: 0.8921 | Val Corr: 0.4747 | Val RMSE: 1.0365\n",
      "   Epoch 10/100 | Train Loss: 0.5233 | Val Loss: 0.6799 | Val Corr: 0.5892 | Val RMSE: 0.8495\n",
      "   Epoch 20/100 | Train Loss: 0.4021 | Val Loss: 0.7211 | Val Corr: 0.5630 | Val RMSE: 0.9192\n",
      "   Epoch 30/100 | Train Loss: 0.3338 | Val Loss: 0.6857 | Val Corr: 0.5799 | Val RMSE: 0.8574\n",
      "Early stopping at epoch 38 (best epoch 23, best corr 0.5935)\n",
      "BEST epoch = 23 | BEST Corr = 0.5935 | RMSE@BEST = 0.8472\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8495 | Val Loss: 0.7581 | Val Corr: 0.4337 | Val RMSE: 0.9666\n",
      "   Epoch 10/100 | Train Loss: 0.5651 | Val Loss: 0.6998 | Val Corr: 0.5769 | Val RMSE: 0.8453\n",
      "   Epoch 20/100 | Train Loss: 0.4386 | Val Loss: 0.6521 | Val Corr: 0.5654 | Val RMSE: 0.8086\n",
      "Early stopping at epoch 29 (best epoch 14, best corr 0.6192)\n",
      "BEST epoch = 14 | BEST Corr = 0.6192 | RMSE@BEST = 0.7649\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8071 | Val Loss: 0.7410 | Val Corr: 0.4363 | Val RMSE: 0.9463\n",
      "   Epoch 10/100 | Train Loss: 0.5440 | Val Loss: 0.9042 | Val Corr: 0.5427 | Val RMSE: 1.1251\n",
      "   Epoch 20/100 | Train Loss: 0.4142 | Val Loss: 0.9284 | Val Corr: 0.5479 | Val RMSE: 1.1556\n",
      "   Epoch 30/100 | Train Loss: 0.4025 | Val Loss: 0.9757 | Val Corr: 0.5765 | Val RMSE: 1.1843\n",
      "   Epoch 40/100 | Train Loss: 0.3327 | Val Loss: 0.8102 | Val Corr: 0.5930 | Val RMSE: 1.0372\n",
      "   Epoch 50/100 | Train Loss: 0.2832 | Val Loss: 0.7434 | Val Corr: 0.5900 | Val RMSE: 0.9648\n",
      "Early stopping at epoch 55 (best epoch 40, best corr 0.5930)\n",
      "BEST epoch = 40 | BEST Corr = 0.5930 | RMSE@BEST = 1.0372\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7900 | Val Loss: 0.8202 | Val Corr: 0.3864 | Val RMSE: 1.0061\n",
      "   Epoch 10/100 | Train Loss: 0.5134 | Val Loss: 0.6674 | Val Corr: 0.5479 | Val RMSE: 0.8537\n",
      "   Epoch 20/100 | Train Loss: 0.4036 | Val Loss: 0.7239 | Val Corr: 0.5828 | Val RMSE: 0.9720\n",
      "   Epoch 30/100 | Train Loss: 0.3351 | Val Loss: 0.7209 | Val Corr: 0.5572 | Val RMSE: 0.9450\n",
      "Early stopping at epoch 31 (best epoch 16, best corr 0.5929)\n",
      "BEST epoch = 16 | BEST Corr = 0.5929 | RMSE@BEST = 0.8615\n",
      "----------------------------------------\n",
      "   [Saved] sorghum_T5000_LD0.8.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.2 | Species: bulls | Feature Count: T5000\n",
      "      Processing Trait: MS\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7995 | Val Loss: 0.8191 | Val Corr: -0.0167 | Val RMSE: 1.0663\n",
      "   Epoch 10/100 | Train Loss: 0.6541 | Val Loss: 0.7831 | Val Corr: 0.3398 | Val RMSE: 0.9842\n",
      "   Epoch 20/100 | Train Loss: 0.5009 | Val Loss: 0.8505 | Val Corr: 0.3781 | Val RMSE: 1.0863\n",
      "   Epoch 30/100 | Train Loss: 0.4230 | Val Loss: 0.7828 | Val Corr: 0.3866 | Val RMSE: 0.9945\n",
      "   Epoch 40/100 | Train Loss: 0.3592 | Val Loss: 0.7751 | Val Corr: 0.3988 | Val RMSE: 0.9983\n",
      "   Epoch 50/100 | Train Loss: 0.3232 | Val Loss: 0.7657 | Val Corr: 0.4109 | Val RMSE: 0.9944\n",
      "   Epoch 60/100 | Train Loss: 0.2811 | Val Loss: 0.7667 | Val Corr: 0.4023 | Val RMSE: 0.9949\n",
      "Early stopping at epoch 62 (best epoch 47, best corr 0.4202)\n",
      "BEST epoch = 47 | BEST Corr = 0.4202 | RMSE@BEST = 1.0227\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.8114 | Val Loss: 0.7666 | Val Corr: 0.1490 | Val RMSE: 0.9866\n",
      "   Epoch 10/100 | Train Loss: 0.6774 | Val Loss: 0.7291 | Val Corr: 0.4453 | Val RMSE: 0.9156\n",
      "   Epoch 20/100 | Train Loss: 0.5412 | Val Loss: 0.6814 | Val Corr: 0.4888 | Val RMSE: 0.8747\n",
      "   Epoch 30/100 | Train Loss: 0.4324 | Val Loss: 0.6633 | Val Corr: 0.4890 | Val RMSE: 0.8666\n",
      "   Epoch 40/100 | Train Loss: 0.3784 | Val Loss: 0.6682 | Val Corr: 0.4866 | Val RMSE: 0.8706\n",
      "   Epoch 50/100 | Train Loss: 0.3441 | Val Loss: 0.6842 | Val Corr: 0.4797 | Val RMSE: 0.8869\n",
      "Early stopping at epoch 53 (best epoch 38, best corr 0.5129)\n",
      "BEST epoch = 38 | BEST Corr = 0.5129 | RMSE@BEST = 0.8588\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7920 | Val Loss: 0.7671 | Val Corr: 0.2575 | Val RMSE: 0.9603\n",
      "   Epoch 10/100 | Train Loss: 0.6318 | Val Loss: 0.6951 | Val Corr: 0.4417 | Val RMSE: 0.8675\n",
      "   Epoch 20/100 | Train Loss: 0.5022 | Val Loss: 0.7236 | Val Corr: 0.4224 | Val RMSE: 0.8933\n",
      "Early stopping at epoch 29 (best epoch 14, best corr 0.4688)\n",
      "BEST epoch = 14 | BEST Corr = 0.4688 | RMSE@BEST = 0.8895\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.8021 | Val Loss: 0.7634 | Val Corr: 0.0982 | Val RMSE: 0.9907\n",
      "   Epoch 10/100 | Train Loss: 0.6616 | Val Loss: 0.8153 | Val Corr: 0.3439 | Val RMSE: 1.0955\n",
      "   Epoch 20/100 | Train Loss: 0.5014 | Val Loss: 0.7234 | Val Corr: 0.4091 | Val RMSE: 0.9338\n",
      "   Epoch 30/100 | Train Loss: 0.4245 | Val Loss: 0.6948 | Val Corr: 0.4166 | Val RMSE: 0.9267\n",
      "   Epoch 40/100 | Train Loss: 0.3564 | Val Loss: 0.7012 | Val Corr: 0.3758 | Val RMSE: 0.9374\n",
      "Early stopping at epoch 41 (best epoch 26, best corr 0.4464)\n",
      "BEST epoch = 26 | BEST Corr = 0.4464 | RMSE@BEST = 0.8994\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7934 | Val Loss: 0.7837 | Val Corr: 0.0809 | Val RMSE: 0.9918\n",
      "   Epoch 10/100 | Train Loss: 0.6032 | Val Loss: 0.7913 | Val Corr: 0.2489 | Val RMSE: 1.0014\n",
      "   Epoch 20/100 | Train Loss: 0.4722 | Val Loss: 0.8087 | Val Corr: 0.2970 | Val RMSE: 1.0085\n",
      "   Epoch 30/100 | Train Loss: 0.3981 | Val Loss: 0.7890 | Val Corr: 0.3042 | Val RMSE: 0.9797\n",
      "Early stopping at epoch 38 (best epoch 23, best corr 0.3270)\n",
      "BEST epoch = 23 | BEST Corr = 0.3270 | RMSE@BEST = 1.0344\n",
      "----------------------------------------\n",
      "      Processing Trait: NMSP\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.8132 | Val Loss: 0.7611 | Val Corr: 0.2212 | Val RMSE: 0.9268\n",
      "   Epoch 10/100 | Train Loss: 0.6444 | Val Loss: 0.6472 | Val Corr: 0.4473 | Val RMSE: 0.8384\n",
      "   Epoch 20/100 | Train Loss: 0.5034 | Val Loss: 0.6005 | Val Corr: 0.5591 | Val RMSE: 0.7777\n",
      "   Epoch 30/100 | Train Loss: 0.4325 | Val Loss: 0.5790 | Val Corr: 0.5796 | Val RMSE: 0.7606\n",
      "   Epoch 40/100 | Train Loss: 0.3964 | Val Loss: 0.5946 | Val Corr: 0.5738 | Val RMSE: 0.7984\n",
      "   Epoch 50/100 | Train Loss: 0.3436 | Val Loss: 0.5909 | Val Corr: 0.5650 | Val RMSE: 0.7808\n",
      "   Epoch 60/100 | Train Loss: 0.3245 | Val Loss: 0.5921 | Val Corr: 0.5733 | Val RMSE: 0.7696\n",
      "   Epoch 70/100 | Train Loss: 0.2936 | Val Loss: 0.5890 | Val Corr: 0.5746 | Val RMSE: 0.7835\n",
      "Early stopping at epoch 71 (best epoch 56, best corr 0.5980)\n",
      "BEST epoch = 56 | BEST Corr = 0.5980 | RMSE@BEST = 0.7487\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7892 | Val Loss: 0.7988 | Val Corr: 0.0853 | Val RMSE: 1.0460\n",
      "   Epoch 10/100 | Train Loss: 0.6131 | Val Loss: 0.7335 | Val Corr: 0.4144 | Val RMSE: 0.9608\n",
      "   Epoch 20/100 | Train Loss: 0.4988 | Val Loss: 0.6881 | Val Corr: 0.5038 | Val RMSE: 0.9295\n",
      "   Epoch 30/100 | Train Loss: 0.4174 | Val Loss: 0.6725 | Val Corr: 0.5271 | Val RMSE: 0.8987\n",
      "   Epoch 40/100 | Train Loss: 0.3546 | Val Loss: 0.6568 | Val Corr: 0.5619 | Val RMSE: 0.8993\n",
      "   Epoch 50/100 | Train Loss: 0.3282 | Val Loss: 0.6654 | Val Corr: 0.5473 | Val RMSE: 0.9059\n",
      "Early stopping at epoch 53 (best epoch 38, best corr 0.5785)\n",
      "BEST epoch = 38 | BEST Corr = 0.5785 | RMSE@BEST = 0.8626\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7588 | Val Loss: 0.7832 | Val Corr: 0.2612 | Val RMSE: 1.0703\n",
      "   Epoch 10/100 | Train Loss: 0.5811 | Val Loss: 0.6797 | Val Corr: 0.4345 | Val RMSE: 0.9364\n",
      "   Epoch 20/100 | Train Loss: 0.4563 | Val Loss: 0.6803 | Val Corr: 0.5016 | Val RMSE: 0.9310\n",
      "   Epoch 30/100 | Train Loss: 0.3904 | Val Loss: 0.6495 | Val Corr: 0.4999 | Val RMSE: 0.8931\n",
      "Early stopping at epoch 32 (best epoch 17, best corr 0.5310)\n",
      "BEST epoch = 17 | BEST Corr = 0.5310 | RMSE@BEST = 0.8810\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7850 | Val Loss: 0.7513 | Val Corr: 0.1674 | Val RMSE: 1.0039\n",
      "   Epoch 10/100 | Train Loss: 0.6445 | Val Loss: 0.6952 | Val Corr: 0.4230 | Val RMSE: 0.9539\n",
      "   Epoch 20/100 | Train Loss: 0.4957 | Val Loss: 0.6811 | Val Corr: 0.4707 | Val RMSE: 0.9434\n",
      "   Epoch 30/100 | Train Loss: 0.4129 | Val Loss: 0.6561 | Val Corr: 0.5014 | Val RMSE: 0.8904\n",
      "   Epoch 40/100 | Train Loss: 0.3778 | Val Loss: 0.6512 | Val Corr: 0.4977 | Val RMSE: 0.9059\n",
      "Early stopping at epoch 43 (best epoch 28, best corr 0.5358)\n",
      "BEST epoch = 28 | BEST Corr = 0.5358 | RMSE@BEST = 0.8572\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.8218 | Val Loss: 0.7771 | Val Corr: 0.2267 | Val RMSE: 1.0115\n",
      "   Epoch 10/100 | Train Loss: 0.6115 | Val Loss: 0.7650 | Val Corr: 0.2628 | Val RMSE: 1.0483\n",
      "   Epoch 20/100 | Train Loss: 0.4415 | Val Loss: 0.7475 | Val Corr: 0.2846 | Val RMSE: 1.0279\n",
      "   Epoch 30/100 | Train Loss: 0.3996 | Val Loss: 0.7457 | Val Corr: 0.3211 | Val RMSE: 0.9849\n",
      "   Epoch 40/100 | Train Loss: 0.3399 | Val Loss: 0.7411 | Val Corr: 0.3405 | Val RMSE: 0.9936\n",
      "   Epoch 50/100 | Train Loss: 0.3249 | Val Loss: 0.7386 | Val Corr: 0.3313 | Val RMSE: 0.9890\n",
      "Early stopping at epoch 54 (best epoch 39, best corr 0.3509)\n",
      "BEST epoch = 39 | BEST Corr = 0.3509 | RMSE@BEST = 0.9834\n",
      "----------------------------------------\n",
      "      Processing Trait: VE\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.8125 | Val Loss: 0.7512 | Val Corr: 0.1495 | Val RMSE: 0.9739\n",
      "   Epoch 10/100 | Train Loss: 0.6503 | Val Loss: 0.7052 | Val Corr: 0.3457 | Val RMSE: 0.9163\n",
      "   Epoch 20/100 | Train Loss: 0.5175 | Val Loss: 0.7017 | Val Corr: 0.3989 | Val RMSE: 0.9120\n",
      "   Epoch 30/100 | Train Loss: 0.4594 | Val Loss: 0.7139 | Val Corr: 0.3994 | Val RMSE: 0.9336\n",
      "Early stopping at epoch 32 (best epoch 17, best corr 0.4336)\n",
      "BEST epoch = 17 | BEST Corr = 0.4336 | RMSE@BEST = 0.8888\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7899 | Val Loss: 0.7871 | Val Corr: 0.2738 | Val RMSE: 1.0427\n",
      "   Epoch 10/100 | Train Loss: 0.6005 | Val Loss: 0.6855 | Val Corr: 0.4961 | Val RMSE: 0.9224\n",
      "   Epoch 20/100 | Train Loss: 0.4913 | Val Loss: 0.6959 | Val Corr: 0.4871 | Val RMSE: 0.9416\n",
      "Early stopping at epoch 29 (best epoch 14, best corr 0.5382)\n",
      "BEST epoch = 14 | BEST Corr = 0.5382 | RMSE@BEST = 0.8960\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7919 | Val Loss: 0.7897 | Val Corr: 0.2627 | Val RMSE: 0.9561\n",
      "   Epoch 10/100 | Train Loss: 0.6458 | Val Loss: 0.7357 | Val Corr: 0.4278 | Val RMSE: 0.9116\n",
      "   Epoch 20/100 | Train Loss: 0.5154 | Val Loss: 0.6994 | Val Corr: 0.4366 | Val RMSE: 0.8842\n",
      "Early stopping at epoch 21 (best epoch 6, best corr 0.4789)\n",
      "BEST epoch = 6 | BEST Corr = 0.4789 | RMSE@BEST = 0.9341\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7934 | Val Loss: 0.8231 | Val Corr: 0.1874 | Val RMSE: 1.0630\n",
      "   Epoch 10/100 | Train Loss: 0.6524 | Val Loss: 0.7538 | Val Corr: 0.4239 | Val RMSE: 0.9496\n",
      "   Epoch 20/100 | Train Loss: 0.5423 | Val Loss: 0.7394 | Val Corr: 0.4565 | Val RMSE: 0.9364\n",
      "   Epoch 30/100 | Train Loss: 0.4645 | Val Loss: 0.7648 | Val Corr: 0.4229 | Val RMSE: 0.9999\n",
      "Early stopping at epoch 34 (best epoch 19, best corr 0.4661)\n",
      "BEST epoch = 19 | BEST Corr = 0.4661 | RMSE@BEST = 0.9461\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.8267 | Val Loss: 0.7068 | Val Corr: 0.2067 | Val RMSE: 0.9321\n",
      "   Epoch 10/100 | Train Loss: 0.6282 | Val Loss: 0.7260 | Val Corr: 0.3698 | Val RMSE: 0.9223\n",
      "   Epoch 20/100 | Train Loss: 0.4996 | Val Loss: 0.7100 | Val Corr: 0.3817 | Val RMSE: 0.9070\n",
      "Early stopping at epoch 22 (best epoch 7, best corr 0.4032)\n",
      "BEST epoch = 7 | BEST Corr = 0.4032 | RMSE@BEST = 0.8774\n",
      "----------------------------------------\n",
      "   [Saved] bulls_T5000_LD0.2.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.4 | Species: bulls | Feature Count: T5000\n",
      "      Processing Trait: MS\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7934 | Val Loss: 0.7971 | Val Corr: 0.1187 | Val RMSE: 1.0386\n",
      "   Epoch 10/100 | Train Loss: 0.6372 | Val Loss: 0.7830 | Val Corr: 0.3189 | Val RMSE: 0.9889\n",
      "   Epoch 20/100 | Train Loss: 0.5008 | Val Loss: 0.8199 | Val Corr: 0.3745 | Val RMSE: 1.0589\n",
      "   Epoch 30/100 | Train Loss: 0.4186 | Val Loss: 0.7847 | Val Corr: 0.3718 | Val RMSE: 0.9892\n",
      "Early stopping at epoch 40 (best epoch 25, best corr 0.3981)\n",
      "BEST epoch = 25 | BEST Corr = 0.3981 | RMSE@BEST = 0.9744\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.8185 | Val Loss: 0.7837 | Val Corr: 0.2096 | Val RMSE: 0.9918\n",
      "   Epoch 10/100 | Train Loss: 0.6527 | Val Loss: 0.7204 | Val Corr: 0.4270 | Val RMSE: 0.9064\n",
      "   Epoch 20/100 | Train Loss: 0.5260 | Val Loss: 0.6892 | Val Corr: 0.4434 | Val RMSE: 0.8943\n",
      "   Epoch 30/100 | Train Loss: 0.4368 | Val Loss: 0.6849 | Val Corr: 0.4431 | Val RMSE: 0.8962\n",
      "Early stopping at epoch 38 (best epoch 23, best corr 0.4852)\n",
      "BEST epoch = 23 | BEST Corr = 0.4852 | RMSE@BEST = 0.9032\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7998 | Val Loss: 0.7652 | Val Corr: 0.1607 | Val RMSE: 0.9568\n",
      "   Epoch 10/100 | Train Loss: 0.6459 | Val Loss: 0.7115 | Val Corr: 0.4052 | Val RMSE: 0.9032\n",
      "   Epoch 20/100 | Train Loss: 0.4981 | Val Loss: 0.7218 | Val Corr: 0.4483 | Val RMSE: 0.8851\n",
      "   Epoch 30/100 | Train Loss: 0.4069 | Val Loss: 0.7142 | Val Corr: 0.4159 | Val RMSE: 0.8956\n",
      "Early stopping at epoch 37 (best epoch 22, best corr 0.4544)\n",
      "BEST epoch = 22 | BEST Corr = 0.4544 | RMSE@BEST = 0.8572\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.8107 | Val Loss: 0.7645 | Val Corr: 0.1297 | Val RMSE: 0.9912\n",
      "   Epoch 10/100 | Train Loss: 0.6632 | Val Loss: 0.7082 | Val Corr: 0.3549 | Val RMSE: 0.9306\n",
      "   Epoch 20/100 | Train Loss: 0.5213 | Val Loss: 0.7195 | Val Corr: 0.3930 | Val RMSE: 0.9431\n",
      "   Epoch 30/100 | Train Loss: 0.4328 | Val Loss: 0.6999 | Val Corr: 0.4032 | Val RMSE: 0.9462\n",
      "   Epoch 40/100 | Train Loss: 0.3636 | Val Loss: 0.7202 | Val Corr: 0.4072 | Val RMSE: 0.9438\n",
      "   Epoch 50/100 | Train Loss: 0.3438 | Val Loss: 0.6973 | Val Corr: 0.4168 | Val RMSE: 0.9338\n",
      "   Epoch 60/100 | Train Loss: 0.3124 | Val Loss: 0.7093 | Val Corr: 0.4151 | Val RMSE: 0.9502\n",
      "Early stopping at epoch 67 (best epoch 52, best corr 0.4359)\n",
      "BEST epoch = 52 | BEST Corr = 0.4359 | RMSE@BEST = 0.9770\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7964 | Val Loss: 0.8013 | Val Corr: 0.0790 | Val RMSE: 1.0042\n",
      "   Epoch 10/100 | Train Loss: 0.5961 | Val Loss: 0.7987 | Val Corr: 0.2262 | Val RMSE: 1.0256\n",
      "   Epoch 20/100 | Train Loss: 0.4701 | Val Loss: 0.8004 | Val Corr: 0.2915 | Val RMSE: 1.0130\n",
      "   Epoch 30/100 | Train Loss: 0.3973 | Val Loss: 0.7689 | Val Corr: 0.2727 | Val RMSE: 1.0115\n",
      "Early stopping at epoch 39 (best epoch 24, best corr 0.3089)\n",
      "BEST epoch = 24 | BEST Corr = 0.3089 | RMSE@BEST = 1.0151\n",
      "----------------------------------------\n",
      "      Processing Trait: NMSP\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.8175 | Val Loss: 0.7593 | Val Corr: 0.2625 | Val RMSE: 0.9231\n",
      "   Epoch 10/100 | Train Loss: 0.6555 | Val Loss: 0.6709 | Val Corr: 0.4774 | Val RMSE: 0.8223\n",
      "   Epoch 20/100 | Train Loss: 0.5190 | Val Loss: 0.6051 | Val Corr: 0.5502 | Val RMSE: 0.7789\n",
      "   Epoch 30/100 | Train Loss: 0.4396 | Val Loss: 0.6071 | Val Corr: 0.5462 | Val RMSE: 0.7936\n",
      "Early stopping at epoch 36 (best epoch 21, best corr 0.5656)\n",
      "BEST epoch = 21 | BEST Corr = 0.5656 | RMSE@BEST = 0.7710\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7825 | Val Loss: 0.8152 | Val Corr: 0.0919 | Val RMSE: 1.0478\n",
      "   Epoch 10/100 | Train Loss: 0.6078 | Val Loss: 0.7057 | Val Corr: 0.4215 | Val RMSE: 0.9810\n",
      "   Epoch 20/100 | Train Loss: 0.4676 | Val Loss: 0.7692 | Val Corr: 0.4860 | Val RMSE: 1.0526\n",
      "   Epoch 30/100 | Train Loss: 0.4056 | Val Loss: 0.7335 | Val Corr: 0.4904 | Val RMSE: 1.0127\n",
      "   Epoch 40/100 | Train Loss: 0.3526 | Val Loss: 0.7088 | Val Corr: 0.5113 | Val RMSE: 0.9837\n",
      "   Epoch 50/100 | Train Loss: 0.3176 | Val Loss: 0.7159 | Val Corr: 0.5011 | Val RMSE: 0.9874\n",
      "   Epoch 60/100 | Train Loss: 0.2985 | Val Loss: 0.7198 | Val Corr: 0.5016 | Val RMSE: 1.0015\n",
      "Early stopping at epoch 64 (best epoch 49, best corr 0.5159)\n",
      "BEST epoch = 49 | BEST Corr = 0.5159 | RMSE@BEST = 0.9841\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7607 | Val Loss: 0.7645 | Val Corr: 0.2397 | Val RMSE: 1.0338\n",
      "   Epoch 10/100 | Train Loss: 0.5717 | Val Loss: 0.7721 | Val Corr: 0.4628 | Val RMSE: 1.0481\n",
      "   Epoch 20/100 | Train Loss: 0.4441 | Val Loss: 0.6948 | Val Corr: 0.4615 | Val RMSE: 0.9537\n",
      "   Epoch 30/100 | Train Loss: 0.3904 | Val Loss: 0.6296 | Val Corr: 0.5418 | Val RMSE: 0.8422\n",
      "   Epoch 40/100 | Train Loss: 0.3472 | Val Loss: 0.6434 | Val Corr: 0.5479 | Val RMSE: 0.8474\n",
      "   Epoch 50/100 | Train Loss: 0.2951 | Val Loss: 0.6230 | Val Corr: 0.5524 | Val RMSE: 0.8552\n",
      "Early stopping at epoch 57 (best epoch 42, best corr 0.5647)\n",
      "BEST epoch = 42 | BEST Corr = 0.5647 | RMSE@BEST = 0.8405\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7802 | Val Loss: 0.7600 | Val Corr: 0.1892 | Val RMSE: 0.9966\n",
      "   Epoch 10/100 | Train Loss: 0.6280 | Val Loss: 0.7503 | Val Corr: 0.4321 | Val RMSE: 0.9235\n",
      "   Epoch 20/100 | Train Loss: 0.4810 | Val Loss: 0.6654 | Val Corr: 0.4805 | Val RMSE: 0.8976\n",
      "   Epoch 30/100 | Train Loss: 0.3977 | Val Loss: 0.7055 | Val Corr: 0.5023 | Val RMSE: 0.8783\n",
      "   Epoch 40/100 | Train Loss: 0.3711 | Val Loss: 0.6718 | Val Corr: 0.4932 | Val RMSE: 0.8892\n",
      "   Epoch 50/100 | Train Loss: 0.3302 | Val Loss: 0.6767 | Val Corr: 0.5035 | Val RMSE: 0.8659\n",
      "Early stopping at epoch 51 (best epoch 36, best corr 0.5198)\n",
      "BEST epoch = 36 | BEST Corr = 0.5198 | RMSE@BEST = 0.8588\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.8124 | Val Loss: 0.8525 | Val Corr: 0.1285 | Val RMSE: 1.0438\n",
      "   Epoch 10/100 | Train Loss: 0.5839 | Val Loss: 0.7901 | Val Corr: 0.2998 | Val RMSE: 1.0711\n",
      "   Epoch 20/100 | Train Loss: 0.4675 | Val Loss: 0.7498 | Val Corr: 0.3138 | Val RMSE: 1.0025\n",
      "   Epoch 30/100 | Train Loss: 0.3835 | Val Loss: 0.7628 | Val Corr: 0.2988 | Val RMSE: 1.0216\n",
      "Early stopping at epoch 40 (best epoch 25, best corr 0.3167)\n",
      "BEST epoch = 25 | BEST Corr = 0.3167 | RMSE@BEST = 1.0226\n",
      "----------------------------------------\n",
      "      Processing Trait: VE\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.8181 | Val Loss: 0.7536 | Val Corr: 0.1497 | Val RMSE: 0.9644\n",
      "   Epoch 10/100 | Train Loss: 0.6216 | Val Loss: 0.7104 | Val Corr: 0.3447 | Val RMSE: 0.9205\n",
      "   Epoch 20/100 | Train Loss: 0.5081 | Val Loss: 0.6922 | Val Corr: 0.4217 | Val RMSE: 0.8916\n",
      "   Epoch 30/100 | Train Loss: 0.4447 | Val Loss: 0.7518 | Val Corr: 0.4193 | Val RMSE: 0.9574\n",
      "   Epoch 40/100 | Train Loss: 0.3906 | Val Loss: 0.6960 | Val Corr: 0.4134 | Val RMSE: 0.9029\n",
      "   Epoch 50/100 | Train Loss: 0.3517 | Val Loss: 0.7023 | Val Corr: 0.4225 | Val RMSE: 0.8999\n",
      "   Epoch 60/100 | Train Loss: 0.3419 | Val Loss: 0.6795 | Val Corr: 0.4288 | Val RMSE: 0.8887\n",
      "Early stopping at epoch 70 (best epoch 55, best corr 0.4594)\n",
      "BEST epoch = 55 | BEST Corr = 0.4594 | RMSE@BEST = 0.8749\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.8001 | Val Loss: 0.7847 | Val Corr: 0.2297 | Val RMSE: 1.0481\n",
      "   Epoch 10/100 | Train Loss: 0.6058 | Val Loss: 0.6943 | Val Corr: 0.4448 | Val RMSE: 0.9529\n",
      "   Epoch 20/100 | Train Loss: 0.4801 | Val Loss: 0.7069 | Val Corr: 0.4571 | Val RMSE: 0.9497\n",
      "Early stopping at epoch 29 (best epoch 14, best corr 0.4806)\n",
      "BEST epoch = 14 | BEST Corr = 0.4806 | RMSE@BEST = 0.9312\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.8038 | Val Loss: 0.8265 | Val Corr: 0.2386 | Val RMSE: 1.0068\n",
      "   Epoch 10/100 | Train Loss: 0.6254 | Val Loss: 0.7098 | Val Corr: 0.4380 | Val RMSE: 0.8800\n",
      "   Epoch 20/100 | Train Loss: 0.5135 | Val Loss: 0.7484 | Val Corr: 0.4369 | Val RMSE: 0.9316\n",
      "Early stopping at epoch 22 (best epoch 7, best corr 0.4730)\n",
      "BEST epoch = 7 | BEST Corr = 0.4730 | RMSE@BEST = 0.9866\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.8020 | Val Loss: 0.8297 | Val Corr: 0.2341 | Val RMSE: 1.0804\n",
      "   Epoch 10/100 | Train Loss: 0.6236 | Val Loss: 0.7296 | Val Corr: 0.4556 | Val RMSE: 0.9440\n",
      "   Epoch 20/100 | Train Loss: 0.5250 | Val Loss: 0.7372 | Val Corr: 0.4369 | Val RMSE: 0.9466\n",
      "   Epoch 30/100 | Train Loss: 0.4574 | Val Loss: 0.7202 | Val Corr: 0.4824 | Val RMSE: 0.9245\n",
      "   Epoch 40/100 | Train Loss: 0.3996 | Val Loss: 0.7148 | Val Corr: 0.4702 | Val RMSE: 0.9369\n",
      "Early stopping at epoch 45 (best epoch 30, best corr 0.4824)\n",
      "BEST epoch = 30 | BEST Corr = 0.4824 | RMSE@BEST = 0.9245\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.8283 | Val Loss: 0.7158 | Val Corr: 0.3352 | Val RMSE: 0.9321\n",
      "   Epoch 10/100 | Train Loss: 0.5931 | Val Loss: 0.7040 | Val Corr: 0.3647 | Val RMSE: 0.9164\n",
      "   Epoch 20/100 | Train Loss: 0.4901 | Val Loss: 0.7906 | Val Corr: 0.3865 | Val RMSE: 0.9771\n",
      "Early stopping at epoch 23 (best epoch 8, best corr 0.3887)\n",
      "BEST epoch = 8 | BEST Corr = 0.3887 | RMSE@BEST = 0.9671\n",
      "----------------------------------------\n",
      "   [Saved] bulls_T5000_LD0.4.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.6 | Species: bulls | Feature Count: T5000\n",
      "      Processing Trait: MS\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.8013 | Val Loss: 0.8474 | Val Corr: 0.1134 | Val RMSE: 1.0608\n",
      "   Epoch 10/100 | Train Loss: 0.6238 | Val Loss: 0.7820 | Val Corr: 0.3287 | Val RMSE: 0.9851\n",
      "   Epoch 20/100 | Train Loss: 0.4857 | Val Loss: 0.8276 | Val Corr: 0.3472 | Val RMSE: 1.0570\n",
      "   Epoch 30/100 | Train Loss: 0.3992 | Val Loss: 0.7837 | Val Corr: 0.3889 | Val RMSE: 0.9736\n",
      "Early stopping at epoch 32 (best epoch 17, best corr 0.3961)\n",
      "BEST epoch = 17 | BEST Corr = 0.3961 | RMSE@BEST = 0.9713\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.8162 | Val Loss: 0.7567 | Val Corr: 0.1782 | Val RMSE: 0.9809\n",
      "   Epoch 10/100 | Train Loss: 0.6567 | Val Loss: 0.7056 | Val Corr: 0.4311 | Val RMSE: 0.8947\n",
      "   Epoch 20/100 | Train Loss: 0.5259 | Val Loss: 0.6722 | Val Corr: 0.4598 | Val RMSE: 0.8939\n",
      "   Epoch 30/100 | Train Loss: 0.4276 | Val Loss: 0.7071 | Val Corr: 0.4585 | Val RMSE: 0.9060\n",
      "Early stopping at epoch 31 (best epoch 16, best corr 0.4814)\n",
      "BEST epoch = 16 | BEST Corr = 0.4814 | RMSE@BEST = 0.8768\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7900 | Val Loss: 0.7690 | Val Corr: 0.1435 | Val RMSE: 0.9570\n",
      "   Epoch 10/100 | Train Loss: 0.6394 | Val Loss: 0.7078 | Val Corr: 0.4252 | Val RMSE: 0.8715\n",
      "   Epoch 20/100 | Train Loss: 0.5088 | Val Loss: 0.6925 | Val Corr: 0.4191 | Val RMSE: 0.8798\n",
      "   Epoch 30/100 | Train Loss: 0.4179 | Val Loss: 0.6969 | Val Corr: 0.4154 | Val RMSE: 0.8895\n",
      "Early stopping at epoch 38 (best epoch 23, best corr 0.4516)\n",
      "BEST epoch = 23 | BEST Corr = 0.4516 | RMSE@BEST = 0.8709\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.8040 | Val Loss: 0.7951 | Val Corr: 0.1452 | Val RMSE: 1.0273\n",
      "   Epoch 10/100 | Train Loss: 0.6384 | Val Loss: 0.7136 | Val Corr: 0.3003 | Val RMSE: 0.9514\n",
      "   Epoch 20/100 | Train Loss: 0.5007 | Val Loss: 0.7012 | Val Corr: 0.3988 | Val RMSE: 0.9447\n",
      "   Epoch 30/100 | Train Loss: 0.4211 | Val Loss: 0.7263 | Val Corr: 0.3577 | Val RMSE: 0.9649\n",
      "Early stopping at epoch 34 (best epoch 19, best corr 0.4064)\n",
      "BEST epoch = 19 | BEST Corr = 0.4064 | RMSE@BEST = 0.9212\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7951 | Val Loss: 0.7817 | Val Corr: 0.0510 | Val RMSE: 0.9934\n",
      "   Epoch 10/100 | Train Loss: 0.5931 | Val Loss: 0.7990 | Val Corr: 0.2519 | Val RMSE: 1.0337\n",
      "   Epoch 20/100 | Train Loss: 0.4697 | Val Loss: 0.7812 | Val Corr: 0.2925 | Val RMSE: 1.0056\n",
      "   Epoch 30/100 | Train Loss: 0.3812 | Val Loss: 0.7679 | Val Corr: 0.2778 | Val RMSE: 0.9865\n",
      "Early stopping at epoch 32 (best epoch 17, best corr 0.3003)\n",
      "BEST epoch = 17 | BEST Corr = 0.3003 | RMSE@BEST = 0.9873\n",
      "----------------------------------------\n",
      "      Processing Trait: NMSP\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.8145 | Val Loss: 0.8204 | Val Corr: 0.1329 | Val RMSE: 0.9648\n",
      "   Epoch 10/100 | Train Loss: 0.6391 | Val Loss: 0.6370 | Val Corr: 0.5160 | Val RMSE: 0.8004\n",
      "   Epoch 20/100 | Train Loss: 0.5045 | Val Loss: 0.5874 | Val Corr: 0.5768 | Val RMSE: 0.7734\n",
      "   Epoch 30/100 | Train Loss: 0.4290 | Val Loss: 0.5791 | Val Corr: 0.5926 | Val RMSE: 0.7692\n",
      "   Epoch 40/100 | Train Loss: 0.3924 | Val Loss: 0.5828 | Val Corr: 0.5868 | Val RMSE: 0.7772\n",
      "Early stopping at epoch 43 (best epoch 28, best corr 0.5984)\n",
      "BEST epoch = 28 | BEST Corr = 0.5984 | RMSE@BEST = 0.7598\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7905 | Val Loss: 0.8136 | Val Corr: 0.0583 | Val RMSE: 1.0487\n",
      "   Epoch 10/100 | Train Loss: 0.5987 | Val Loss: 0.7278 | Val Corr: 0.4314 | Val RMSE: 1.0121\n",
      "   Epoch 20/100 | Train Loss: 0.4658 | Val Loss: 0.7163 | Val Corr: 0.4669 | Val RMSE: 0.9963\n",
      "   Epoch 30/100 | Train Loss: 0.4061 | Val Loss: 0.6921 | Val Corr: 0.4897 | Val RMSE: 0.9448\n",
      "   Epoch 40/100 | Train Loss: 0.3458 | Val Loss: 0.6947 | Val Corr: 0.5007 | Val RMSE: 0.9478\n",
      "Early stopping at epoch 42 (best epoch 27, best corr 0.5060)\n",
      "BEST epoch = 27 | BEST Corr = 0.5060 | RMSE@BEST = 0.9547\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7621 | Val Loss: 0.7849 | Val Corr: 0.1495 | Val RMSE: 1.0736\n",
      "   Epoch 10/100 | Train Loss: 0.5777 | Val Loss: 0.6610 | Val Corr: 0.4611 | Val RMSE: 0.8807\n",
      "   Epoch 20/100 | Train Loss: 0.4496 | Val Loss: 0.6154 | Val Corr: 0.5436 | Val RMSE: 0.8282\n",
      "   Epoch 30/100 | Train Loss: 0.3738 | Val Loss: 0.6300 | Val Corr: 0.5636 | Val RMSE: 0.8509\n",
      "   Epoch 40/100 | Train Loss: 0.3510 | Val Loss: 0.6459 | Val Corr: 0.5483 | Val RMSE: 0.8425\n",
      "Early stopping at epoch 46 (best epoch 31, best corr 0.5739)\n",
      "BEST epoch = 31 | BEST Corr = 0.5739 | RMSE@BEST = 0.8378\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7955 | Val Loss: 0.7681 | Val Corr: 0.0324 | Val RMSE: 1.0035\n",
      "   Epoch 10/100 | Train Loss: 0.6366 | Val Loss: 0.7241 | Val Corr: 0.3951 | Val RMSE: 1.0332\n",
      "   Epoch 20/100 | Train Loss: 0.4759 | Val Loss: 0.6871 | Val Corr: 0.4350 | Val RMSE: 0.9595\n",
      "   Epoch 30/100 | Train Loss: 0.4173 | Val Loss: 0.6693 | Val Corr: 0.4550 | Val RMSE: 0.9304\n",
      "   Epoch 40/100 | Train Loss: 0.3571 | Val Loss: 0.6803 | Val Corr: 0.4468 | Val RMSE: 0.9260\n",
      "Early stopping at epoch 42 (best epoch 27, best corr 0.4722)\n",
      "BEST epoch = 27 | BEST Corr = 0.4722 | RMSE@BEST = 0.9429\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.8106 | Val Loss: 0.8008 | Val Corr: 0.0932 | Val RMSE: 1.0209\n",
      "   Epoch 10/100 | Train Loss: 0.5763 | Val Loss: 0.7717 | Val Corr: 0.2241 | Val RMSE: 1.0390\n",
      "   Epoch 20/100 | Train Loss: 0.4605 | Val Loss: 0.7910 | Val Corr: 0.2826 | Val RMSE: 1.0266\n",
      "   Epoch 30/100 | Train Loss: 0.3868 | Val Loss: 0.7615 | Val Corr: 0.2968 | Val RMSE: 1.0160\n",
      "Early stopping at epoch 39 (best epoch 24, best corr 0.3223)\n",
      "BEST epoch = 24 | BEST Corr = 0.3223 | RMSE@BEST = 1.0022\n",
      "----------------------------------------\n",
      "      Processing Trait: VE\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.8119 | Val Loss: 0.7528 | Val Corr: 0.1739 | Val RMSE: 0.9620\n",
      "   Epoch 10/100 | Train Loss: 0.6332 | Val Loss: 0.6918 | Val Corr: 0.3746 | Val RMSE: 0.9071\n",
      "   Epoch 20/100 | Train Loss: 0.5161 | Val Loss: 0.7126 | Val Corr: 0.3606 | Val RMSE: 0.9400\n",
      "   Epoch 30/100 | Train Loss: 0.4263 | Val Loss: 0.6974 | Val Corr: 0.3994 | Val RMSE: 0.9139\n",
      "Early stopping at epoch 32 (best epoch 17, best corr 0.4387)\n",
      "BEST epoch = 17 | BEST Corr = 0.4387 | RMSE@BEST = 0.9233\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.8011 | Val Loss: 0.7913 | Val Corr: 0.2344 | Val RMSE: 1.0434\n",
      "   Epoch 10/100 | Train Loss: 0.6096 | Val Loss: 0.7274 | Val Corr: 0.4619 | Val RMSE: 0.9550\n",
      "   Epoch 20/100 | Train Loss: 0.4726 | Val Loss: 0.6898 | Val Corr: 0.4906 | Val RMSE: 0.9397\n",
      "   Epoch 30/100 | Train Loss: 0.4133 | Val Loss: 0.6984 | Val Corr: 0.5111 | Val RMSE: 0.9455\n",
      "   Epoch 40/100 | Train Loss: 0.3642 | Val Loss: 0.7184 | Val Corr: 0.4828 | Val RMSE: 0.9693\n",
      "Early stopping at epoch 44 (best epoch 29, best corr 0.5177)\n",
      "BEST epoch = 29 | BEST Corr = 0.5177 | RMSE@BEST = 0.9299\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7990 | Val Loss: 0.7910 | Val Corr: 0.2732 | Val RMSE: 0.9617\n",
      "   Epoch 10/100 | Train Loss: 0.6218 | Val Loss: 0.7595 | Val Corr: 0.3847 | Val RMSE: 0.9400\n",
      "   Epoch 20/100 | Train Loss: 0.5065 | Val Loss: 0.7080 | Val Corr: 0.4309 | Val RMSE: 0.8878\n",
      "Early stopping at epoch 29 (best epoch 14, best corr 0.4347)\n",
      "BEST epoch = 14 | BEST Corr = 0.4347 | RMSE@BEST = 0.8792\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7980 | Val Loss: 0.8223 | Val Corr: 0.1115 | Val RMSE: 1.0411\n",
      "   Epoch 10/100 | Train Loss: 0.6437 | Val Loss: 0.7560 | Val Corr: 0.4160 | Val RMSE: 0.9863\n",
      "   Epoch 20/100 | Train Loss: 0.5199 | Val Loss: 0.7571 | Val Corr: 0.4422 | Val RMSE: 0.9688\n",
      "   Epoch 30/100 | Train Loss: 0.4500 | Val Loss: 0.7902 | Val Corr: 0.4457 | Val RMSE: 1.0173\n",
      "Early stopping at epoch 36 (best epoch 21, best corr 0.4558)\n",
      "BEST epoch = 21 | BEST Corr = 0.4558 | RMSE@BEST = 0.9359\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.8387 | Val Loss: 0.7325 | Val Corr: 0.2421 | Val RMSE: 0.9511\n",
      "   Epoch 10/100 | Train Loss: 0.6286 | Val Loss: 0.6752 | Val Corr: 0.3459 | Val RMSE: 0.8887\n",
      "   Epoch 20/100 | Train Loss: 0.4830 | Val Loss: 0.6983 | Val Corr: 0.3417 | Val RMSE: 0.9031\n",
      "   Epoch 30/100 | Train Loss: 0.4169 | Val Loss: 0.6935 | Val Corr: 0.3847 | Val RMSE: 0.8877\n",
      "   Epoch 40/100 | Train Loss: 0.3765 | Val Loss: 0.6952 | Val Corr: 0.3684 | Val RMSE: 0.9209\n",
      "Early stopping at epoch 44 (best epoch 29, best corr 0.3925)\n",
      "BEST epoch = 29 | BEST Corr = 0.3925 | RMSE@BEST = 0.8800\n",
      "----------------------------------------\n",
      "   [Saved] bulls_T5000_LD0.6.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.8 | Species: bulls | Feature Count: T5000\n",
      "      Processing Trait: MS\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7988 | Val Loss: 0.8170 | Val Corr: 0.1388 | Val RMSE: 1.0633\n",
      "   Epoch 10/100 | Train Loss: 0.6276 | Val Loss: 0.7642 | Val Corr: 0.3791 | Val RMSE: 0.9675\n",
      "   Epoch 20/100 | Train Loss: 0.4996 | Val Loss: 0.8218 | Val Corr: 0.4078 | Val RMSE: 1.0460\n",
      "   Epoch 30/100 | Train Loss: 0.4099 | Val Loss: 0.7550 | Val Corr: 0.4138 | Val RMSE: 0.9732\n",
      "   Epoch 40/100 | Train Loss: 0.3679 | Val Loss: 0.7727 | Val Corr: 0.4169 | Val RMSE: 0.9875\n",
      "   Epoch 50/100 | Train Loss: 0.3215 | Val Loss: 0.7838 | Val Corr: 0.4132 | Val RMSE: 1.0129\n",
      "Early stopping at epoch 56 (best epoch 41, best corr 0.4341)\n",
      "BEST epoch = 41 | BEST Corr = 0.4341 | RMSE@BEST = 1.0284\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.8173 | Val Loss: 0.7659 | Val Corr: 0.1866 | Val RMSE: 0.9849\n",
      "   Epoch 10/100 | Train Loss: 0.6340 | Val Loss: 0.7468 | Val Corr: 0.4201 | Val RMSE: 0.9261\n",
      "   Epoch 20/100 | Train Loss: 0.5143 | Val Loss: 0.8033 | Val Corr: 0.4442 | Val RMSE: 0.9835\n",
      "   Epoch 30/100 | Train Loss: 0.4266 | Val Loss: 0.6702 | Val Corr: 0.4480 | Val RMSE: 0.8948\n",
      "Early stopping at epoch 39 (best epoch 24, best corr 0.4784)\n",
      "BEST epoch = 24 | BEST Corr = 0.4784 | RMSE@BEST = 0.8730\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7962 | Val Loss: 0.7623 | Val Corr: 0.0947 | Val RMSE: 0.9590\n",
      "   Epoch 10/100 | Train Loss: 0.6401 | Val Loss: 0.7370 | Val Corr: 0.3501 | Val RMSE: 0.9124\n",
      "   Epoch 20/100 | Train Loss: 0.5078 | Val Loss: 0.7624 | Val Corr: 0.3867 | Val RMSE: 0.9333\n",
      "   Epoch 30/100 | Train Loss: 0.4055 | Val Loss: 0.7307 | Val Corr: 0.4033 | Val RMSE: 0.9186\n",
      "Early stopping at epoch 40 (best epoch 25, best corr 0.4293)\n",
      "BEST epoch = 25 | BEST Corr = 0.4293 | RMSE@BEST = 0.9444\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.8075 | Val Loss: 0.7603 | Val Corr: 0.1333 | Val RMSE: 0.9885\n",
      "   Epoch 10/100 | Train Loss: 0.6542 | Val Loss: 0.7014 | Val Corr: 0.3665 | Val RMSE: 0.9261\n",
      "   Epoch 20/100 | Train Loss: 0.5046 | Val Loss: 0.7315 | Val Corr: 0.3955 | Val RMSE: 0.9419\n",
      "   Epoch 30/100 | Train Loss: 0.4240 | Val Loss: 0.6934 | Val Corr: 0.4054 | Val RMSE: 0.9305\n",
      "   Epoch 40/100 | Train Loss: 0.3647 | Val Loss: 0.7015 | Val Corr: 0.3889 | Val RMSE: 0.9467\n",
      "Early stopping at epoch 43 (best epoch 28, best corr 0.4194)\n",
      "BEST epoch = 28 | BEST Corr = 0.4194 | RMSE@BEST = 0.9136\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7995 | Val Loss: 0.7844 | Val Corr: 0.1388 | Val RMSE: 0.9885\n",
      "   Epoch 10/100 | Train Loss: 0.5774 | Val Loss: 0.7601 | Val Corr: 0.3184 | Val RMSE: 0.9507\n",
      "   Epoch 20/100 | Train Loss: 0.4858 | Val Loss: 0.7928 | Val Corr: 0.3478 | Val RMSE: 0.9809\n",
      "   Epoch 30/100 | Train Loss: 0.3926 | Val Loss: 0.7717 | Val Corr: 0.3625 | Val RMSE: 0.9596\n",
      "   Epoch 40/100 | Train Loss: 0.3669 | Val Loss: 0.7630 | Val Corr: 0.3595 | Val RMSE: 0.9552\n",
      "   Epoch 50/100 | Train Loss: 0.3083 | Val Loss: 0.7732 | Val Corr: 0.3444 | Val RMSE: 0.9723\n",
      "Early stopping at epoch 54 (best epoch 39, best corr 0.3691)\n",
      "BEST epoch = 39 | BEST Corr = 0.3691 | RMSE@BEST = 0.9555\n",
      "----------------------------------------\n",
      "      Processing Trait: NMSP\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.8149 | Val Loss: 0.8800 | Val Corr: 0.2725 | Val RMSE: 1.0099\n",
      "   Epoch 10/100 | Train Loss: 0.6382 | Val Loss: 0.6305 | Val Corr: 0.5240 | Val RMSE: 0.8248\n",
      "   Epoch 20/100 | Train Loss: 0.5052 | Val Loss: 0.5795 | Val Corr: 0.5978 | Val RMSE: 0.7678\n",
      "   Epoch 30/100 | Train Loss: 0.4199 | Val Loss: 0.5834 | Val Corr: 0.5912 | Val RMSE: 0.7532\n",
      "   Epoch 40/100 | Train Loss: 0.3679 | Val Loss: 0.5901 | Val Corr: 0.5959 | Val RMSE: 0.7461\n",
      "   Epoch 50/100 | Train Loss: 0.3402 | Val Loss: 0.5806 | Val Corr: 0.6080 | Val RMSE: 0.7669\n",
      "Early stopping at epoch 53 (best epoch 38, best corr 0.6139)\n",
      "BEST epoch = 38 | BEST Corr = 0.6139 | RMSE@BEST = 0.7369\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7865 | Val Loss: 0.8286 | Val Corr: 0.1089 | Val RMSE: 1.0514\n",
      "   Epoch 10/100 | Train Loss: 0.6024 | Val Loss: 0.7090 | Val Corr: 0.4472 | Val RMSE: 0.9446\n",
      "   Epoch 20/100 | Train Loss: 0.4826 | Val Loss: 0.7287 | Val Corr: 0.4843 | Val RMSE: 1.0152\n",
      "   Epoch 30/100 | Train Loss: 0.4061 | Val Loss: 0.6958 | Val Corr: 0.4956 | Val RMSE: 0.9648\n",
      "Early stopping at epoch 32 (best epoch 17, best corr 0.5366)\n",
      "BEST epoch = 17 | BEST Corr = 0.5366 | RMSE@BEST = 0.9210\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7574 | Val Loss: 0.7551 | Val Corr: 0.2704 | Val RMSE: 1.0060\n",
      "   Epoch 10/100 | Train Loss: 0.5744 | Val Loss: 0.6699 | Val Corr: 0.4647 | Val RMSE: 0.9092\n",
      "   Epoch 20/100 | Train Loss: 0.4445 | Val Loss: 0.6366 | Val Corr: 0.5465 | Val RMSE: 0.8578\n",
      "   Epoch 30/100 | Train Loss: 0.3716 | Val Loss: 0.6304 | Val Corr: 0.5454 | Val RMSE: 0.8580\n",
      "   Epoch 40/100 | Train Loss: 0.3475 | Val Loss: 0.6285 | Val Corr: 0.5516 | Val RMSE: 0.8343\n",
      "   Epoch 50/100 | Train Loss: 0.3013 | Val Loss: 0.6070 | Val Corr: 0.5635 | Val RMSE: 0.8262\n",
      "Early stopping at epoch 57 (best epoch 42, best corr 0.5830)\n",
      "BEST epoch = 42 | BEST Corr = 0.5830 | RMSE@BEST = 0.8044\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7816 | Val Loss: 0.7595 | Val Corr: 0.1170 | Val RMSE: 1.0009\n",
      "   Epoch 10/100 | Train Loss: 0.6237 | Val Loss: 0.6824 | Val Corr: 0.3882 | Val RMSE: 0.9286\n",
      "   Epoch 20/100 | Train Loss: 0.4918 | Val Loss: 0.6596 | Val Corr: 0.4798 | Val RMSE: 0.8895\n",
      "   Epoch 30/100 | Train Loss: 0.4113 | Val Loss: 0.6725 | Val Corr: 0.4497 | Val RMSE: 0.9198\n",
      "Early stopping at epoch 34 (best epoch 19, best corr 0.4904)\n",
      "BEST epoch = 19 | BEST Corr = 0.4904 | RMSE@BEST = 0.8894\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.8148 | Val Loss: 0.8195 | Val Corr: 0.1374 | Val RMSE: 1.0261\n",
      "   Epoch 10/100 | Train Loss: 0.5831 | Val Loss: 0.8270 | Val Corr: 0.2709 | Val RMSE: 1.0301\n",
      "   Epoch 20/100 | Train Loss: 0.4380 | Val Loss: 0.8063 | Val Corr: 0.2805 | Val RMSE: 1.0388\n",
      "   Epoch 30/100 | Train Loss: 0.3941 | Val Loss: 0.7777 | Val Corr: 0.2842 | Val RMSE: 1.0142\n",
      "Early stopping at epoch 40 (best epoch 25, best corr 0.3138)\n",
      "BEST epoch = 25 | BEST Corr = 0.3138 | RMSE@BEST = 1.0042\n",
      "----------------------------------------\n",
      "      Processing Trait: VE\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.8090 | Val Loss: 0.7525 | Val Corr: 0.1783 | Val RMSE: 0.9788\n",
      "   Epoch 10/100 | Train Loss: 0.6305 | Val Loss: 0.6843 | Val Corr: 0.3616 | Val RMSE: 0.9069\n",
      "   Epoch 20/100 | Train Loss: 0.5026 | Val Loss: 0.6863 | Val Corr: 0.3851 | Val RMSE: 0.9141\n",
      "   Epoch 30/100 | Train Loss: 0.4454 | Val Loss: 0.7044 | Val Corr: 0.3873 | Val RMSE: 0.9174\n",
      "   Epoch 40/100 | Train Loss: 0.3924 | Val Loss: 0.6876 | Val Corr: 0.3998 | Val RMSE: 0.9113\n",
      "   Epoch 50/100 | Train Loss: 0.3483 | Val Loss: 0.6721 | Val Corr: 0.3933 | Val RMSE: 0.9085\n",
      "Early stopping at epoch 52 (best epoch 37, best corr 0.4146)\n",
      "BEST epoch = 37 | BEST Corr = 0.4146 | RMSE@BEST = 0.9454\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.8031 | Val Loss: 0.7813 | Val Corr: 0.2683 | Val RMSE: 1.0404\n",
      "   Epoch 10/100 | Train Loss: 0.6096 | Val Loss: 0.7188 | Val Corr: 0.4522 | Val RMSE: 0.9965\n",
      "   Epoch 20/100 | Train Loss: 0.4864 | Val Loss: 0.7095 | Val Corr: 0.4337 | Val RMSE: 0.9748\n",
      "   Epoch 30/100 | Train Loss: 0.4186 | Val Loss: 0.6782 | Val Corr: 0.4869 | Val RMSE: 0.9452\n",
      "Early stopping at epoch 32 (best epoch 17, best corr 0.4972)\n",
      "BEST epoch = 17 | BEST Corr = 0.4972 | RMSE@BEST = 0.9578\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7965 | Val Loss: 0.8701 | Val Corr: 0.2835 | Val RMSE: 1.0597\n",
      "   Epoch 10/100 | Train Loss: 0.6140 | Val Loss: 0.7214 | Val Corr: 0.4152 | Val RMSE: 0.8850\n",
      "   Epoch 20/100 | Train Loss: 0.4881 | Val Loss: 0.7138 | Val Corr: 0.4167 | Val RMSE: 0.8914\n",
      "   Epoch 30/100 | Train Loss: 0.4318 | Val Loss: 0.7320 | Val Corr: 0.4031 | Val RMSE: 0.9245\n",
      "Early stopping at epoch 38 (best epoch 23, best corr 0.4493)\n",
      "BEST epoch = 23 | BEST Corr = 0.4493 | RMSE@BEST = 0.8942\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7974 | Val Loss: 0.8117 | Val Corr: 0.2713 | Val RMSE: 1.0317\n",
      "   Epoch 10/100 | Train Loss: 0.6311 | Val Loss: 0.7847 | Val Corr: 0.4348 | Val RMSE: 1.0273\n",
      "   Epoch 20/100 | Train Loss: 0.5203 | Val Loss: 0.8360 | Val Corr: 0.4164 | Val RMSE: 1.0860\n",
      "   Epoch 30/100 | Train Loss: 0.4528 | Val Loss: 0.7664 | Val Corr: 0.4307 | Val RMSE: 0.9916\n",
      "Early stopping at epoch 39 (best epoch 24, best corr 0.4627)\n",
      "BEST epoch = 24 | BEST Corr = 0.4627 | RMSE@BEST = 0.9961\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.8304 | Val Loss: 0.7287 | Val Corr: 0.1670 | Val RMSE: 0.9545\n",
      "   Epoch 10/100 | Train Loss: 0.6274 | Val Loss: 0.7060 | Val Corr: 0.3156 | Val RMSE: 0.9137\n",
      "   Epoch 20/100 | Train Loss: 0.4797 | Val Loss: 0.7021 | Val Corr: 0.3706 | Val RMSE: 0.9030\n",
      "   Epoch 30/100 | Train Loss: 0.4306 | Val Loss: 0.7186 | Val Corr: 0.3469 | Val RMSE: 0.9224\n",
      "   Epoch 40/100 | Train Loss: 0.3947 | Val Loss: 0.7006 | Val Corr: 0.3411 | Val RMSE: 0.9316\n",
      "Early stopping at epoch 41 (best epoch 26, best corr 0.3764)\n",
      "BEST epoch = 26 | BEST Corr = 0.3764 | RMSE@BEST = 0.8956\n",
      "----------------------------------------\n",
      "   [Saved] bulls_T5000_LD0.8.json\n",
      "\n",
      "############################################################\n",
      "RUNNING EXPERIMENT: POOLING STRATEGY ABLATION\n",
      "############################################################\n",
      "\n",
      "   >>> Processing Pooling Type: MAP | Species: rice | Feature Count: T5000\n",
      "      Processing Trait: SW\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.8207 | Val Loss: 0.6766 | Val Corr: 0.4940 | Val RMSE: 0.8944\n",
      "   Epoch 10/100 | Train Loss: 0.6847 | Val Loss: 0.5693 | Val Corr: 0.6312 | Val RMSE: 0.7560\n",
      "   Epoch 20/100 | Train Loss: 0.5951 | Val Loss: 0.5286 | Val Corr: 0.6755 | Val RMSE: 0.6991\n",
      "   Epoch 30/100 | Train Loss: 0.5422 | Val Loss: 0.5370 | Val Corr: 0.5952 | Val RMSE: 0.7214\n",
      "Early stopping at epoch 31 (best epoch 16, best corr 0.6777)\n",
      "BEST epoch = 16 | BEST Corr = 0.6777 | RMSE@BEST = 0.7023\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7672 | Val Loss: 0.7071 | Val Corr: 0.3913 | Val RMSE: 1.0495\n",
      "   Epoch 10/100 | Train Loss: 0.6213 | Val Loss: 0.7167 | Val Corr: 0.5454 | Val RMSE: 0.9271\n",
      "   Epoch 20/100 | Train Loss: 0.5323 | Val Loss: 0.6009 | Val Corr: 0.5779 | Val RMSE: 0.8603\n",
      "   Epoch 30/100 | Train Loss: 0.4905 | Val Loss: 0.5840 | Val Corr: 0.5698 | Val RMSE: 0.8743\n",
      "   Epoch 40/100 | Train Loss: 0.4499 | Val Loss: 0.6287 | Val Corr: 0.5870 | Val RMSE: 0.8831\n",
      "   Epoch 50/100 | Train Loss: 0.4267 | Val Loss: 0.6002 | Val Corr: 0.6027 | Val RMSE: 0.8784\n",
      "   Epoch 60/100 | Train Loss: 0.3673 | Val Loss: 0.6270 | Val Corr: 0.5907 | Val RMSE: 0.8730\n",
      "Early stopping at epoch 68 (best epoch 53, best corr 0.6042)\n",
      "BEST epoch = 53 | BEST Corr = 0.6042 | RMSE@BEST = 0.8726\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7749 | Val Loss: 0.8030 | Val Corr: 0.4854 | Val RMSE: 1.1143\n",
      "   Epoch 10/100 | Train Loss: 0.6516 | Val Loss: 0.6406 | Val Corr: 0.6011 | Val RMSE: 0.9275\n",
      "   Epoch 20/100 | Train Loss: 0.5361 | Val Loss: 0.5776 | Val Corr: 0.6607 | Val RMSE: 0.8443\n",
      "   Epoch 30/100 | Train Loss: 0.4747 | Val Loss: 0.6168 | Val Corr: 0.6610 | Val RMSE: 0.8777\n",
      "   Epoch 40/100 | Train Loss: 0.4620 | Val Loss: 0.6253 | Val Corr: 0.6530 | Val RMSE: 0.9154\n",
      "   Epoch 50/100 | Train Loss: 0.4126 | Val Loss: 0.5712 | Val Corr: 0.6808 | Val RMSE: 0.8588\n",
      "   Epoch 60/100 | Train Loss: 0.3584 | Val Loss: 0.6145 | Val Corr: 0.6850 | Val RMSE: 0.8865\n",
      "   Epoch 70/100 | Train Loss: 0.3662 | Val Loss: 0.5643 | Val Corr: 0.6912 | Val RMSE: 0.8333\n",
      "   Epoch 80/100 | Train Loss: 0.3629 | Val Loss: 0.5756 | Val Corr: 0.6934 | Val RMSE: 0.8466\n",
      "Early stopping at epoch 83 (best epoch 68, best corr 0.7048)\n",
      "BEST epoch = 68 | BEST Corr = 0.7048 | RMSE@BEST = 0.8279\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7685 | Val Loss: 0.7150 | Val Corr: 0.3200 | Val RMSE: 0.9402\n",
      "   Epoch 10/100 | Train Loss: 0.6368 | Val Loss: 0.6420 | Val Corr: 0.4268 | Val RMSE: 0.8586\n",
      "   Epoch 20/100 | Train Loss: 0.5059 | Val Loss: 0.6362 | Val Corr: 0.4710 | Val RMSE: 0.9245\n",
      "Early stopping at epoch 30 (best epoch 15, best corr 0.4810)\n",
      "BEST epoch = 15 | BEST Corr = 0.4810 | RMSE@BEST = 0.8325\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7662 | Val Loss: 0.6954 | Val Corr: 0.3608 | Val RMSE: 0.9773\n",
      "   Epoch 10/100 | Train Loss: 0.5943 | Val Loss: 0.6469 | Val Corr: 0.4951 | Val RMSE: 0.8642\n",
      "   Epoch 20/100 | Train Loss: 0.5599 | Val Loss: 0.6732 | Val Corr: 0.4846 | Val RMSE: 0.8858\n",
      "   Epoch 30/100 | Train Loss: 0.4627 | Val Loss: 0.6152 | Val Corr: 0.5270 | Val RMSE: 0.8569\n",
      "   Epoch 40/100 | Train Loss: 0.3747 | Val Loss: 0.6356 | Val Corr: 0.5143 | Val RMSE: 0.9109\n",
      "Early stopping at epoch 47 (best epoch 32, best corr 0.5312)\n",
      "BEST epoch = 32 | BEST Corr = 0.5312 | RMSE@BEST = 0.8651\n",
      "----------------------------------------\n",
      "      Processing Trait: FLW\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7253 | Val Loss: 0.6850 | Val Corr: 0.3483 | Val RMSE: 1.0358\n",
      "   Epoch 10/100 | Train Loss: 0.6459 | Val Loss: 0.6230 | Val Corr: 0.4513 | Val RMSE: 0.9387\n",
      "   Epoch 20/100 | Train Loss: 0.6022 | Val Loss: 0.6116 | Val Corr: 0.4252 | Val RMSE: 0.9527\n",
      "   Epoch 30/100 | Train Loss: 0.5799 | Val Loss: 0.6066 | Val Corr: 0.4842 | Val RMSE: 0.9144\n",
      "Early stopping at epoch 37 (best epoch 22, best corr 0.4964)\n",
      "BEST epoch = 22 | BEST Corr = 0.4964 | RMSE@BEST = 0.9014\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7501 | Val Loss: 0.7190 | Val Corr: 0.2704 | Val RMSE: 1.1516\n",
      "   Epoch 10/100 | Train Loss: 0.6248 | Val Loss: 0.7229 | Val Corr: 0.3531 | Val RMSE: 1.0888\n",
      "   Epoch 20/100 | Train Loss: 0.5628 | Val Loss: 0.7130 | Val Corr: 0.4390 | Val RMSE: 1.0614\n",
      "   Epoch 30/100 | Train Loss: 0.5263 | Val Loss: 0.7290 | Val Corr: 0.4024 | Val RMSE: 1.0581\n",
      "   Epoch 40/100 | Train Loss: 0.5078 | Val Loss: 0.6993 | Val Corr: 0.4467 | Val RMSE: 1.0310\n",
      "Early stopping at epoch 49 (best epoch 34, best corr 0.4950)\n",
      "BEST epoch = 34 | BEST Corr = 0.4950 | RMSE@BEST = 1.0487\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7271 | Val Loss: 0.7236 | Val Corr: 0.3380 | Val RMSE: 1.0792\n",
      "   Epoch 10/100 | Train Loss: 0.6455 | Val Loss: 0.6782 | Val Corr: 0.3623 | Val RMSE: 1.0089\n",
      "   Epoch 20/100 | Train Loss: 0.5927 | Val Loss: 0.6502 | Val Corr: 0.4605 | Val RMSE: 0.9693\n",
      "   Epoch 30/100 | Train Loss: 0.5660 | Val Loss: 0.6749 | Val Corr: 0.4888 | Val RMSE: 0.9901\n",
      "   Epoch 40/100 | Train Loss: 0.4914 | Val Loss: 0.6971 | Val Corr: 0.4989 | Val RMSE: 1.0079\n",
      "   Epoch 50/100 | Train Loss: 0.5184 | Val Loss: 0.6646 | Val Corr: 0.4532 | Val RMSE: 0.9648\n",
      "Early stopping at epoch 56 (best epoch 41, best corr 0.5134)\n",
      "BEST epoch = 41 | BEST Corr = 0.5134 | RMSE@BEST = 0.9588\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7630 | Val Loss: 0.6522 | Val Corr: 0.3217 | Val RMSE: 0.8560\n",
      "   Epoch 10/100 | Train Loss: 0.7344 | Val Loss: 0.6027 | Val Corr: 0.2691 | Val RMSE: 0.8173\n",
      "   Epoch 20/100 | Train Loss: 0.5913 | Val Loss: 0.5935 | Val Corr: 0.2842 | Val RMSE: 0.8166\n",
      "   Epoch 30/100 | Train Loss: 0.5740 | Val Loss: 0.5998 | Val Corr: 0.3221 | Val RMSE: 0.8239\n",
      "   Epoch 40/100 | Train Loss: 0.5143 | Val Loss: 0.6326 | Val Corr: 0.3990 | Val RMSE: 0.8406\n",
      "   Epoch 50/100 | Train Loss: 0.4947 | Val Loss: 0.5761 | Val Corr: 0.4044 | Val RMSE: 0.7964\n",
      "   Epoch 60/100 | Train Loss: 0.4902 | Val Loss: 0.5833 | Val Corr: 0.4114 | Val RMSE: 0.8067\n",
      "   Epoch 70/100 | Train Loss: 0.4545 | Val Loss: 0.5854 | Val Corr: 0.4108 | Val RMSE: 0.7988\n",
      "   Epoch 80/100 | Train Loss: 0.4336 | Val Loss: 0.5834 | Val Corr: 0.4019 | Val RMSE: 0.7950\n",
      "Early stopping at epoch 88 (best epoch 73, best corr 0.4194)\n",
      "BEST epoch = 73 | BEST Corr = 0.4194 | RMSE@BEST = 0.7923\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7230 | Val Loss: 0.5802 | Val Corr: 0.3148 | Val RMSE: 0.8298\n",
      "   Epoch 10/100 | Train Loss: 0.6350 | Val Loss: 0.5675 | Val Corr: 0.4031 | Val RMSE: 0.7668\n",
      "   Epoch 20/100 | Train Loss: 0.5551 | Val Loss: 0.6240 | Val Corr: 0.3618 | Val RMSE: 0.8388\n",
      "Early stopping at epoch 25 (best epoch 10, best corr 0.4031)\n",
      "BEST epoch = 10 | BEST Corr = 0.4031 | RMSE@BEST = 0.7668\n",
      "----------------------------------------\n",
      "      Processing Trait: AC\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7267 | Val Loss: 0.6045 | Val Corr: 0.1523 | Val RMSE: 0.9425\n",
      "   Epoch 10/100 | Train Loss: 0.6948 | Val Loss: 0.5488 | Val Corr: 0.2481 | Val RMSE: 0.9103\n",
      "   Epoch 20/100 | Train Loss: 0.6291 | Val Loss: 0.4901 | Val Corr: 0.3452 | Val RMSE: 0.8997\n",
      "   Epoch 30/100 | Train Loss: 0.5909 | Val Loss: 0.5220 | Val Corr: 0.2598 | Val RMSE: 0.9083\n",
      "Early stopping at epoch 36 (best epoch 21, best corr 0.3513)\n",
      "BEST epoch = 21 | BEST Corr = 0.3513 | RMSE@BEST = 0.8893\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7181 | Val Loss: 0.5968 | Val Corr: 0.1167 | Val RMSE: 0.9282\n",
      "   Epoch 10/100 | Train Loss: 0.6276 | Val Loss: 0.5214 | Val Corr: 0.3605 | Val RMSE: 0.8904\n",
      "   Epoch 20/100 | Train Loss: 0.5608 | Val Loss: 0.5173 | Val Corr: 0.3899 | Val RMSE: 0.8648\n",
      "Early stopping at epoch 27 (best epoch 12, best corr 0.4549)\n",
      "BEST epoch = 12 | BEST Corr = 0.4549 | RMSE@BEST = 0.8515\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.6841 | Val Loss: 0.6512 | Val Corr: 0.0418 | Val RMSE: 1.0691\n",
      "   Epoch 10/100 | Train Loss: 0.6080 | Val Loss: 0.6179 | Val Corr: 0.3092 | Val RMSE: 1.0252\n",
      "   Epoch 20/100 | Train Loss: 0.5291 | Val Loss: 0.6034 | Val Corr: 0.3702 | Val RMSE: 1.0171\n",
      "   Epoch 30/100 | Train Loss: 0.4696 | Val Loss: 0.6062 | Val Corr: 0.3701 | Val RMSE: 1.0203\n",
      "Early stopping at epoch 32 (best epoch 17, best corr 0.4554)\n",
      "BEST epoch = 17 | BEST Corr = 0.4554 | RMSE@BEST = 0.9811\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.6511 | Val Loss: 0.7663 | Val Corr: 0.1243 | Val RMSE: 1.1087\n",
      "   Epoch 10/100 | Train Loss: 0.5994 | Val Loss: 0.7311 | Val Corr: 0.1345 | Val RMSE: 1.1240\n",
      "   Epoch 20/100 | Train Loss: 0.5295 | Val Loss: 0.7011 | Val Corr: 0.3223 | Val RMSE: 1.0511\n",
      "   Epoch 30/100 | Train Loss: 0.4771 | Val Loss: 0.6542 | Val Corr: 0.4356 | Val RMSE: 1.0143\n",
      "   Epoch 40/100 | Train Loss: 0.4387 | Val Loss: 0.6590 | Val Corr: 0.4019 | Val RMSE: 1.0346\n",
      "   Epoch 50/100 | Train Loss: 0.4222 | Val Loss: 0.6619 | Val Corr: 0.3283 | Val RMSE: 1.1064\n",
      "Early stopping at epoch 54 (best epoch 39, best corr 0.4803)\n",
      "BEST epoch = 39 | BEST Corr = 0.4803 | RMSE@BEST = 0.9814\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7562 | Val Loss: 0.6490 | Val Corr: 0.2197 | Val RMSE: 0.9466\n",
      "   Epoch 10/100 | Train Loss: 0.6135 | Val Loss: 0.5429 | Val Corr: 0.3979 | Val RMSE: 0.9036\n",
      "   Epoch 20/100 | Train Loss: 0.5639 | Val Loss: 0.5858 | Val Corr: 0.4735 | Val RMSE: 0.8498\n",
      "   Epoch 30/100 | Train Loss: 0.5036 | Val Loss: 0.5308 | Val Corr: 0.5285 | Val RMSE: 0.8255\n",
      "   Epoch 40/100 | Train Loss: 0.4478 | Val Loss: 0.5485 | Val Corr: 0.5567 | Val RMSE: 0.7900\n",
      "   Epoch 50/100 | Train Loss: 0.4251 | Val Loss: 0.5502 | Val Corr: 0.5087 | Val RMSE: 0.8148\n",
      "Early stopping at epoch 56 (best epoch 41, best corr 0.5700)\n",
      "BEST epoch = 41 | BEST Corr = 0.5700 | RMSE@BEST = 0.7808\n",
      "----------------------------------------\n",
      "      Processing Trait: PH\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7774 | Val Loss: 0.6305 | Val Corr: 0.4638 | Val RMSE: 0.9047\n",
      "   Epoch 10/100 | Train Loss: 0.6808 | Val Loss: 0.6219 | Val Corr: 0.5131 | Val RMSE: 0.8233\n",
      "   Epoch 20/100 | Train Loss: 0.6278 | Val Loss: 0.5383 | Val Corr: 0.5439 | Val RMSE: 0.7709\n",
      "   Epoch 30/100 | Train Loss: 0.5725 | Val Loss: 0.5261 | Val Corr: 0.5756 | Val RMSE: 0.7439\n",
      "   Epoch 40/100 | Train Loss: 0.5541 | Val Loss: 0.5255 | Val Corr: 0.5958 | Val RMSE: 0.7441\n",
      "   Epoch 50/100 | Train Loss: 0.5117 | Val Loss: 0.5085 | Val Corr: 0.6528 | Val RMSE: 0.6892\n",
      "   Epoch 60/100 | Train Loss: 0.4524 | Val Loss: 0.5248 | Val Corr: 0.6324 | Val RMSE: 0.7100\n",
      "   Epoch 70/100 | Train Loss: 0.4274 | Val Loss: 0.5162 | Val Corr: 0.6300 | Val RMSE: 0.7038\n",
      "Early stopping at epoch 78 (best epoch 63, best corr 0.6836)\n",
      "BEST epoch = 63 | BEST Corr = 0.6836 | RMSE@BEST = 0.6940\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7442 | Val Loss: 0.5886 | Val Corr: 0.3501 | Val RMSE: 0.8681\n",
      "   Epoch 10/100 | Train Loss: 0.6482 | Val Loss: 0.5422 | Val Corr: 0.4255 | Val RMSE: 0.7893\n",
      "   Epoch 20/100 | Train Loss: 0.5985 | Val Loss: 0.5950 | Val Corr: 0.4365 | Val RMSE: 0.8232\n",
      "Early stopping at epoch 28 (best epoch 13, best corr 0.4720)\n",
      "BEST epoch = 13 | BEST Corr = 0.4720 | RMSE@BEST = 0.7954\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7512 | Val Loss: 0.6941 | Val Corr: -0.0007 | Val RMSE: 0.9970\n",
      "   Epoch 10/100 | Train Loss: 0.6514 | Val Loss: 0.6950 | Val Corr: 0.1650 | Val RMSE: 0.9910\n",
      "   Epoch 20/100 | Train Loss: 0.6065 | Val Loss: 0.7051 | Val Corr: 0.2703 | Val RMSE: 0.9765\n",
      "   Epoch 30/100 | Train Loss: 0.5043 | Val Loss: 0.6357 | Val Corr: 0.4039 | Val RMSE: 0.9249\n",
      "   Epoch 40/100 | Train Loss: 0.4774 | Val Loss: 0.6901 | Val Corr: 0.4005 | Val RMSE: 0.9591\n",
      "   Epoch 50/100 | Train Loss: 0.4789 | Val Loss: 0.7192 | Val Corr: 0.3498 | Val RMSE: 0.9810\n",
      "Early stopping at epoch 54 (best epoch 39, best corr 0.4181)\n",
      "BEST epoch = 39 | BEST Corr = 0.4181 | RMSE@BEST = 0.9350\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.6845 | Val Loss: 0.9108 | Val Corr: 0.2225 | Val RMSE: 1.2866\n",
      "   Epoch 10/100 | Train Loss: 0.6195 | Val Loss: 0.7993 | Val Corr: 0.2721 | Val RMSE: 1.2261\n",
      "   Epoch 20/100 | Train Loss: 0.5274 | Val Loss: 0.7980 | Val Corr: 0.2474 | Val RMSE: 1.2459\n",
      "   Epoch 30/100 | Train Loss: 0.5109 | Val Loss: 0.7422 | Val Corr: 0.3587 | Val RMSE: 1.1943\n",
      "   Epoch 40/100 | Train Loss: 0.4475 | Val Loss: 0.7541 | Val Corr: 0.3505 | Val RMSE: 1.2052\n",
      "   Epoch 50/100 | Train Loss: 0.4116 | Val Loss: 0.7198 | Val Corr: 0.4048 | Val RMSE: 1.1672\n",
      "   Epoch 60/100 | Train Loss: 0.3798 | Val Loss: 0.7687 | Val Corr: 0.4076 | Val RMSE: 1.1927\n",
      "   Epoch 70/100 | Train Loss: 0.3734 | Val Loss: 0.7148 | Val Corr: 0.4270 | Val RMSE: 1.1500\n",
      "   Epoch 80/100 | Train Loss: 0.3772 | Val Loss: 0.7168 | Val Corr: 0.4268 | Val RMSE: 1.1520\n",
      "Early stopping at epoch 90 (best epoch 75, best corr 0.4475)\n",
      "BEST epoch = 75 | BEST Corr = 0.4475 | RMSE@BEST = 1.1381\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7144 | Val Loss: 0.6125 | Val Corr: 0.3724 | Val RMSE: 0.9151\n",
      "   Epoch 10/100 | Train Loss: 0.6652 | Val Loss: 0.5901 | Val Corr: 0.4456 | Val RMSE: 0.8249\n",
      "   Epoch 20/100 | Train Loss: 0.5798 | Val Loss: 0.5855 | Val Corr: 0.4694 | Val RMSE: 0.8079\n",
      "   Epoch 30/100 | Train Loss: 0.5184 | Val Loss: 0.5669 | Val Corr: 0.5405 | Val RMSE: 0.7736\n",
      "   Epoch 40/100 | Train Loss: 0.4611 | Val Loss: 0.5758 | Val Corr: 0.5300 | Val RMSE: 0.7934\n",
      "Early stopping at epoch 43 (best epoch 28, best corr 0.5639)\n",
      "BEST epoch = 28 | BEST Corr = 0.5639 | RMSE@BEST = 0.7504\n",
      "----------------------------------------\n",
      "      Processing Trait: SNPP\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7620 | Val Loss: 0.6471 | Val Corr: 0.2919 | Val RMSE: 0.9072\n",
      "   Epoch 10/100 | Train Loss: 0.6980 | Val Loss: 0.6180 | Val Corr: 0.3825 | Val RMSE: 0.8457\n",
      "   Epoch 20/100 | Train Loss: 0.6563 | Val Loss: 0.5988 | Val Corr: 0.4258 | Val RMSE: 0.8236\n",
      "   Epoch 30/100 | Train Loss: 0.6143 | Val Loss: 0.5779 | Val Corr: 0.4540 | Val RMSE: 0.8080\n",
      "   Epoch 40/100 | Train Loss: 0.5948 | Val Loss: 0.5655 | Val Corr: 0.5073 | Val RMSE: 0.7916\n",
      "   Epoch 50/100 | Train Loss: 0.5498 | Val Loss: 0.5780 | Val Corr: 0.4475 | Val RMSE: 0.8185\n",
      "Early stopping at epoch 55 (best epoch 40, best corr 0.5073)\n",
      "BEST epoch = 40 | BEST Corr = 0.5073 | RMSE@BEST = 0.7916\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7434 | Val Loss: 0.6079 | Val Corr: 0.0833 | Val RMSE: 0.9600\n",
      "   Epoch 10/100 | Train Loss: 0.6848 | Val Loss: 0.6000 | Val Corr: 0.2296 | Val RMSE: 0.9336\n",
      "   Epoch 20/100 | Train Loss: 0.6187 | Val Loss: 0.5812 | Val Corr: 0.4039 | Val RMSE: 0.8822\n",
      "   Epoch 30/100 | Train Loss: 0.5511 | Val Loss: 0.5845 | Val Corr: 0.4024 | Val RMSE: 0.8864\n",
      "   Epoch 40/100 | Train Loss: 0.5798 | Val Loss: 0.6266 | Val Corr: 0.4028 | Val RMSE: 0.9202\n",
      "   Epoch 50/100 | Train Loss: 0.4918 | Val Loss: 0.5928 | Val Corr: 0.4171 | Val RMSE: 0.9039\n",
      "   Epoch 60/100 | Train Loss: 0.4730 | Val Loss: 0.5781 | Val Corr: 0.4684 | Val RMSE: 0.8775\n",
      "   Epoch 70/100 | Train Loss: 0.4223 | Val Loss: 0.5769 | Val Corr: 0.4543 | Val RMSE: 0.8793\n",
      "Early stopping at epoch 72 (best epoch 57, best corr 0.4759)\n",
      "BEST epoch = 57 | BEST Corr = 0.4759 | RMSE@BEST = 0.8698\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7239 | Val Loss: 0.7354 | Val Corr: 0.1415 | Val RMSE: 1.1211\n",
      "   Epoch 10/100 | Train Loss: 0.6526 | Val Loss: 0.6891 | Val Corr: 0.4519 | Val RMSE: 1.0554\n",
      "   Epoch 20/100 | Train Loss: 0.6124 | Val Loss: 0.6836 | Val Corr: 0.4817 | Val RMSE: 1.0338\n",
      "   Epoch 30/100 | Train Loss: 0.5632 | Val Loss: 0.6825 | Val Corr: 0.5270 | Val RMSE: 0.9629\n",
      "   Epoch 40/100 | Train Loss: 0.5499 | Val Loss: 0.6855 | Val Corr: 0.4825 | Val RMSE: 0.9767\n",
      "   Epoch 50/100 | Train Loss: 0.5058 | Val Loss: 0.7305 | Val Corr: 0.4814 | Val RMSE: 0.9871\n",
      "Early stopping at epoch 51 (best epoch 36, best corr 0.5611)\n",
      "BEST epoch = 36 | BEST Corr = 0.5611 | RMSE@BEST = 0.9301\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7268 | Val Loss: 0.7316 | Val Corr: 0.0477 | Val RMSE: 1.1004\n",
      "   Epoch 10/100 | Train Loss: 0.6351 | Val Loss: 0.7487 | Val Corr: 0.0904 | Val RMSE: 1.0724\n",
      "   Epoch 20/100 | Train Loss: 0.5959 | Val Loss: 0.7254 | Val Corr: 0.1819 | Val RMSE: 1.0993\n",
      "Early stopping at epoch 30 (best epoch 15, best corr 0.1933)\n",
      "BEST epoch = 15 | BEST Corr = 0.1933 | RMSE@BEST = 1.0913\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7564 | Val Loss: 0.5732 | Val Corr: 0.1888 | Val RMSE: 0.9504\n",
      "   Epoch 10/100 | Train Loss: 0.6892 | Val Loss: 0.5379 | Val Corr: 0.2837 | Val RMSE: 0.8947\n",
      "   Epoch 20/100 | Train Loss: 0.6096 | Val Loss: 0.5732 | Val Corr: 0.2888 | Val RMSE: 0.8924\n",
      "   Epoch 30/100 | Train Loss: 0.5683 | Val Loss: 0.5911 | Val Corr: 0.3202 | Val RMSE: 0.9416\n",
      "Early stopping at epoch 33 (best epoch 18, best corr 0.3669)\n",
      "BEST epoch = 18 | BEST Corr = 0.3669 | RMSE@BEST = 0.8853\n",
      "----------------------------------------\n",
      "   [Saved] rice_T5000_MAP.json\n",
      "\n",
      "   >>> Processing Pooling Type: AVG | Species: rice | Feature Count: T5000\n",
      "      Processing Trait: SW\n",
      "Seed 0 | Pool AVG | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 0: Given input size: (64x1x2). Calculated output size: (64x1x0). Output size is too small\n",
      "Seed 1 | Pool AVG | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 1: Given input size: (64x1x2). Calculated output size: (64x1x0). Output size is too small\n",
      "Seed 2 | Pool AVG | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 2: Given input size: (64x1x2). Calculated output size: (64x1x0). Output size is too small\n",
      "Seed 3 | Pool AVG | Train Samples: 331 | Test Samples: 82\n",
      "         [Error] Seed 3: Given input size: (64x1x2). Calculated output size: (64x1x0). Output size is too small\n",
      "Seed 4 | Pool AVG | Train Samples: 331 | Test Samples: 82\n",
      "         [Error] Seed 4: Given input size: (64x1x2). Calculated output size: (64x1x0). Output size is too small\n",
      "      Processing Trait: FLW\n",
      "Seed 0 | Pool AVG | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 0: Given input size: (64x1x2). Calculated output size: (64x1x0). Output size is too small\n",
      "Seed 1 | Pool AVG | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 1: Given input size: (64x1x2). Calculated output size: (64x1x0). Output size is too small\n",
      "Seed 2 | Pool AVG | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 2: Given input size: (64x1x2). Calculated output size: (64x1x0). Output size is too small\n",
      "Seed 3 | Pool AVG | Train Samples: 331 | Test Samples: 82\n",
      "         [Error] Seed 3: Given input size: (64x1x2). Calculated output size: (64x1x0). Output size is too small\n",
      "Seed 4 | Pool AVG | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7157 | Val Loss: 0.5681 | Val Corr: 0.3487 | Val RMSE: 0.8283\n",
      "   Epoch 10/100 | Train Loss: 0.6186 | Val Loss: 0.5759 | Val Corr: 0.3903 | Val RMSE: 0.7755\n",
      "   Epoch 20/100 | Train Loss: 0.5610 | Val Loss: 0.6707 | Val Corr: 0.3793 | Val RMSE: 0.8916\n",
      "Early stopping at epoch 29 (best epoch 14, best corr 0.3975)\n",
      "BEST epoch = 14 | BEST Corr = 0.3975 | RMSE@BEST = 0.7678\n",
      "----------------------------------------\n",
      "      Processing Trait: AC\n",
      "Seed 0 | Pool AVG | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 0: Given input size: (64x1x2). Calculated output size: (64x1x0). Output size is too small\n",
      "Seed 1 | Pool AVG | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 1: Given input size: (64x1x2). Calculated output size: (64x1x0). Output size is too small\n",
      "Seed 2 | Pool AVG | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 2: Given input size: (64x1x2). Calculated output size: (64x1x0). Output size is too small\n",
      "Seed 3 | Pool AVG | Train Samples: 331 | Test Samples: 82\n",
      "         [Error] Seed 3: Given input size: (64x1x2). Calculated output size: (64x1x0). Output size is too small\n",
      "Seed 4 | Pool AVG | Train Samples: 331 | Test Samples: 82\n",
      "         [Error] Seed 4: Given input size: (64x1x2). Calculated output size: (64x1x0). Output size is too small\n",
      "      Processing Trait: PH\n",
      "Seed 0 | Pool AVG | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 0: Given input size: (64x1x2). Calculated output size: (64x1x0). Output size is too small\n",
      "Seed 1 | Pool AVG | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7235 | Val Loss: 0.5999 | Val Corr: 0.3333 | Val RMSE: 0.8668\n",
      "   Epoch 10/100 | Train Loss: 0.5975 | Val Loss: 0.5397 | Val Corr: 0.4158 | Val RMSE: 0.7993\n",
      "   Epoch 20/100 | Train Loss: 0.5662 | Val Loss: 0.6019 | Val Corr: 0.3477 | Val RMSE: 0.8361\n",
      "Early stopping at epoch 30 (best epoch 15, best corr 0.4522)\n",
      "BEST epoch = 15 | BEST Corr = 0.4522 | RMSE@BEST = 0.8109\n",
      "----------------------------------------\n",
      "Seed 2 | Pool AVG | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.6866 | Val Loss: 0.6684 | Val Corr: 0.0008 | Val RMSE: 0.9973\n",
      "   Epoch 10/100 | Train Loss: 0.6337 | Val Loss: 0.6987 | Val Corr: 0.2202 | Val RMSE: 0.9767\n",
      "   Epoch 20/100 | Train Loss: 0.5660 | Val Loss: 0.6498 | Val Corr: 0.3541 | Val RMSE: 0.9338\n",
      "   Epoch 30/100 | Train Loss: 0.5007 | Val Loss: 0.7041 | Val Corr: 0.3581 | Val RMSE: 0.9720\n",
      "   Epoch 40/100 | Train Loss: 0.4383 | Val Loss: 0.6636 | Val Corr: 0.4075 | Val RMSE: 0.9171\n",
      "   Epoch 50/100 | Train Loss: 0.3968 | Val Loss: 0.7820 | Val Corr: 0.3745 | Val RMSE: 1.0302\n",
      "Early stopping at epoch 53 (best epoch 38, best corr 0.4410)\n",
      "BEST epoch = 38 | BEST Corr = 0.4410 | RMSE@BEST = 0.9179\n",
      "----------------------------------------\n",
      "Seed 3 | Pool AVG | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.6556 | Val Loss: 0.8811 | Val Corr: 0.1855 | Val RMSE: 1.2717\n",
      "   Epoch 10/100 | Train Loss: 0.5684 | Val Loss: 0.7900 | Val Corr: 0.3012 | Val RMSE: 1.2190\n",
      "   Epoch 20/100 | Train Loss: 0.4919 | Val Loss: 0.7819 | Val Corr: 0.3365 | Val RMSE: 1.2110\n",
      "   Epoch 30/100 | Train Loss: 0.4571 | Val Loss: 0.7527 | Val Corr: 0.3898 | Val RMSE: 1.1851\n",
      "   Epoch 40/100 | Train Loss: 0.3815 | Val Loss: 0.7290 | Val Corr: 0.4290 | Val RMSE: 1.1772\n",
      "   Epoch 50/100 | Train Loss: 0.3865 | Val Loss: 0.7382 | Val Corr: 0.4378 | Val RMSE: 1.1726\n",
      "Early stopping at epoch 54 (best epoch 39, best corr 0.4421)\n",
      "BEST epoch = 39 | BEST Corr = 0.4421 | RMSE@BEST = 1.1631\n",
      "----------------------------------------\n",
      "Seed 4 | Pool AVG | Train Samples: 331 | Test Samples: 82\n",
      "         [Error] Seed 4: Given input size: (64x1x2). Calculated output size: (64x1x0). Output size is too small\n",
      "      Processing Trait: SNPP\n",
      "Seed 0 | Pool AVG | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 0: Given input size: (64x1x2). Calculated output size: (64x1x0). Output size is too small\n",
      "Seed 1 | Pool AVG | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.6698 | Val Loss: 0.6336 | Val Corr: 0.0396 | Val RMSE: 0.9611\n",
      "   Epoch 10/100 | Train Loss: 0.6753 | Val Loss: 0.5860 | Val Corr: 0.2523 | Val RMSE: 0.9301\n",
      "   Epoch 20/100 | Train Loss: 0.6016 | Val Loss: 0.6263 | Val Corr: 0.4036 | Val RMSE: 0.9169\n",
      "   Epoch 30/100 | Train Loss: 0.5121 | Val Loss: 0.6663 | Val Corr: 0.4817 | Val RMSE: 0.9085\n",
      "   Epoch 40/100 | Train Loss: 0.5255 | Val Loss: 0.6075 | Val Corr: 0.5185 | Val RMSE: 0.8547\n",
      "   Epoch 50/100 | Train Loss: 0.4505 | Val Loss: 0.6518 | Val Corr: 0.4643 | Val RMSE: 0.8902\n",
      "Early stopping at epoch 55 (best epoch 40, best corr 0.5185)\n",
      "BEST epoch = 40 | BEST Corr = 0.5185 | RMSE@BEST = 0.8547\n",
      "----------------------------------------\n",
      "Seed 2 | Pool AVG | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 2: Given input size: (64x1x2). Calculated output size: (64x1x0). Output size is too small\n",
      "Seed 3 | Pool AVG | Train Samples: 331 | Test Samples: 82\n",
      "         [Error] Seed 3: Given input size: (64x1x2). Calculated output size: (64x1x0). Output size is too small\n",
      "Seed 4 | Pool AVG | Train Samples: 331 | Test Samples: 82\n",
      "         [Error] Seed 4: Given input size: (64x1x2). Calculated output size: (64x1x0). Output size is too small\n",
      "   [Saved] rice_T5000_AVG.json\n",
      "\n",
      "   >>> Processing Pooling Type: MAX | Species: rice | Feature Count: T5000\n",
      "      Processing Trait: SW\n",
      "Seed 0 | Pool MAX | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 0: max_pool1d() Invalid computed output size: 0\n",
      "Seed 1 | Pool MAX | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 1: max_pool1d() Invalid computed output size: 0\n",
      "Seed 2 | Pool MAX | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 2: max_pool1d() Invalid computed output size: 0\n",
      "Seed 3 | Pool MAX | Train Samples: 331 | Test Samples: 82\n",
      "         [Error] Seed 3: max_pool1d() Invalid computed output size: 0\n",
      "Seed 4 | Pool MAX | Train Samples: 331 | Test Samples: 82\n",
      "         [Error] Seed 4: max_pool1d() Invalid computed output size: 0\n",
      "      Processing Trait: FLW\n",
      "Seed 0 | Pool MAX | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 0: max_pool1d() Invalid computed output size: 0\n",
      "Seed 1 | Pool MAX | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 1: max_pool1d() Invalid computed output size: 0\n",
      "Seed 2 | Pool MAX | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 2: max_pool1d() Invalid computed output size: 0\n",
      "Seed 3 | Pool MAX | Train Samples: 331 | Test Samples: 82\n",
      "         [Error] Seed 3: max_pool1d() Invalid computed output size: 0\n",
      "Seed 4 | Pool MAX | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7222 | Val Loss: 0.5633 | Val Corr: 0.1913 | Val RMSE: 0.8287\n",
      "   Epoch 10/100 | Train Loss: 0.6630 | Val Loss: 0.5696 | Val Corr: 0.3570 | Val RMSE: 0.8022\n",
      "   Epoch 20/100 | Train Loss: 0.6349 | Val Loss: 0.6054 | Val Corr: 0.3594 | Val RMSE: 0.8167\n",
      "   Epoch 30/100 | Train Loss: 0.5640 | Val Loss: 0.6127 | Val Corr: 0.3213 | Val RMSE: 0.8208\n",
      "Early stopping at epoch 32 (best epoch 17, best corr 0.3739)\n",
      "BEST epoch = 17 | BEST Corr = 0.3739 | RMSE@BEST = 0.7872\n",
      "----------------------------------------\n",
      "      Processing Trait: AC\n",
      "Seed 0 | Pool MAX | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 0: max_pool1d() Invalid computed output size: 0\n",
      "Seed 1 | Pool MAX | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 1: max_pool1d() Invalid computed output size: 0\n",
      "Seed 2 | Pool MAX | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 2: max_pool1d() Invalid computed output size: 0\n",
      "Seed 3 | Pool MAX | Train Samples: 331 | Test Samples: 82\n",
      "         [Error] Seed 3: max_pool1d() Invalid computed output size: 0\n",
      "Seed 4 | Pool MAX | Train Samples: 331 | Test Samples: 82\n",
      "         [Error] Seed 4: max_pool1d() Invalid computed output size: 0\n",
      "      Processing Trait: PH\n",
      "Seed 0 | Pool MAX | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 0: max_pool1d() Invalid computed output size: 0\n",
      "Seed 1 | Pool MAX | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7341 | Val Loss: 0.6142 | Val Corr: 0.3670 | Val RMSE: 0.8733\n",
      "   Epoch 10/100 | Train Loss: 0.6913 | Val Loss: 0.5764 | Val Corr: 0.3743 | Val RMSE: 0.8436\n",
      "   Epoch 20/100 | Train Loss: 0.6506 | Val Loss: 0.5746 | Val Corr: 0.4981 | Val RMSE: 0.8256\n",
      "   Epoch 30/100 | Train Loss: 0.5539 | Val Loss: 0.5663 | Val Corr: 0.4114 | Val RMSE: 0.8151\n",
      "Early stopping at epoch 35 (best epoch 20, best corr 0.4981)\n",
      "BEST epoch = 20 | BEST Corr = 0.4981 | RMSE@BEST = 0.8256\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAX | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.6982 | Val Loss: 0.6724 | Val Corr: 0.0466 | Val RMSE: 0.9988\n",
      "   Epoch 10/100 | Train Loss: 0.6561 | Val Loss: 0.6708 | Val Corr: 0.0956 | Val RMSE: 0.9892\n",
      "   Epoch 20/100 | Train Loss: 0.6127 | Val Loss: 0.6466 | Val Corr: 0.3552 | Val RMSE: 0.9535\n",
      "   Epoch 30/100 | Train Loss: 0.5624 | Val Loss: 0.6381 | Val Corr: 0.4100 | Val RMSE: 0.9143\n",
      "   Epoch 40/100 | Train Loss: 0.5164 | Val Loss: 0.6336 | Val Corr: 0.4513 | Val RMSE: 0.8866\n",
      "   Epoch 50/100 | Train Loss: 0.4855 | Val Loss: 0.6621 | Val Corr: 0.4382 | Val RMSE: 0.9111\n",
      "   Epoch 60/100 | Train Loss: 0.4622 | Val Loss: 0.6464 | Val Corr: 0.4534 | Val RMSE: 0.8988\n",
      "Early stopping at epoch 61 (best epoch 46, best corr 0.4756)\n",
      "BEST epoch = 46 | BEST Corr = 0.4756 | RMSE@BEST = 0.8768\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAX | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.6698 | Val Loss: 0.8829 | Val Corr: 0.1229 | Val RMSE: 1.2727\n",
      "   Epoch 10/100 | Train Loss: 0.6123 | Val Loss: 0.8580 | Val Corr: 0.1820 | Val RMSE: 1.2618\n",
      "   Epoch 20/100 | Train Loss: 0.5945 | Val Loss: 0.8114 | Val Corr: 0.2944 | Val RMSE: 1.2321\n",
      "   Epoch 30/100 | Train Loss: 0.5320 | Val Loss: 0.7772 | Val Corr: 0.3406 | Val RMSE: 1.2147\n",
      "   Epoch 40/100 | Train Loss: 0.4651 | Val Loss: 0.7524 | Val Corr: 0.3931 | Val RMSE: 1.1887\n",
      "   Epoch 50/100 | Train Loss: 0.4507 | Val Loss: 0.7147 | Val Corr: 0.4210 | Val RMSE: 1.1676\n",
      "   Epoch 60/100 | Train Loss: 0.4001 | Val Loss: 0.7468 | Val Corr: 0.4362 | Val RMSE: 1.1780\n",
      "   Epoch 70/100 | Train Loss: 0.3901 | Val Loss: 0.7455 | Val Corr: 0.4509 | Val RMSE: 1.1674\n",
      "   Epoch 80/100 | Train Loss: 0.3668 | Val Loss: 0.7641 | Val Corr: 0.4609 | Val RMSE: 1.1780\n",
      "   Epoch 90/100 | Train Loss: 0.3593 | Val Loss: 0.7607 | Val Corr: 0.4629 | Val RMSE: 1.1770\n",
      "Early stopping at epoch 97 (best epoch 82, best corr 0.4649)\n",
      "BEST epoch = 82 | BEST Corr = 0.4649 | RMSE@BEST = 1.1597\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAX | Train Samples: 331 | Test Samples: 82\n",
      "         [Error] Seed 4: max_pool1d() Invalid computed output size: 0\n",
      "      Processing Trait: SNPP\n",
      "Seed 0 | Pool MAX | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 0: max_pool1d() Invalid computed output size: 0\n",
      "Seed 1 | Pool MAX | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.6866 | Val Loss: 0.6550 | Val Corr: -0.0043 | Val RMSE: 0.9675\n",
      "   Epoch 10/100 | Train Loss: 0.7026 | Val Loss: 0.6263 | Val Corr: 0.1148 | Val RMSE: 0.9556\n",
      "   Epoch 20/100 | Train Loss: 0.6622 | Val Loss: 0.6578 | Val Corr: 0.2225 | Val RMSE: 0.9659\n",
      "   Epoch 30/100 | Train Loss: 0.6113 | Val Loss: 0.6571 | Val Corr: 0.3558 | Val RMSE: 0.9494\n",
      "   Epoch 40/100 | Train Loss: 0.5746 | Val Loss: 0.5962 | Val Corr: 0.4285 | Val RMSE: 0.8923\n",
      "   Epoch 50/100 | Train Loss: 0.5113 | Val Loss: 0.6869 | Val Corr: 0.4609 | Val RMSE: 0.9392\n",
      "   Epoch 60/100 | Train Loss: 0.5262 | Val Loss: 0.6050 | Val Corr: 0.4912 | Val RMSE: 0.8681\n",
      "Early stopping at epoch 70 (best epoch 55, best corr 0.4995)\n",
      "BEST epoch = 55 | BEST Corr = 0.4995 | RMSE@BEST = 0.8907\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAX | Train Samples: 330 | Test Samples: 83\n",
      "         [Error] Seed 2: max_pool1d() Invalid computed output size: 0\n",
      "Seed 3 | Pool MAX | Train Samples: 331 | Test Samples: 82\n",
      "         [Error] Seed 3: max_pool1d() Invalid computed output size: 0\n",
      "Seed 4 | Pool MAX | Train Samples: 331 | Test Samples: 82\n",
      "         [Error] Seed 4: max_pool1d() Invalid computed output size: 0\n",
      "   [Saved] rice_T5000_MAX.json\n",
      "\n",
      "   >>> Processing Pooling Type: LIP | Species: rice | Feature Count: T5000\n",
      "      Processing Trait: SW\n",
      "Seed 0 | Pool LIP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7429 | Val Loss: 0.6801 | Val Corr: 0.4452 | Val RMSE: 0.8989\n",
      "   Epoch 10/100 | Train Loss: 0.6777 | Val Loss: 0.6039 | Val Corr: 0.5813 | Val RMSE: 0.8171\n",
      "   Epoch 20/100 | Train Loss: 0.5545 | Val Loss: 0.5744 | Val Corr: 0.5542 | Val RMSE: 0.7767\n",
      "   Epoch 30/100 | Train Loss: 0.4836 | Val Loss: 0.5252 | Val Corr: 0.6070 | Val RMSE: 0.7269\n",
      "Early stopping at epoch 38 (best epoch 23, best corr 0.6383)\n",
      "BEST epoch = 23 | BEST Corr = 0.6383 | RMSE@BEST = 0.7421\n",
      "----------------------------------------\n",
      "Seed 1 | Pool LIP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7446 | Val Loss: 0.7084 | Val Corr: 0.3868 | Val RMSE: 1.0514\n",
      "   Epoch 10/100 | Train Loss: 0.6065 | Val Loss: 0.6462 | Val Corr: 0.5744 | Val RMSE: 0.9551\n",
      "   Epoch 20/100 | Train Loss: 0.5321 | Val Loss: 0.5876 | Val Corr: 0.6074 | Val RMSE: 0.8525\n",
      "   Epoch 30/100 | Train Loss: 0.4687 | Val Loss: 0.5773 | Val Corr: 0.6209 | Val RMSE: 0.8267\n",
      "   Epoch 40/100 | Train Loss: 0.4313 | Val Loss: 0.5806 | Val Corr: 0.6368 | Val RMSE: 0.8184\n",
      "   Epoch 50/100 | Train Loss: 0.3794 | Val Loss: 0.6432 | Val Corr: 0.6477 | Val RMSE: 0.8429\n",
      "   Epoch 60/100 | Train Loss: 0.3501 | Val Loss: 0.6271 | Val Corr: 0.6352 | Val RMSE: 0.8373\n",
      "Early stopping at epoch 66 (best epoch 51, best corr 0.6532)\n",
      "BEST epoch = 51 | BEST Corr = 0.6532 | RMSE@BEST = 0.8732\n",
      "----------------------------------------\n",
      "Seed 2 | Pool LIP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7251 | Val Loss: 0.8003 | Val Corr: 0.4394 | Val RMSE: 1.1145\n",
      "   Epoch 10/100 | Train Loss: 0.6223 | Val Loss: 0.6739 | Val Corr: 0.5799 | Val RMSE: 0.9625\n",
      "   Epoch 20/100 | Train Loss: 0.5263 | Val Loss: 0.6494 | Val Corr: 0.6228 | Val RMSE: 0.9351\n",
      "   Epoch 30/100 | Train Loss: 0.4751 | Val Loss: 0.6837 | Val Corr: 0.5996 | Val RMSE: 0.9723\n",
      "   Epoch 40/100 | Train Loss: 0.4229 | Val Loss: 0.6298 | Val Corr: 0.6274 | Val RMSE: 0.9050\n",
      "   Epoch 50/100 | Train Loss: 0.4099 | Val Loss: 0.6192 | Val Corr: 0.6388 | Val RMSE: 0.9007\n",
      "   Epoch 60/100 | Train Loss: 0.3516 | Val Loss: 0.6410 | Val Corr: 0.6446 | Val RMSE: 0.9041\n",
      "Early stopping at epoch 66 (best epoch 51, best corr 0.6545)\n",
      "BEST epoch = 51 | BEST Corr = 0.6545 | RMSE@BEST = 0.9157\n",
      "----------------------------------------\n",
      "Seed 3 | Pool LIP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7398 | Val Loss: 0.7154 | Val Corr: 0.2393 | Val RMSE: 0.9435\n",
      "   Epoch 10/100 | Train Loss: 0.6298 | Val Loss: 0.7390 | Val Corr: 0.3910 | Val RMSE: 0.9381\n",
      "   Epoch 20/100 | Train Loss: 0.5110 | Val Loss: 0.5722 | Val Corr: 0.4642 | Val RMSE: 0.8335\n",
      "   Epoch 30/100 | Train Loss: 0.4680 | Val Loss: 0.5699 | Val Corr: 0.4868 | Val RMSE: 0.8567\n",
      "   Epoch 40/100 | Train Loss: 0.4112 | Val Loss: 0.6526 | Val Corr: 0.5028 | Val RMSE: 0.8986\n",
      "   Epoch 50/100 | Train Loss: 0.3623 | Val Loss: 0.6603 | Val Corr: 0.5097 | Val RMSE: 0.9161\n",
      "   Epoch 60/100 | Train Loss: 0.3620 | Val Loss: 0.6256 | Val Corr: 0.5227 | Val RMSE: 0.9182\n",
      "Early stopping at epoch 63 (best epoch 48, best corr 0.5266)\n",
      "BEST epoch = 48 | BEST Corr = 0.5266 | RMSE@BEST = 0.8831\n",
      "----------------------------------------\n",
      "Seed 4 | Pool LIP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7551 | Val Loss: 0.7042 | Val Corr: 0.3807 | Val RMSE: 0.9827\n",
      "   Epoch 10/100 | Train Loss: 0.5914 | Val Loss: 0.6560 | Val Corr: 0.4471 | Val RMSE: 0.8803\n",
      "   Epoch 20/100 | Train Loss: 0.4965 | Val Loss: 0.6328 | Val Corr: 0.4694 | Val RMSE: 0.8695\n",
      "   Epoch 30/100 | Train Loss: 0.4591 | Val Loss: 0.6968 | Val Corr: 0.4644 | Val RMSE: 0.9184\n",
      "   Epoch 40/100 | Train Loss: 0.3943 | Val Loss: 0.7404 | Val Corr: 0.4859 | Val RMSE: 0.9657\n",
      "   Epoch 50/100 | Train Loss: 0.3685 | Val Loss: 0.7306 | Val Corr: 0.4709 | Val RMSE: 0.9647\n",
      "Early stopping at epoch 52 (best epoch 37, best corr 0.4978)\n",
      "BEST epoch = 37 | BEST Corr = 0.4978 | RMSE@BEST = 1.0235\n",
      "----------------------------------------\n",
      "      Processing Trait: FLW\n",
      "Seed 0 | Pool LIP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.6800 | Val Loss: 0.6834 | Val Corr: 0.3641 | Val RMSE: 1.0421\n",
      "   Epoch 10/100 | Train Loss: 0.6307 | Val Loss: 0.6517 | Val Corr: 0.3975 | Val RMSE: 0.9777\n",
      "   Epoch 20/100 | Train Loss: 0.6044 | Val Loss: 0.6269 | Val Corr: 0.4065 | Val RMSE: 0.9534\n",
      "   Epoch 30/100 | Train Loss: 0.5180 | Val Loss: 0.6198 | Val Corr: 0.4306 | Val RMSE: 0.9392\n",
      "   Epoch 40/100 | Train Loss: 0.4775 | Val Loss: 0.6195 | Val Corr: 0.4030 | Val RMSE: 0.9539\n",
      "   Epoch 50/100 | Train Loss: 0.4674 | Val Loss: 0.6226 | Val Corr: 0.3824 | Val RMSE: 0.9763\n",
      "Early stopping at epoch 56 (best epoch 41, best corr 0.4496)\n",
      "BEST epoch = 41 | BEST Corr = 0.4496 | RMSE@BEST = 0.9287\n",
      "----------------------------------------\n",
      "Seed 1 | Pool LIP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.6980 | Val Loss: 0.7433 | Val Corr: 0.0074 | Val RMSE: 1.1584\n",
      "   Epoch 10/100 | Train Loss: 0.6011 | Val Loss: 0.7345 | Val Corr: 0.3343 | Val RMSE: 1.0959\n",
      "   Epoch 20/100 | Train Loss: 0.5484 | Val Loss: 0.7105 | Val Corr: 0.4357 | Val RMSE: 1.0640\n",
      "   Epoch 30/100 | Train Loss: 0.4923 | Val Loss: 0.7023 | Val Corr: 0.4499 | Val RMSE: 1.0405\n",
      "   Epoch 40/100 | Train Loss: 0.5113 | Val Loss: 0.7663 | Val Corr: 0.3736 | Val RMSE: 1.0808\n",
      "Early stopping at epoch 45 (best epoch 30, best corr 0.4499)\n",
      "BEST epoch = 30 | BEST Corr = 0.4499 | RMSE@BEST = 1.0405\n",
      "----------------------------------------\n",
      "Seed 2 | Pool LIP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.6927 | Val Loss: 0.7266 | Val Corr: 0.3709 | Val RMSE: 1.0790\n",
      "   Epoch 10/100 | Train Loss: 0.6201 | Val Loss: 0.6938 | Val Corr: 0.3441 | Val RMSE: 1.0177\n",
      "   Epoch 20/100 | Train Loss: 0.5855 | Val Loss: 0.6560 | Val Corr: 0.4247 | Val RMSE: 0.9813\n",
      "   Epoch 30/100 | Train Loss: 0.5624 | Val Loss: 0.6281 | Val Corr: 0.4603 | Val RMSE: 0.9589\n",
      "Early stopping at epoch 32 (best epoch 17, best corr 0.4750)\n",
      "BEST epoch = 17 | BEST Corr = 0.4750 | RMSE@BEST = 0.9680\n",
      "----------------------------------------\n",
      "Seed 3 | Pool LIP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7049 | Val Loss: 0.6565 | Val Corr: 0.2379 | Val RMSE: 0.8586\n",
      "   Epoch 10/100 | Train Loss: 0.7188 | Val Loss: 0.6317 | Val Corr: 0.3293 | Val RMSE: 0.8213\n",
      "   Epoch 20/100 | Train Loss: 0.5905 | Val Loss: 0.5860 | Val Corr: 0.3598 | Val RMSE: 0.8028\n",
      "   Epoch 30/100 | Train Loss: 0.5423 | Val Loss: 0.6373 | Val Corr: 0.3931 | Val RMSE: 0.8296\n",
      "   Epoch 40/100 | Train Loss: 0.5176 | Val Loss: 0.6710 | Val Corr: 0.4218 | Val RMSE: 0.8728\n",
      "Early stopping at epoch 48 (best epoch 33, best corr 0.4333)\n",
      "BEST epoch = 33 | BEST Corr = 0.4333 | RMSE@BEST = 0.7940\n",
      "----------------------------------------\n",
      "Seed 4 | Pool LIP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7202 | Val Loss: 0.5881 | Val Corr: 0.2611 | Val RMSE: 0.8302\n",
      "   Epoch 10/100 | Train Loss: 0.6378 | Val Loss: 0.5916 | Val Corr: 0.3671 | Val RMSE: 0.7862\n",
      "   Epoch 20/100 | Train Loss: 0.5829 | Val Loss: 0.6454 | Val Corr: 0.3649 | Val RMSE: 0.8729\n",
      "Early stopping at epoch 27 (best epoch 12, best corr 0.3821)\n",
      "BEST epoch = 12 | BEST Corr = 0.3821 | RMSE@BEST = 0.7677\n",
      "----------------------------------------\n",
      "      Processing Trait: AC\n",
      "Seed 0 | Pool LIP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.6826 | Val Loss: 0.5704 | Val Corr: 0.1591 | Val RMSE: 0.9372\n",
      "   Epoch 10/100 | Train Loss: 0.6019 | Val Loss: 0.5266 | Val Corr: 0.2732 | Val RMSE: 0.9144\n",
      "   Epoch 20/100 | Train Loss: 0.5796 | Val Loss: 0.5070 | Val Corr: 0.3399 | Val RMSE: 0.8959\n",
      "   Epoch 30/100 | Train Loss: 0.5217 | Val Loss: 0.4767 | Val Corr: 0.3352 | Val RMSE: 0.9059\n",
      "   Epoch 40/100 | Train Loss: 0.4645 | Val Loss: 0.4708 | Val Corr: 0.3854 | Val RMSE: 0.8742\n",
      "   Epoch 50/100 | Train Loss: 0.4014 | Val Loss: 0.5220 | Val Corr: 0.3349 | Val RMSE: 0.9312\n",
      "Early stopping at epoch 56 (best epoch 41, best corr 0.4029)\n",
      "BEST epoch = 41 | BEST Corr = 0.4029 | RMSE@BEST = 0.8678\n",
      "----------------------------------------\n",
      "Seed 1 | Pool LIP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.6735 | Val Loss: 0.5884 | Val Corr: -0.0128 | Val RMSE: 0.9283\n",
      "   Epoch 10/100 | Train Loss: 0.5934 | Val Loss: 0.5421 | Val Corr: 0.3401 | Val RMSE: 0.8938\n",
      "   Epoch 20/100 | Train Loss: 0.5244 | Val Loss: 0.5200 | Val Corr: 0.3706 | Val RMSE: 0.8961\n",
      "   Epoch 30/100 | Train Loss: 0.4615 | Val Loss: 0.5293 | Val Corr: 0.3992 | Val RMSE: 0.8898\n",
      "   Epoch 40/100 | Train Loss: 0.4567 | Val Loss: 0.5255 | Val Corr: 0.4120 | Val RMSE: 0.8804\n",
      "   Epoch 50/100 | Train Loss: 0.3813 | Val Loss: 0.5960 | Val Corr: 0.3775 | Val RMSE: 0.9650\n",
      "Early stopping at epoch 52 (best epoch 37, best corr 0.4384)\n",
      "BEST epoch = 37 | BEST Corr = 0.4384 | RMSE@BEST = 0.8926\n",
      "----------------------------------------\n",
      "Seed 2 | Pool LIP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.6553 | Val Loss: 0.6527 | Val Corr: -0.0722 | Val RMSE: 1.0668\n",
      "   Epoch 10/100 | Train Loss: 0.5599 | Val Loss: 0.6284 | Val Corr: 0.2194 | Val RMSE: 1.0468\n",
      "   Epoch 20/100 | Train Loss: 0.5798 | Val Loss: 0.6058 | Val Corr: 0.3604 | Val RMSE: 1.0047\n",
      "   Epoch 30/100 | Train Loss: 0.4711 | Val Loss: 0.6008 | Val Corr: 0.3317 | Val RMSE: 1.0149\n",
      "   Epoch 40/100 | Train Loss: 0.4333 | Val Loss: 0.6392 | Val Corr: 0.3187 | Val RMSE: 1.1004\n",
      "Early stopping at epoch 44 (best epoch 29, best corr 0.3776)\n",
      "BEST epoch = 29 | BEST Corr = 0.3776 | RMSE@BEST = 0.9881\n",
      "----------------------------------------\n",
      "Seed 3 | Pool LIP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.6068 | Val Loss: 0.7562 | Val Corr: 0.1808 | Val RMSE: 1.1156\n",
      "   Epoch 10/100 | Train Loss: 0.5385 | Val Loss: 0.7726 | Val Corr: 0.1016 | Val RMSE: 1.1024\n",
      "   Epoch 20/100 | Train Loss: 0.5112 | Val Loss: 0.6807 | Val Corr: 0.3042 | Val RMSE: 1.0944\n",
      "   Epoch 30/100 | Train Loss: 0.4476 | Val Loss: 0.6638 | Val Corr: 0.3938 | Val RMSE: 1.0699\n",
      "   Epoch 40/100 | Train Loss: 0.3953 | Val Loss: 0.6795 | Val Corr: 0.3524 | Val RMSE: 1.0938\n",
      "   Epoch 50/100 | Train Loss: 0.3590 | Val Loss: 0.6841 | Val Corr: 0.4874 | Val RMSE: 0.9872\n",
      "Early stopping at epoch 58 (best epoch 43, best corr 0.5084)\n",
      "BEST epoch = 43 | BEST Corr = 0.5084 | RMSE@BEST = 0.9598\n",
      "----------------------------------------\n",
      "Seed 4 | Pool LIP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.6620 | Val Loss: 0.6031 | Val Corr: 0.2276 | Val RMSE: 0.9541\n",
      "   Epoch 10/100 | Train Loss: 0.5862 | Val Loss: 0.5415 | Val Corr: 0.4187 | Val RMSE: 0.8896\n",
      "   Epoch 20/100 | Train Loss: 0.5058 | Val Loss: 0.5942 | Val Corr: 0.4873 | Val RMSE: 0.8568\n",
      "   Epoch 30/100 | Train Loss: 0.4825 | Val Loss: 0.6217 | Val Corr: 0.5416 | Val RMSE: 0.8185\n",
      "   Epoch 40/100 | Train Loss: 0.4396 | Val Loss: 0.5523 | Val Corr: 0.5098 | Val RMSE: 0.8148\n",
      "Early stopping at epoch 46 (best epoch 31, best corr 0.5640)\n",
      "BEST epoch = 31 | BEST Corr = 0.5640 | RMSE@BEST = 0.8445\n",
      "----------------------------------------\n",
      "      Processing Trait: PH\n",
      "Seed 0 | Pool LIP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7093 | Val Loss: 0.6274 | Val Corr: 0.4154 | Val RMSE: 0.9054\n",
      "   Epoch 10/100 | Train Loss: 0.6618 | Val Loss: 0.6121 | Val Corr: 0.5122 | Val RMSE: 0.8578\n",
      "   Epoch 20/100 | Train Loss: 0.6064 | Val Loss: 0.5564 | Val Corr: 0.6277 | Val RMSE: 0.7641\n",
      "   Epoch 30/100 | Train Loss: 0.5846 | Val Loss: 0.5353 | Val Corr: 0.6800 | Val RMSE: 0.7250\n",
      "   Epoch 40/100 | Train Loss: 0.4917 | Val Loss: 0.5139 | Val Corr: 0.6212 | Val RMSE: 0.7094\n",
      "   Epoch 50/100 | Train Loss: 0.4957 | Val Loss: 0.5194 | Val Corr: 0.6839 | Val RMSE: 0.6916\n",
      "Early stopping at epoch 51 (best epoch 36, best corr 0.6860)\n",
      "BEST epoch = 36 | BEST Corr = 0.6860 | RMSE@BEST = 0.7149\n",
      "----------------------------------------\n",
      "Seed 1 | Pool LIP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7262 | Val Loss: 0.6039 | Val Corr: 0.3190 | Val RMSE: 0.8741\n",
      "   Epoch 10/100 | Train Loss: 0.6502 | Val Loss: 0.5702 | Val Corr: 0.3906 | Val RMSE: 0.8225\n",
      "   Epoch 20/100 | Train Loss: 0.5728 | Val Loss: 0.5424 | Val Corr: 0.4099 | Val RMSE: 0.7968\n",
      "   Epoch 30/100 | Train Loss: 0.4866 | Val Loss: 0.5713 | Val Corr: 0.4234 | Val RMSE: 0.8029\n",
      "Early stopping at epoch 32 (best epoch 17, best corr 0.4974)\n",
      "BEST epoch = 17 | BEST Corr = 0.4974 | RMSE@BEST = 0.7709\n",
      "----------------------------------------\n",
      "Seed 2 | Pool LIP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.7063 | Val Loss: 0.6845 | Val Corr: -0.0290 | Val RMSE: 0.9949\n",
      "   Epoch 10/100 | Train Loss: 0.6401 | Val Loss: 0.7524 | Val Corr: 0.2440 | Val RMSE: 1.0109\n",
      "   Epoch 20/100 | Train Loss: 0.5696 | Val Loss: 0.7722 | Val Corr: 0.3951 | Val RMSE: 1.0019\n",
      "   Epoch 30/100 | Train Loss: 0.5143 | Val Loss: 0.6352 | Val Corr: 0.4060 | Val RMSE: 0.9097\n",
      "   Epoch 40/100 | Train Loss: 0.4803 | Val Loss: 0.6863 | Val Corr: 0.3696 | Val RMSE: 0.9566\n",
      "Early stopping at epoch 44 (best epoch 29, best corr 0.4275)\n",
      "BEST epoch = 29 | BEST Corr = 0.4275 | RMSE@BEST = 0.9080\n",
      "----------------------------------------\n",
      "Seed 3 | Pool LIP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.6422 | Val Loss: 0.8928 | Val Corr: 0.1173 | Val RMSE: 1.2780\n",
      "   Epoch 10/100 | Train Loss: 0.5971 | Val Loss: 0.8186 | Val Corr: 0.1919 | Val RMSE: 1.2491\n",
      "   Epoch 20/100 | Train Loss: 0.5507 | Val Loss: 0.7980 | Val Corr: 0.2924 | Val RMSE: 1.2488\n",
      "   Epoch 30/100 | Train Loss: 0.4776 | Val Loss: 0.7644 | Val Corr: 0.3679 | Val RMSE: 1.1980\n",
      "   Epoch 40/100 | Train Loss: 0.4097 | Val Loss: 0.8570 | Val Corr: 0.3399 | Val RMSE: 1.3007\n",
      "   Epoch 50/100 | Train Loss: 0.4035 | Val Loss: 0.7953 | Val Corr: 0.3835 | Val RMSE: 1.2347\n",
      "   Epoch 60/100 | Train Loss: 0.3771 | Val Loss: 0.8159 | Val Corr: 0.4135 | Val RMSE: 1.2598\n",
      "   Epoch 70/100 | Train Loss: 0.3465 | Val Loss: 0.7858 | Val Corr: 0.4152 | Val RMSE: 1.2300\n",
      "   Epoch 80/100 | Train Loss: 0.3278 | Val Loss: 0.8164 | Val Corr: 0.4279 | Val RMSE: 1.2535\n",
      "   Epoch 90/100 | Train Loss: 0.3236 | Val Loss: 0.8405 | Val Corr: 0.4220 | Val RMSE: 1.2785\n",
      "Early stopping at epoch 93 (best epoch 78, best corr 0.4283)\n",
      "BEST epoch = 78 | BEST Corr = 0.4283 | RMSE@BEST = 1.2441\n",
      "----------------------------------------\n",
      "Seed 4 | Pool LIP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.7067 | Val Loss: 0.6050 | Val Corr: 0.4340 | Val RMSE: 0.9050\n",
      "   Epoch 10/100 | Train Loss: 0.6504 | Val Loss: 0.5926 | Val Corr: 0.4665 | Val RMSE: 0.8326\n",
      "   Epoch 20/100 | Train Loss: 0.5523 | Val Loss: 0.5598 | Val Corr: 0.5393 | Val RMSE: 0.7861\n",
      "   Epoch 30/100 | Train Loss: 0.5114 | Val Loss: 0.5663 | Val Corr: 0.5855 | Val RMSE: 0.7619\n",
      "   Epoch 40/100 | Train Loss: 0.4352 | Val Loss: 0.5530 | Val Corr: 0.5630 | Val RMSE: 0.7492\n",
      "Early stopping at epoch 48 (best epoch 33, best corr 0.6127)\n",
      "BEST epoch = 33 | BEST Corr = 0.6127 | RMSE@BEST = 0.7357\n",
      "----------------------------------------\n",
      "      Processing Trait: SNPP\n",
      "Seed 0 | Pool LIP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.6659 | Val Loss: 0.6354 | Val Corr: 0.2684 | Val RMSE: 0.8933\n",
      "   Epoch 10/100 | Train Loss: 0.6603 | Val Loss: 0.6309 | Val Corr: 0.2815 | Val RMSE: 0.8865\n",
      "   Epoch 20/100 | Train Loss: 0.6392 | Val Loss: 0.5954 | Val Corr: 0.3842 | Val RMSE: 0.8462\n",
      "   Epoch 30/100 | Train Loss: 0.5996 | Val Loss: 0.5945 | Val Corr: 0.4377 | Val RMSE: 0.8609\n",
      "   Epoch 40/100 | Train Loss: 0.5799 | Val Loss: 0.5663 | Val Corr: 0.4913 | Val RMSE: 0.7849\n",
      "Early stopping at epoch 49 (best epoch 34, best corr 0.5003)\n",
      "BEST epoch = 34 | BEST Corr = 0.5003 | RMSE@BEST = 0.7953\n",
      "----------------------------------------\n",
      "Seed 1 | Pool LIP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.6759 | Val Loss: 0.6262 | Val Corr: 0.0089 | Val RMSE: 0.9641\n",
      "   Epoch 10/100 | Train Loss: 0.6696 | Val Loss: 0.6062 | Val Corr: 0.1909 | Val RMSE: 0.9429\n",
      "   Epoch 20/100 | Train Loss: 0.6269 | Val Loss: 0.5822 | Val Corr: 0.3416 | Val RMSE: 0.9042\n",
      "   Epoch 30/100 | Train Loss: 0.5649 | Val Loss: 0.6132 | Val Corr: 0.4431 | Val RMSE: 0.8798\n",
      "   Epoch 40/100 | Train Loss: 0.5222 | Val Loss: 0.6486 | Val Corr: 0.4278 | Val RMSE: 0.9345\n",
      "   Epoch 50/100 | Train Loss: 0.4612 | Val Loss: 0.6407 | Val Corr: 0.4399 | Val RMSE: 0.9420\n",
      "Early stopping at epoch 53 (best epoch 38, best corr 0.4679)\n",
      "BEST epoch = 38 | BEST Corr = 0.4679 | RMSE@BEST = 0.8696\n",
      "----------------------------------------\n",
      "Seed 2 | Pool LIP | Train Samples: 330 | Test Samples: 83\n",
      "   Epoch 1/100 | Train Loss: 0.6565 | Val Loss: 0.7051 | Val Corr: 0.0411 | Val RMSE: 1.1159\n",
      "   Epoch 10/100 | Train Loss: 0.6398 | Val Loss: 0.6802 | Val Corr: 0.3799 | Val RMSE: 1.0781\n",
      "   Epoch 20/100 | Train Loss: 0.5937 | Val Loss: 0.6636 | Val Corr: 0.5174 | Val RMSE: 0.9944\n",
      "   Epoch 30/100 | Train Loss: 0.5378 | Val Loss: 0.6644 | Val Corr: 0.5403 | Val RMSE: 0.9724\n",
      "Early stopping at epoch 37 (best epoch 22, best corr 0.5700)\n",
      "BEST epoch = 22 | BEST Corr = 0.5700 | RMSE@BEST = 0.9969\n",
      "----------------------------------------\n",
      "Seed 3 | Pool LIP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.6510 | Val Loss: 0.7338 | Val Corr: 0.0596 | Val RMSE: 1.1017\n",
      "   Epoch 10/100 | Train Loss: 0.6384 | Val Loss: 0.7052 | Val Corr: 0.1347 | Val RMSE: 1.0825\n",
      "   Epoch 20/100 | Train Loss: 0.5359 | Val Loss: 0.7834 | Val Corr: 0.1697 | Val RMSE: 1.1330\n",
      "   Epoch 30/100 | Train Loss: 0.5667 | Val Loss: 0.7474 | Val Corr: 0.1741 | Val RMSE: 1.1164\n",
      "Early stopping at epoch 39 (best epoch 24, best corr 0.2212)\n",
      "BEST epoch = 24 | BEST Corr = 0.2212 | RMSE@BEST = 1.0578\n",
      "----------------------------------------\n",
      "Seed 4 | Pool LIP | Train Samples: 331 | Test Samples: 82\n",
      "   Epoch 1/100 | Train Loss: 0.6748 | Val Loss: 0.5507 | Val Corr: 0.1742 | Val RMSE: 0.9304\n",
      "   Epoch 10/100 | Train Loss: 0.6689 | Val Loss: 0.5230 | Val Corr: 0.2875 | Val RMSE: 0.9104\n",
      "   Epoch 20/100 | Train Loss: 0.6294 | Val Loss: 0.5298 | Val Corr: 0.3496 | Val RMSE: 0.9043\n",
      "   Epoch 30/100 | Train Loss: 0.5832 | Val Loss: 0.5279 | Val Corr: 0.3495 | Val RMSE: 0.8852\n",
      "Early stopping at epoch 38 (best epoch 23, best corr 0.3797)\n",
      "BEST epoch = 23 | BEST Corr = 0.3797 | RMSE@BEST = 0.8712\n",
      "----------------------------------------\n",
      "   [Saved] rice_T5000_LIP.json\n",
      "\n",
      "   >>> Processing Pooling Type: MAP | Species: sorghum | Feature Count: T5000\n",
      "      Processing Trait: HT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.7587 | Val Loss: 0.7654 | Val Corr: 0.3631 | Val RMSE: 0.9385\n",
      "   Epoch 10/100 | Train Loss: 0.5969 | Val Loss: 0.6807 | Val Corr: 0.5225 | Val RMSE: 0.8542\n",
      "   Epoch 20/100 | Train Loss: 0.4597 | Val Loss: 0.6476 | Val Corr: 0.4963 | Val RMSE: 0.8227\n",
      "Early stopping at epoch 21 (best epoch 6, best corr 0.5263)\n",
      "BEST epoch = 6 | BEST Corr = 0.5263 | RMSE@BEST = 0.8045\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7769 | Val Loss: 0.7801 | Val Corr: 0.4261 | Val RMSE: 1.0718\n",
      "   Epoch 10/100 | Train Loss: 0.5500 | Val Loss: 0.6806 | Val Corr: 0.4634 | Val RMSE: 0.9967\n",
      "   Epoch 20/100 | Train Loss: 0.4175 | Val Loss: 0.7134 | Val Corr: 0.5081 | Val RMSE: 1.0171\n",
      "   Epoch 30/100 | Train Loss: 0.3506 | Val Loss: 0.6545 | Val Corr: 0.5417 | Val RMSE: 0.9132\n",
      "   Epoch 40/100 | Train Loss: 0.3331 | Val Loss: 0.6730 | Val Corr: 0.5217 | Val RMSE: 0.9499\n",
      "   Epoch 50/100 | Train Loss: 0.3117 | Val Loss: 0.6752 | Val Corr: 0.5133 | Val RMSE: 0.9324\n",
      "Early stopping at epoch 57 (best epoch 42, best corr 0.5475)\n",
      "BEST epoch = 42 | BEST Corr = 0.5475 | RMSE@BEST = 0.9203\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7765 | Val Loss: 0.8996 | Val Corr: 0.2745 | Val RMSE: 1.0701\n",
      "   Epoch 10/100 | Train Loss: 0.5661 | Val Loss: 0.6401 | Val Corr: 0.5442 | Val RMSE: 0.8521\n",
      "   Epoch 20/100 | Train Loss: 0.4585 | Val Loss: 0.6355 | Val Corr: 0.5754 | Val RMSE: 0.8487\n",
      "   Epoch 30/100 | Train Loss: 0.3850 | Val Loss: 0.7276 | Val Corr: 0.5552 | Val RMSE: 0.8965\n",
      "Early stopping at epoch 36 (best epoch 21, best corr 0.5809)\n",
      "BEST epoch = 21 | BEST Corr = 0.5809 | RMSE@BEST = 0.8192\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7570 | Val Loss: 0.8076 | Val Corr: 0.3428 | Val RMSE: 1.0015\n",
      "   Epoch 10/100 | Train Loss: 0.5567 | Val Loss: 0.7209 | Val Corr: 0.5013 | Val RMSE: 0.8961\n",
      "   Epoch 20/100 | Train Loss: 0.4604 | Val Loss: 0.7436 | Val Corr: 0.5143 | Val RMSE: 0.9332\n",
      "   Epoch 30/100 | Train Loss: 0.3790 | Val Loss: 0.6876 | Val Corr: 0.5561 | Val RMSE: 0.8584\n",
      "   Epoch 40/100 | Train Loss: 0.3441 | Val Loss: 0.7372 | Val Corr: 0.5215 | Val RMSE: 0.8950\n",
      "Early stopping at epoch 43 (best epoch 28, best corr 0.5791)\n",
      "BEST epoch = 28 | BEST Corr = 0.5791 | RMSE@BEST = 0.8339\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7775 | Val Loss: 0.7716 | Val Corr: 0.4145 | Val RMSE: 0.9749\n",
      "   Epoch 10/100 | Train Loss: 0.5631 | Val Loss: 0.6351 | Val Corr: 0.5851 | Val RMSE: 0.7926\n",
      "   Epoch 20/100 | Train Loss: 0.4304 | Val Loss: 0.6700 | Val Corr: 0.5253 | Val RMSE: 0.8396\n",
      "   Epoch 30/100 | Train Loss: 0.3835 | Val Loss: 0.6813 | Val Corr: 0.5147 | Val RMSE: 0.8498\n",
      "Early stopping at epoch 32 (best epoch 17, best corr 0.5887)\n",
      "BEST epoch = 17 | BEST Corr = 0.5887 | RMSE@BEST = 0.8261\n",
      "----------------------------------------\n",
      "      Processing Trait: MO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.7679 | Val Loss: 0.8459 | Val Corr: 0.1492 | Val RMSE: 1.0589\n",
      "   Epoch 10/100 | Train Loss: 0.6178 | Val Loss: 0.8470 | Val Corr: 0.2188 | Val RMSE: 1.0608\n",
      "   Epoch 20/100 | Train Loss: 0.4776 | Val Loss: 0.8292 | Val Corr: 0.2276 | Val RMSE: 1.0467\n",
      "Early stopping at epoch 21 (best epoch 6, best corr 0.2783)\n",
      "BEST epoch = 6 | BEST Corr = 0.2783 | RMSE@BEST = 1.0994\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7981 | Val Loss: 0.6824 | Val Corr: 0.4106 | Val RMSE: 0.8446\n",
      "   Epoch 10/100 | Train Loss: 0.6019 | Val Loss: 0.6747 | Val Corr: 0.4387 | Val RMSE: 0.8171\n",
      "   Epoch 20/100 | Train Loss: 0.4813 | Val Loss: 0.7190 | Val Corr: 0.4084 | Val RMSE: 0.8767\n",
      "Early stopping at epoch 24 (best epoch 9, best corr 0.4507)\n",
      "BEST epoch = 9 | BEST Corr = 0.4507 | RMSE@BEST = 0.8118\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7820 | Val Loss: 0.7526 | Val Corr: 0.3653 | Val RMSE: 0.9482\n",
      "   Epoch 10/100 | Train Loss: 0.6203 | Val Loss: 0.6416 | Val Corr: 0.4350 | Val RMSE: 0.8211\n",
      "   Epoch 20/100 | Train Loss: 0.4714 | Val Loss: 0.6996 | Val Corr: 0.3912 | Val RMSE: 0.8824\n",
      "Early stopping at epoch 27 (best epoch 12, best corr 0.4912)\n",
      "BEST epoch = 12 | BEST Corr = 0.4912 | RMSE@BEST = 0.8099\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7762 | Val Loss: 0.7489 | Val Corr: 0.1002 | Val RMSE: 0.9524\n",
      "   Epoch 10/100 | Train Loss: 0.6046 | Val Loss: 0.7069 | Val Corr: 0.2373 | Val RMSE: 0.9723\n",
      "   Epoch 20/100 | Train Loss: 0.4973 | Val Loss: 0.6754 | Val Corr: 0.3706 | Val RMSE: 0.9072\n",
      "   Epoch 30/100 | Train Loss: 0.4496 | Val Loss: 0.6718 | Val Corr: 0.3678 | Val RMSE: 0.9063\n",
      "Early stopping at epoch 35 (best epoch 20, best corr 0.3706)\n",
      "BEST epoch = 20 | BEST Corr = 0.3706 | RMSE@BEST = 0.9072\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7630 | Val Loss: 0.9151 | Val Corr: 0.4022 | Val RMSE: 1.1881\n",
      "   Epoch 10/100 | Train Loss: 0.5505 | Val Loss: 0.8004 | Val Corr: 0.4283 | Val RMSE: 1.0763\n",
      "   Epoch 20/100 | Train Loss: 0.4446 | Val Loss: 0.7960 | Val Corr: 0.4602 | Val RMSE: 1.0615\n",
      "   Epoch 30/100 | Train Loss: 0.3553 | Val Loss: 0.7671 | Val Corr: 0.4986 | Val RMSE: 1.0384\n",
      "   Epoch 40/100 | Train Loss: 0.3616 | Val Loss: 0.7606 | Val Corr: 0.5058 | Val RMSE: 1.0274\n",
      "   Epoch 50/100 | Train Loss: 0.3262 | Val Loss: 0.7818 | Val Corr: 0.4992 | Val RMSE: 1.0423\n",
      "Early stopping at epoch 57 (best epoch 42, best corr 0.5324)\n",
      "BEST epoch = 42 | BEST Corr = 0.5324 | RMSE@BEST = 1.0166\n",
      "----------------------------------------\n",
      "      Processing Trait: YLD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.7934 | Val Loss: 0.8573 | Val Corr: 0.4363 | Val RMSE: 1.0268\n",
      "   Epoch 10/100 | Train Loss: 0.5635 | Val Loss: 0.9385 | Val Corr: 0.5499 | Val RMSE: 1.1682\n",
      "   Epoch 20/100 | Train Loss: 0.4721 | Val Loss: 0.7158 | Val Corr: 0.5822 | Val RMSE: 0.9173\n",
      "Early stopping at epoch 28 (best epoch 13, best corr 0.6254)\n",
      "BEST epoch = 13 | BEST Corr = 0.6254 | RMSE@BEST = 0.9097\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7721 | Val Loss: 0.8921 | Val Corr: 0.4747 | Val RMSE: 1.0365\n",
      "   Epoch 10/100 | Train Loss: 0.5233 | Val Loss: 0.6799 | Val Corr: 0.5892 | Val RMSE: 0.8495\n",
      "   Epoch 20/100 | Train Loss: 0.4021 | Val Loss: 0.7211 | Val Corr: 0.5630 | Val RMSE: 0.9192\n",
      "   Epoch 30/100 | Train Loss: 0.3338 | Val Loss: 0.6857 | Val Corr: 0.5799 | Val RMSE: 0.8574\n",
      "Early stopping at epoch 38 (best epoch 23, best corr 0.5935)\n",
      "BEST epoch = 23 | BEST Corr = 0.5935 | RMSE@BEST = 0.8472\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8495 | Val Loss: 0.7581 | Val Corr: 0.4337 | Val RMSE: 0.9666\n",
      "   Epoch 10/100 | Train Loss: 0.5651 | Val Loss: 0.6998 | Val Corr: 0.5769 | Val RMSE: 0.8453\n",
      "   Epoch 20/100 | Train Loss: 0.4386 | Val Loss: 0.6521 | Val Corr: 0.5654 | Val RMSE: 0.8086\n",
      "Early stopping at epoch 29 (best epoch 14, best corr 0.6192)\n",
      "BEST epoch = 14 | BEST Corr = 0.6192 | RMSE@BEST = 0.7649\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8071 | Val Loss: 0.7410 | Val Corr: 0.4363 | Val RMSE: 0.9463\n",
      "   Epoch 10/100 | Train Loss: 0.5440 | Val Loss: 0.9042 | Val Corr: 0.5427 | Val RMSE: 1.1251\n",
      "   Epoch 20/100 | Train Loss: 0.4142 | Val Loss: 0.9284 | Val Corr: 0.5479 | Val RMSE: 1.1556\n",
      "   Epoch 30/100 | Train Loss: 0.4025 | Val Loss: 0.9757 | Val Corr: 0.5765 | Val RMSE: 1.1843\n",
      "   Epoch 40/100 | Train Loss: 0.3327 | Val Loss: 0.8102 | Val Corr: 0.5930 | Val RMSE: 1.0372\n",
      "   Epoch 50/100 | Train Loss: 0.2832 | Val Loss: 0.7434 | Val Corr: 0.5900 | Val RMSE: 0.9648\n",
      "Early stopping at epoch 55 (best epoch 40, best corr 0.5930)\n",
      "BEST epoch = 40 | BEST Corr = 0.5930 | RMSE@BEST = 1.0372\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7900 | Val Loss: 0.8202 | Val Corr: 0.3864 | Val RMSE: 1.0061\n",
      "   Epoch 10/100 | Train Loss: 0.5134 | Val Loss: 0.6674 | Val Corr: 0.5479 | Val RMSE: 0.8537\n",
      "   Epoch 20/100 | Train Loss: 0.4036 | Val Loss: 0.7239 | Val Corr: 0.5828 | Val RMSE: 0.9720\n",
      "   Epoch 30/100 | Train Loss: 0.3351 | Val Loss: 0.7209 | Val Corr: 0.5572 | Val RMSE: 0.9450\n",
      "Early stopping at epoch 31 (best epoch 16, best corr 0.5929)\n",
      "BEST epoch = 16 | BEST Corr = 0.5929 | RMSE@BEST = 0.8615\n",
      "----------------------------------------\n",
      "   [Saved] sorghum_T5000_MAP.json\n",
      "\n",
      "   >>> Processing Pooling Type: AVG | Species: sorghum | Feature Count: T5000\n",
      "      Processing Trait: HT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool AVG | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.8255 | Val Loss: 0.7536 | Val Corr: 0.3232 | Val RMSE: 0.9444\n",
      "   Epoch 10/100 | Train Loss: 0.5995 | Val Loss: 0.6601 | Val Corr: 0.5403 | Val RMSE: 0.8219\n",
      "   Epoch 20/100 | Train Loss: 0.4434 | Val Loss: 0.6498 | Val Corr: 0.5253 | Val RMSE: 0.8221\n",
      "Early stopping at epoch 28 (best epoch 13, best corr 0.5427)\n",
      "BEST epoch = 13 | BEST Corr = 0.5427 | RMSE@BEST = 0.8328\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool AVG | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8338 | Val Loss: 0.7828 | Val Corr: 0.4885 | Val RMSE: 1.0753\n",
      "   Epoch 10/100 | Train Loss: 0.5415 | Val Loss: 0.6718 | Val Corr: 0.5677 | Val RMSE: 0.9322\n",
      "   Epoch 20/100 | Train Loss: 0.4150 | Val Loss: 0.6343 | Val Corr: 0.5598 | Val RMSE: 0.8941\n",
      "   Epoch 30/100 | Train Loss: 0.3622 | Val Loss: 0.6918 | Val Corr: 0.5780 | Val RMSE: 0.9778\n",
      "   Epoch 40/100 | Train Loss: 0.3332 | Val Loss: 0.7377 | Val Corr: 0.5853 | Val RMSE: 1.0125\n",
      "   Epoch 50/100 | Train Loss: 0.2833 | Val Loss: 0.6579 | Val Corr: 0.6130 | Val RMSE: 0.9236\n",
      "Early stopping at epoch 59 (best epoch 44, best corr 0.6138)\n",
      "BEST epoch = 44 | BEST Corr = 0.6138 | RMSE@BEST = 0.8852\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool AVG | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7983 | Val Loss: 0.8200 | Val Corr: 0.3152 | Val RMSE: 1.0024\n",
      "   Epoch 10/100 | Train Loss: 0.5993 | Val Loss: 0.7497 | Val Corr: 0.4300 | Val RMSE: 0.9440\n",
      "   Epoch 20/100 | Train Loss: 0.4277 | Val Loss: 0.8465 | Val Corr: 0.4392 | Val RMSE: 1.1355\n",
      "   Epoch 30/100 | Train Loss: 0.3966 | Val Loss: 0.7123 | Val Corr: 0.4675 | Val RMSE: 0.9089\n",
      "   Epoch 40/100 | Train Loss: 0.3176 | Val Loss: 0.6781 | Val Corr: 0.5126 | Val RMSE: 0.8834\n",
      "   Epoch 50/100 | Train Loss: 0.2810 | Val Loss: 0.6316 | Val Corr: 0.5418 | Val RMSE: 0.8597\n",
      "Early stopping at epoch 58 (best epoch 43, best corr 0.5462)\n",
      "BEST epoch = 43 | BEST Corr = 0.5462 | RMSE@BEST = 0.8773\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool AVG | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7692 | Val Loss: 0.7742 | Val Corr: 0.2028 | Val RMSE: 1.0228\n",
      "   Epoch 10/100 | Train Loss: 0.5707 | Val Loss: 0.7736 | Val Corr: 0.3849 | Val RMSE: 1.0238\n",
      "   Epoch 20/100 | Train Loss: 0.4227 | Val Loss: 0.7817 | Val Corr: 0.5109 | Val RMSE: 1.0059\n",
      "   Epoch 30/100 | Train Loss: 0.4397 | Val Loss: 0.8920 | Val Corr: 0.4919 | Val RMSE: 1.1346\n",
      "Early stopping at epoch 31 (best epoch 16, best corr 0.5168)\n",
      "BEST epoch = 16 | BEST Corr = 0.5168 | RMSE@BEST = 1.1292\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool AVG | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8139 | Val Loss: 0.7531 | Val Corr: 0.3599 | Val RMSE: 0.9713\n",
      "   Epoch 10/100 | Train Loss: 0.5707 | Val Loss: 0.7395 | Val Corr: 0.5217 | Val RMSE: 0.9027\n",
      "   Epoch 20/100 | Train Loss: 0.4064 | Val Loss: 0.7472 | Val Corr: 0.5650 | Val RMSE: 0.9235\n",
      "Early stopping at epoch 30 (best epoch 15, best corr 0.5813)\n",
      "BEST epoch = 15 | BEST Corr = 0.5813 | RMSE@BEST = 0.9663\n",
      "----------------------------------------\n",
      "      Processing Trait: MO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool AVG | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.7772 | Val Loss: 0.8413 | Val Corr: 0.1848 | Val RMSE: 1.0804\n",
      "   Epoch 10/100 | Train Loss: 0.6604 | Val Loss: 0.8247 | Val Corr: 0.1384 | Val RMSE: 1.0682\n",
      "Early stopping at epoch 18 (best epoch 3, best corr 0.2893)\n",
      "BEST epoch = 3 | BEST Corr = 0.2893 | RMSE@BEST = 1.0779\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool AVG | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8780 | Val Loss: 0.7108 | Val Corr: 0.2561 | Val RMSE: 0.8682\n",
      "   Epoch 10/100 | Train Loss: 0.6081 | Val Loss: 1.2369 | Val Corr: 0.3982 | Val RMSE: 1.3844\n",
      "   Epoch 20/100 | Train Loss: 0.4917 | Val Loss: 0.9666 | Val Corr: 0.3876 | Val RMSE: 1.1034\n",
      "   Epoch 30/100 | Train Loss: 0.4372 | Val Loss: 0.6875 | Val Corr: 0.3824 | Val RMSE: 0.8461\n",
      "Early stopping at epoch 31 (best epoch 16, best corr 0.4307)\n",
      "BEST epoch = 16 | BEST Corr = 0.4307 | RMSE@BEST = 0.9083\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool AVG | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8553 | Val Loss: 0.7273 | Val Corr: 0.3557 | Val RMSE: 0.9082\n",
      "   Epoch 10/100 | Train Loss: 0.6757 | Val Loss: 0.6453 | Val Corr: 0.4662 | Val RMSE: 0.8137\n",
      "   Epoch 20/100 | Train Loss: 0.4979 | Val Loss: 0.6991 | Val Corr: 0.4157 | Val RMSE: 0.8852\n",
      "Early stopping at epoch 23 (best epoch 8, best corr 0.4922)\n",
      "BEST epoch = 8 | BEST Corr = 0.4922 | RMSE@BEST = 1.3211\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool AVG | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8036 | Val Loss: 0.7685 | Val Corr: 0.2337 | Val RMSE: 0.9591\n",
      "   Epoch 10/100 | Train Loss: 0.6171 | Val Loss: 1.3782 | Val Corr: 0.2581 | Val RMSE: 1.5542\n",
      "   Epoch 20/100 | Train Loss: 0.5148 | Val Loss: 0.7851 | Val Corr: 0.2751 | Val RMSE: 1.0563\n",
      "   Epoch 30/100 | Train Loss: 0.4122 | Val Loss: 0.7204 | Val Corr: 0.3295 | Val RMSE: 0.9491\n",
      "   Epoch 40/100 | Train Loss: 0.3817 | Val Loss: 0.7624 | Val Corr: 0.3299 | Val RMSE: 0.9446\n",
      "   Epoch 50/100 | Train Loss: 0.3374 | Val Loss: 0.7631 | Val Corr: 0.3610 | Val RMSE: 0.9789\n",
      "   Epoch 60/100 | Train Loss: 0.2964 | Val Loss: 0.7312 | Val Corr: 0.3691 | Val RMSE: 0.9464\n",
      "   Epoch 70/100 | Train Loss: 0.2944 | Val Loss: 0.7785 | Val Corr: 0.3657 | Val RMSE: 0.9928\n",
      "   Epoch 80/100 | Train Loss: 0.3002 | Val Loss: 0.7703 | Val Corr: 0.3715 | Val RMSE: 0.9890\n",
      "Early stopping at epoch 82 (best epoch 67, best corr 0.3781)\n",
      "BEST epoch = 67 | BEST Corr = 0.3781 | RMSE@BEST = 0.9996\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool AVG | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7856 | Val Loss: 0.9100 | Val Corr: 0.1980 | Val RMSE: 1.1937\n",
      "   Epoch 10/100 | Train Loss: 0.6161 | Val Loss: 1.0221 | Val Corr: 0.3307 | Val RMSE: 1.3301\n",
      "   Epoch 20/100 | Train Loss: 0.4712 | Val Loss: 0.9738 | Val Corr: 0.3992 | Val RMSE: 1.2575\n",
      "   Epoch 30/100 | Train Loss: 0.4343 | Val Loss: 1.0621 | Val Corr: 0.3916 | Val RMSE: 1.3528\n",
      "   Epoch 40/100 | Train Loss: 0.3295 | Val Loss: 0.8307 | Val Corr: 0.4291 | Val RMSE: 1.0818\n",
      "Early stopping at epoch 43 (best epoch 28, best corr 0.4601)\n",
      "BEST epoch = 28 | BEST Corr = 0.4601 | RMSE@BEST = 1.0562\n",
      "----------------------------------------\n",
      "      Processing Trait: YLD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool AVG | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.8057 | Val Loss: 0.8567 | Val Corr: 0.3989 | Val RMSE: 1.0292\n",
      "   Epoch 10/100 | Train Loss: 0.6683 | Val Loss: 0.7628 | Val Corr: 0.5399 | Val RMSE: 0.9593\n",
      "   Epoch 20/100 | Train Loss: 0.3836 | Val Loss: 0.7249 | Val Corr: 0.5469 | Val RMSE: 0.9314\n",
      "Early stopping at epoch 30 (best epoch 15, best corr 0.5626)\n",
      "BEST epoch = 15 | BEST Corr = 0.5626 | RMSE@BEST = 1.1539\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool AVG | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8262 | Val Loss: 0.9094 | Val Corr: 0.4886 | Val RMSE: 1.0489\n",
      "   Epoch 10/100 | Train Loss: 0.4865 | Val Loss: 0.7968 | Val Corr: 0.5667 | Val RMSE: 0.9501\n",
      "   Epoch 20/100 | Train Loss: 0.4114 | Val Loss: 0.7162 | Val Corr: 0.5499 | Val RMSE: 0.8823\n",
      "Early stopping at epoch 26 (best epoch 11, best corr 0.5982)\n",
      "BEST epoch = 11 | BEST Corr = 0.5982 | RMSE@BEST = 0.9392\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool AVG | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8175 | Val Loss: 0.7669 | Val Corr: 0.4435 | Val RMSE: 0.9817\n",
      "   Epoch 10/100 | Train Loss: 0.5533 | Val Loss: 0.7897 | Val Corr: 0.5171 | Val RMSE: 1.0086\n",
      "   Epoch 20/100 | Train Loss: 0.5035 | Val Loss: 0.8358 | Val Corr: 0.5517 | Val RMSE: 1.0456\n",
      "Early stopping at epoch 28 (best epoch 13, best corr 0.5793)\n",
      "BEST epoch = 13 | BEST Corr = 0.5793 | RMSE@BEST = 1.3192\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool AVG | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8807 | Val Loss: 0.7442 | Val Corr: 0.4187 | Val RMSE: 0.9514\n",
      "   Epoch 10/100 | Train Loss: 0.5474 | Val Loss: 0.6964 | Val Corr: 0.4690 | Val RMSE: 0.8650\n",
      "   Epoch 20/100 | Train Loss: 0.4142 | Val Loss: 0.6755 | Val Corr: 0.5121 | Val RMSE: 0.8357\n",
      "Early stopping at epoch 30 (best epoch 15, best corr 0.5367)\n",
      "BEST epoch = 15 | BEST Corr = 0.5367 | RMSE@BEST = 1.0502\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool AVG | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8322 | Val Loss: 0.8226 | Val Corr: 0.3680 | Val RMSE: 1.0058\n",
      "   Epoch 10/100 | Train Loss: 0.5426 | Val Loss: 0.6890 | Val Corr: 0.5041 | Val RMSE: 0.8827\n",
      "   Epoch 20/100 | Train Loss: 0.3924 | Val Loss: 0.7075 | Val Corr: 0.4683 | Val RMSE: 0.9160\n",
      "Early stopping at epoch 30 (best epoch 15, best corr 0.5342)\n",
      "BEST epoch = 15 | BEST Corr = 0.5342 | RMSE@BEST = 0.8674\n",
      "----------------------------------------\n",
      "   [Saved] sorghum_T5000_AVG.json\n",
      "\n",
      "   >>> Processing Pooling Type: MAX | Species: sorghum | Feature Count: T5000\n",
      "      Processing Trait: HT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAX | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.8208 | Val Loss: 0.7699 | Val Corr: 0.0664 | Val RMSE: 0.9417\n",
      "   Epoch 10/100 | Train Loss: 0.6957 | Val Loss: 0.8282 | Val Corr: 0.3891 | Val RMSE: 0.9898\n",
      "   Epoch 20/100 | Train Loss: 0.5898 | Val Loss: 0.8636 | Val Corr: 0.5382 | Val RMSE: 1.0384\n",
      "   Epoch 30/100 | Train Loss: 0.5107 | Val Loss: 1.2455 | Val Corr: 0.5207 | Val RMSE: 1.4207\n",
      "Early stopping at epoch 36 (best epoch 21, best corr 0.5547)\n",
      "BEST epoch = 21 | BEST Corr = 0.5547 | RMSE@BEST = 1.0345\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAX | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7984 | Val Loss: 0.7956 | Val Corr: 0.3283 | Val RMSE: 1.1027\n",
      "   Epoch 10/100 | Train Loss: 0.6661 | Val Loss: 0.7839 | Val Corr: 0.4651 | Val RMSE: 1.1077\n",
      "   Epoch 20/100 | Train Loss: 0.5721 | Val Loss: 0.7984 | Val Corr: 0.5510 | Val RMSE: 1.1232\n",
      "   Epoch 30/100 | Train Loss: 0.4486 | Val Loss: 0.7694 | Val Corr: 0.5095 | Val RMSE: 1.0981\n",
      "Early stopping at epoch 37 (best epoch 22, best corr 0.5799)\n",
      "BEST epoch = 22 | BEST Corr = 0.5799 | RMSE@BEST = 1.1440\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAX | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8212 | Val Loss: 0.8077 | Val Corr: 0.3202 | Val RMSE: 1.0026\n",
      "   Epoch 10/100 | Train Loss: 0.6805 | Val Loss: 0.8506 | Val Corr: 0.4490 | Val RMSE: 1.0251\n",
      "   Epoch 20/100 | Train Loss: 0.6025 | Val Loss: 0.8804 | Val Corr: 0.4903 | Val RMSE: 1.0631\n",
      "   Epoch 30/100 | Train Loss: 0.5012 | Val Loss: 1.0514 | Val Corr: 0.5700 | Val RMSE: 1.2074\n",
      "   Epoch 40/100 | Train Loss: 0.4782 | Val Loss: 1.0317 | Val Corr: 0.5479 | Val RMSE: 1.1784\n",
      "Early stopping at epoch 45 (best epoch 30, best corr 0.5700)\n",
      "BEST epoch = 30 | BEST Corr = 0.5700 | RMSE@BEST = 1.2074\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAX | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7800 | Val Loss: 0.7800 | Val Corr: 0.2780 | Val RMSE: 1.0096\n",
      "   Epoch 10/100 | Train Loss: 0.6521 | Val Loss: 0.7524 | Val Corr: 0.4353 | Val RMSE: 0.9818\n",
      "   Epoch 20/100 | Train Loss: 0.6047 | Val Loss: 0.7498 | Val Corr: 0.4808 | Val RMSE: 0.9697\n",
      "   Epoch 30/100 | Train Loss: 0.5607 | Val Loss: 0.7349 | Val Corr: 0.4787 | Val RMSE: 0.9479\n",
      "   Epoch 40/100 | Train Loss: 0.4601 | Val Loss: 0.7314 | Val Corr: 0.4802 | Val RMSE: 0.8936\n",
      "Early stopping at epoch 41 (best epoch 26, best corr 0.5136)\n",
      "BEST epoch = 26 | BEST Corr = 0.5136 | RMSE@BEST = 0.9935\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAX | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8096 | Val Loss: 0.7710 | Val Corr: -0.0317 | Val RMSE: 0.9772\n",
      "   Epoch 10/100 | Train Loss: 0.7313 | Val Loss: 0.7892 | Val Corr: 0.4556 | Val RMSE: 0.9680\n",
      "   Epoch 20/100 | Train Loss: 0.5643 | Val Loss: 0.7111 | Val Corr: 0.4528 | Val RMSE: 0.9165\n",
      "   Epoch 30/100 | Train Loss: 0.5037 | Val Loss: 0.7323 | Val Corr: 0.5216 | Val RMSE: 0.8729\n",
      "   Epoch 40/100 | Train Loss: 0.4402 | Val Loss: 0.6979 | Val Corr: 0.5362 | Val RMSE: 0.8355\n",
      "   Epoch 50/100 | Train Loss: 0.3949 | Val Loss: 0.7023 | Val Corr: 0.5520 | Val RMSE: 0.9017\n",
      "   Epoch 60/100 | Train Loss: 0.3329 | Val Loss: 0.7422 | Val Corr: 0.5441 | Val RMSE: 0.9474\n",
      "   Epoch 70/100 | Train Loss: 0.3301 | Val Loss: 0.6842 | Val Corr: 0.5527 | Val RMSE: 0.8741\n",
      "Early stopping at epoch 73 (best epoch 58, best corr 0.5661)\n",
      "BEST epoch = 58 | BEST Corr = 0.5661 | RMSE@BEST = 0.9433\n",
      "----------------------------------------\n",
      "      Processing Trait: MO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAX | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.7862 | Val Loss: 0.8335 | Val Corr: -0.2184 | Val RMSE: 1.0645\n",
      "   Epoch 10/100 | Train Loss: 0.7215 | Val Loss: 0.8281 | Val Corr: 0.2019 | Val RMSE: 1.0464\n",
      "   Epoch 20/100 | Train Loss: 0.6340 | Val Loss: 0.8885 | Val Corr: 0.1387 | Val RMSE: 1.0960\n",
      "Early stopping at epoch 27 (best epoch 12, best corr 0.2590)\n",
      "BEST epoch = 12 | BEST Corr = 0.2590 | RMSE@BEST = 1.0441\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAX | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8754 | Val Loss: 0.8250 | Val Corr: 0.1141 | Val RMSE: 0.9676\n",
      "   Epoch 10/100 | Train Loss: 0.7324 | Val Loss: 1.0930 | Val Corr: 0.3194 | Val RMSE: 1.2458\n",
      "   Epoch 20/100 | Train Loss: 0.6272 | Val Loss: 1.4227 | Val Corr: 0.4003 | Val RMSE: 1.5800\n",
      "   Epoch 30/100 | Train Loss: 0.5331 | Val Loss: 1.7800 | Val Corr: 0.4564 | Val RMSE: 1.9237\n",
      "   Epoch 40/100 | Train Loss: 0.4501 | Val Loss: 1.8703 | Val Corr: 0.4156 | Val RMSE: 2.0121\n",
      "   Epoch 50/100 | Train Loss: 0.3805 | Val Loss: 1.7101 | Val Corr: 0.4258 | Val RMSE: 1.8530\n",
      "Early stopping at epoch 54 (best epoch 39, best corr 0.4816)\n",
      "BEST epoch = 39 | BEST Corr = 0.4816 | RMSE@BEST = 2.0844\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAX | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8251 | Val Loss: 0.7261 | Val Corr: 0.2500 | Val RMSE: 0.9080\n",
      "   Epoch 10/100 | Train Loss: 0.7482 | Val Loss: 0.9926 | Val Corr: 0.5007 | Val RMSE: 1.2279\n",
      "   Epoch 20/100 | Train Loss: 0.6828 | Val Loss: 1.0414 | Val Corr: 0.5097 | Val RMSE: 1.2696\n",
      "   Epoch 30/100 | Train Loss: 0.5682 | Val Loss: 1.1208 | Val Corr: 0.4812 | Val RMSE: 1.3421\n",
      "Early stopping at epoch 37 (best epoch 22, best corr 0.5130)\n",
      "BEST epoch = 22 | BEST Corr = 0.5130 | RMSE@BEST = 1.3229\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAX | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8090 | Val Loss: 0.7660 | Val Corr: 0.1778 | Val RMSE: 0.9568\n",
      "   Epoch 10/100 | Train Loss: 0.7055 | Val Loss: 0.9453 | Val Corr: 0.2168 | Val RMSE: 1.1435\n",
      "Early stopping at epoch 18 (best epoch 3, best corr 0.2344)\n",
      "BEST epoch = 3 | BEST Corr = 0.2344 | RMSE@BEST = 0.9968\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAX | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7715 | Val Loss: 0.9107 | Val Corr: 0.1406 | Val RMSE: 1.1881\n",
      "   Epoch 10/100 | Train Loss: 0.7435 | Val Loss: 1.6448 | Val Corr: 0.4098 | Val RMSE: 1.9105\n",
      "   Epoch 20/100 | Train Loss: 0.6147 | Val Loss: 2.2946 | Val Corr: 0.3905 | Val RMSE: 2.4886\n",
      "Early stopping at epoch 26 (best epoch 11, best corr 0.4444)\n",
      "BEST epoch = 11 | BEST Corr = 0.4444 | RMSE@BEST = 1.8820\n",
      "----------------------------------------\n",
      "      Processing Trait: YLD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAX | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.7924 | Val Loss: 0.8571 | Val Corr: 0.1615 | Val RMSE: 1.0336\n",
      "   Epoch 10/100 | Train Loss: 0.7319 | Val Loss: 0.7980 | Val Corr: 0.5178 | Val RMSE: 0.9652\n",
      "   Epoch 20/100 | Train Loss: 0.5437 | Val Loss: 0.7791 | Val Corr: 0.5354 | Val RMSE: 0.9607\n",
      "   Epoch 30/100 | Train Loss: 0.4448 | Val Loss: 0.7268 | Val Corr: 0.5721 | Val RMSE: 0.9161\n",
      "   Epoch 40/100 | Train Loss: 0.3916 | Val Loss: 0.7995 | Val Corr: 0.5946 | Val RMSE: 1.0070\n",
      "   Epoch 50/100 | Train Loss: 0.3435 | Val Loss: 0.6510 | Val Corr: 0.6043 | Val RMSE: 0.8420\n",
      "   Epoch 60/100 | Train Loss: 0.3170 | Val Loss: 0.6515 | Val Corr: 0.6048 | Val RMSE: 0.8269\n",
      "Early stopping at epoch 67 (best epoch 52, best corr 0.6173)\n",
      "BEST epoch = 52 | BEST Corr = 0.6173 | RMSE@BEST = 0.8488\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAX | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8588 | Val Loss: 0.9247 | Val Corr: 0.3349 | Val RMSE: 1.0804\n",
      "   Epoch 10/100 | Train Loss: 0.5995 | Val Loss: 0.9731 | Val Corr: 0.5375 | Val RMSE: 1.1796\n",
      "   Epoch 20/100 | Train Loss: 0.5484 | Val Loss: 1.0034 | Val Corr: 0.5357 | Val RMSE: 1.2230\n",
      "   Epoch 30/100 | Train Loss: 0.4039 | Val Loss: 0.8990 | Val Corr: 0.5491 | Val RMSE: 1.0978\n",
      "   Epoch 40/100 | Train Loss: 0.3920 | Val Loss: 0.8485 | Val Corr: 0.5740 | Val RMSE: 1.0342\n",
      "Early stopping at epoch 49 (best epoch 34, best corr 0.5834)\n",
      "BEST epoch = 34 | BEST Corr = 0.5834 | RMSE@BEST = 1.1081\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAX | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8334 | Val Loss: 0.7605 | Val Corr: 0.3714 | Val RMSE: 0.9665\n",
      "   Epoch 10/100 | Train Loss: 0.7394 | Val Loss: 0.7702 | Val Corr: 0.5735 | Val RMSE: 0.9855\n",
      "   Epoch 20/100 | Train Loss: 0.6436 | Val Loss: 0.6833 | Val Corr: 0.5538 | Val RMSE: 0.8763\n",
      "Early stopping at epoch 27 (best epoch 12, best corr 0.5983)\n",
      "BEST epoch = 12 | BEST Corr = 0.5983 | RMSE@BEST = 0.8994\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAX | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8682 | Val Loss: 0.7406 | Val Corr: 0.2238 | Val RMSE: 0.9438\n",
      "   Epoch 10/100 | Train Loss: 0.6880 | Val Loss: 0.6991 | Val Corr: 0.4465 | Val RMSE: 0.8957\n",
      "   Epoch 20/100 | Train Loss: 0.5889 | Val Loss: 0.8398 | Val Corr: 0.5047 | Val RMSE: 1.0665\n",
      "   Epoch 30/100 | Train Loss: 0.4943 | Val Loss: 0.9918 | Val Corr: 0.5518 | Val RMSE: 1.2229\n",
      "   Epoch 40/100 | Train Loss: 0.4526 | Val Loss: 1.0451 | Val Corr: 0.5308 | Val RMSE: 1.2750\n",
      "Early stopping at epoch 41 (best epoch 26, best corr 0.5544)\n",
      "BEST epoch = 26 | BEST Corr = 0.5544 | RMSE@BEST = 1.0512\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAX | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8352 | Val Loss: 0.8247 | Val Corr: 0.1870 | Val RMSE: 1.0198\n",
      "   Epoch 10/100 | Train Loss: 0.6933 | Val Loss: 0.8170 | Val Corr: 0.4835 | Val RMSE: 1.0435\n",
      "   Epoch 20/100 | Train Loss: 0.5054 | Val Loss: 0.7796 | Val Corr: 0.5285 | Val RMSE: 0.9918\n",
      "   Epoch 30/100 | Train Loss: 0.4724 | Val Loss: 0.8770 | Val Corr: 0.5484 | Val RMSE: 1.1470\n",
      "   Epoch 40/100 | Train Loss: 0.3924 | Val Loss: 0.7511 | Val Corr: 0.5910 | Val RMSE: 0.9914\n",
      "   Epoch 50/100 | Train Loss: 0.3619 | Val Loss: 0.6563 | Val Corr: 0.5766 | Val RMSE: 0.8751\n",
      "Early stopping at epoch 55 (best epoch 40, best corr 0.5910)\n",
      "BEST epoch = 40 | BEST Corr = 0.5910 | RMSE@BEST = 0.9914\n",
      "----------------------------------------\n",
      "   [Saved] sorghum_T5000_MAX.json\n",
      "\n",
      "   >>> Processing Pooling Type: LIP | Species: sorghum | Feature Count: T5000\n",
      "      Processing Trait: HT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool LIP | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.7945 | Val Loss: 0.7661 | Val Corr: 0.4040 | Val RMSE: 0.9399\n",
      "   Epoch 10/100 | Train Loss: 0.6452 | Val Loss: 0.8577 | Val Corr: 0.5411 | Val RMSE: 1.0457\n",
      "   Epoch 20/100 | Train Loss: 0.4847 | Val Loss: 0.6517 | Val Corr: 0.5832 | Val RMSE: 0.8233\n",
      "   Epoch 30/100 | Train Loss: 0.4152 | Val Loss: 0.7702 | Val Corr: 0.5627 | Val RMSE: 0.9508\n",
      "Early stopping at epoch 38 (best epoch 23, best corr 0.5956)\n",
      "BEST epoch = 23 | BEST Corr = 0.5956 | RMSE@BEST = 0.9106\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool LIP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8314 | Val Loss: 0.7822 | Val Corr: 0.2096 | Val RMSE: 1.0721\n",
      "   Epoch 10/100 | Train Loss: 0.5849 | Val Loss: 0.7514 | Val Corr: 0.4878 | Val RMSE: 1.0775\n",
      "   Epoch 20/100 | Train Loss: 0.4468 | Val Loss: 0.6249 | Val Corr: 0.5482 | Val RMSE: 0.8991\n",
      "   Epoch 30/100 | Train Loss: 0.3450 | Val Loss: 0.7233 | Val Corr: 0.5140 | Val RMSE: 0.9411\n",
      "   Epoch 40/100 | Train Loss: 0.3112 | Val Loss: 0.7960 | Val Corr: 0.5509 | Val RMSE: 1.0975\n",
      "   Epoch 50/100 | Train Loss: 0.2675 | Val Loss: 0.6756 | Val Corr: 0.5526 | Val RMSE: 0.8992\n",
      "Early stopping at epoch 53 (best epoch 38, best corr 0.5812)\n",
      "BEST epoch = 38 | BEST Corr = 0.5812 | RMSE@BEST = 1.0813\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool LIP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7783 | Val Loss: 0.8231 | Val Corr: 0.3014 | Val RMSE: 1.0041\n",
      "   Epoch 10/100 | Train Loss: 0.6351 | Val Loss: 0.7384 | Val Corr: 0.5156 | Val RMSE: 0.9245\n",
      "   Epoch 20/100 | Train Loss: 0.4994 | Val Loss: 0.6597 | Val Corr: 0.5622 | Val RMSE: 0.8530\n",
      "   Epoch 30/100 | Train Loss: 0.4447 | Val Loss: 0.6908 | Val Corr: 0.5754 | Val RMSE: 0.8459\n",
      "   Epoch 40/100 | Train Loss: 0.3780 | Val Loss: 0.7512 | Val Corr: 0.5605 | Val RMSE: 0.8968\n",
      "Early stopping at epoch 46 (best epoch 31, best corr 0.5806)\n",
      "BEST epoch = 31 | BEST Corr = 0.5806 | RMSE@BEST = 0.9271\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool LIP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8290 | Val Loss: 0.8109 | Val Corr: 0.2318 | Val RMSE: 1.0040\n",
      "   Epoch 10/100 | Train Loss: 0.6192 | Val Loss: 0.8864 | Val Corr: 0.4507 | Val RMSE: 1.0571\n",
      "   Epoch 20/100 | Train Loss: 0.4939 | Val Loss: 1.0378 | Val Corr: 0.5136 | Val RMSE: 1.2280\n",
      "   Epoch 30/100 | Train Loss: 0.3934 | Val Loss: 0.7557 | Val Corr: 0.4634 | Val RMSE: 0.9369\n",
      "   Epoch 40/100 | Train Loss: 0.3881 | Val Loss: 0.7112 | Val Corr: 0.5021 | Val RMSE: 0.8982\n",
      "   Epoch 50/100 | Train Loss: 0.2908 | Val Loss: 0.7112 | Val Corr: 0.5278 | Val RMSE: 0.8919\n",
      "Early stopping at epoch 59 (best epoch 44, best corr 0.5324)\n",
      "BEST epoch = 44 | BEST Corr = 0.5324 | RMSE@BEST = 0.8608\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool LIP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8534 | Val Loss: 0.7824 | Val Corr: 0.2191 | Val RMSE: 0.9819\n",
      "   Epoch 10/100 | Train Loss: 0.5861 | Val Loss: 0.8571 | Val Corr: 0.5170 | Val RMSE: 1.0209\n",
      "   Epoch 20/100 | Train Loss: 0.4513 | Val Loss: 0.8550 | Val Corr: 0.5361 | Val RMSE: 1.0192\n",
      "   Epoch 30/100 | Train Loss: 0.4079 | Val Loss: 0.6990 | Val Corr: 0.5607 | Val RMSE: 0.8475\n",
      "   Epoch 40/100 | Train Loss: 0.3347 | Val Loss: 0.7614 | Val Corr: 0.5870 | Val RMSE: 0.9232\n",
      "   Epoch 50/100 | Train Loss: 0.3005 | Val Loss: 0.7406 | Val Corr: 0.5786 | Val RMSE: 0.9093\n",
      "Early stopping at epoch 56 (best epoch 41, best corr 0.5921)\n",
      "BEST epoch = 41 | BEST Corr = 0.5921 | RMSE@BEST = 0.8208\n",
      "----------------------------------------\n",
      "      Processing Trait: MO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool LIP | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.8058 | Val Loss: 0.8388 | Val Corr: 0.2573 | Val RMSE: 1.0748\n",
      "   Epoch 10/100 | Train Loss: 0.7012 | Val Loss: 0.8813 | Val Corr: 0.2364 | Val RMSE: 1.0910\n",
      "Early stopping at epoch 17 (best epoch 2, best corr 0.3002)\n",
      "BEST epoch = 2 | BEST Corr = 0.3002 | RMSE@BEST = 1.1099\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool LIP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8567 | Val Loss: 0.6923 | Val Corr: 0.3214 | Val RMSE: 0.8515\n",
      "   Epoch 10/100 | Train Loss: 0.6981 | Val Loss: 0.7212 | Val Corr: 0.3990 | Val RMSE: 0.8327\n",
      "   Epoch 20/100 | Train Loss: 0.5318 | Val Loss: 0.6288 | Val Corr: 0.3982 | Val RMSE: 0.7815\n",
      "   Epoch 30/100 | Train Loss: 0.4286 | Val Loss: 0.6927 | Val Corr: 0.3878 | Val RMSE: 0.8191\n",
      "Early stopping at epoch 38 (best epoch 23, best corr 0.4484)\n",
      "BEST epoch = 23 | BEST Corr = 0.4484 | RMSE@BEST = 0.8897\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool LIP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8325 | Val Loss: 0.7237 | Val Corr: 0.3737 | Val RMSE: 0.9059\n",
      "   Epoch 10/100 | Train Loss: 0.6788 | Val Loss: 0.6643 | Val Corr: 0.4786 | Val RMSE: 0.8326\n",
      "   Epoch 20/100 | Train Loss: 0.5222 | Val Loss: 0.7737 | Val Corr: 0.3645 | Val RMSE: 0.9837\n",
      "Early stopping at epoch 24 (best epoch 9, best corr 0.4867)\n",
      "BEST epoch = 9 | BEST Corr = 0.4867 | RMSE@BEST = 0.9402\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool LIP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8215 | Val Loss: 0.7517 | Val Corr: 0.2038 | Val RMSE: 0.9502\n",
      "   Epoch 10/100 | Train Loss: 0.6823 | Val Loss: 1.0910 | Val Corr: 0.2375 | Val RMSE: 1.2632\n",
      "   Epoch 20/100 | Train Loss: 0.5578 | Val Loss: 0.6848 | Val Corr: 0.2993 | Val RMSE: 0.9350\n",
      "   Epoch 30/100 | Train Loss: 0.4564 | Val Loss: 0.6972 | Val Corr: 0.3047 | Val RMSE: 0.9507\n",
      "   Epoch 40/100 | Train Loss: 0.4244 | Val Loss: 0.7294 | Val Corr: 0.3458 | Val RMSE: 0.9848\n",
      "   Epoch 50/100 | Train Loss: 0.3717 | Val Loss: 0.7149 | Val Corr: 0.3479 | Val RMSE: 0.9530\n",
      "   Epoch 60/100 | Train Loss: 0.3018 | Val Loss: 0.8283 | Val Corr: 0.3495 | Val RMSE: 1.0855\n",
      "Early stopping at epoch 67 (best epoch 52, best corr 0.3670)\n",
      "BEST epoch = 52 | BEST Corr = 0.3670 | RMSE@BEST = 1.0951\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool LIP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7609 | Val Loss: 0.9132 | Val Corr: 0.2510 | Val RMSE: 1.1984\n",
      "   Epoch 10/100 | Train Loss: 0.6483 | Val Loss: 0.8369 | Val Corr: 0.3695 | Val RMSE: 1.1084\n",
      "   Epoch 20/100 | Train Loss: 0.5185 | Val Loss: 0.8203 | Val Corr: 0.4104 | Val RMSE: 1.1059\n",
      "   Epoch 30/100 | Train Loss: 0.4605 | Val Loss: 0.9984 | Val Corr: 0.3957 | Val RMSE: 1.2848\n",
      "   Epoch 40/100 | Train Loss: 0.3730 | Val Loss: 0.8255 | Val Corr: 0.3938 | Val RMSE: 1.0928\n",
      "Early stopping at epoch 42 (best epoch 27, best corr 0.4429)\n",
      "BEST epoch = 27 | BEST Corr = 0.4429 | RMSE@BEST = 1.1114\n",
      "----------------------------------------\n",
      "      Processing Trait: YLD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool LIP | Train Samples: 360 | Test Samples: 91\n",
      "   Epoch 1/100 | Train Loss: 0.7955 | Val Loss: 0.8568 | Val Corr: 0.4613 | Val RMSE: 1.0345\n",
      "   Epoch 10/100 | Train Loss: 0.5908 | Val Loss: 0.8019 | Val Corr: 0.5396 | Val RMSE: 0.9715\n",
      "   Epoch 20/100 | Train Loss: 0.4427 | Val Loss: 0.7042 | Val Corr: 0.5738 | Val RMSE: 0.8760\n",
      "   Epoch 30/100 | Train Loss: 0.4129 | Val Loss: 0.7021 | Val Corr: 0.6084 | Val RMSE: 0.8587\n",
      "   Epoch 40/100 | Train Loss: 0.3103 | Val Loss: 0.8044 | Val Corr: 0.6077 | Val RMSE: 0.9741\n",
      "Early stopping at epoch 41 (best epoch 26, best corr 0.6202)\n",
      "BEST epoch = 26 | BEST Corr = 0.6202 | RMSE@BEST = 0.8210\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool LIP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.7900 | Val Loss: 0.9014 | Val Corr: 0.4593 | Val RMSE: 1.0409\n",
      "   Epoch 10/100 | Train Loss: 0.5649 | Val Loss: 0.8519 | Val Corr: 0.5314 | Val RMSE: 1.0355\n",
      "   Epoch 20/100 | Train Loss: 0.4688 | Val Loss: 0.7551 | Val Corr: 0.5557 | Val RMSE: 0.8999\n",
      "Early stopping at epoch 26 (best epoch 11, best corr 0.5589)\n",
      "BEST epoch = 11 | BEST Corr = 0.5589 | RMSE@BEST = 0.9733\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool LIP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8433 | Val Loss: 0.7587 | Val Corr: 0.4583 | Val RMSE: 0.9667\n",
      "   Epoch 10/100 | Train Loss: 0.6518 | Val Loss: 0.6902 | Val Corr: 0.5687 | Val RMSE: 0.8770\n",
      "   Epoch 20/100 | Train Loss: 0.4572 | Val Loss: 0.9237 | Val Corr: 0.5455 | Val RMSE: 1.1373\n",
      "Early stopping at epoch 23 (best epoch 8, best corr 0.5859)\n",
      "BEST epoch = 8 | BEST Corr = 0.5859 | RMSE@BEST = 0.8213\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool LIP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8760 | Val Loss: 0.7436 | Val Corr: 0.3112 | Val RMSE: 0.9435\n",
      "   Epoch 10/100 | Train Loss: 0.5952 | Val Loss: 1.0828 | Val Corr: 0.4518 | Val RMSE: 1.3104\n",
      "   Epoch 20/100 | Train Loss: 0.4849 | Val Loss: 0.6558 | Val Corr: 0.5295 | Val RMSE: 0.8076\n",
      "   Epoch 30/100 | Train Loss: 0.3503 | Val Loss: 0.6234 | Val Corr: 0.5540 | Val RMSE: 0.7985\n",
      "   Epoch 40/100 | Train Loss: 0.3031 | Val Loss: 0.6453 | Val Corr: 0.5709 | Val RMSE: 0.8466\n",
      "   Epoch 50/100 | Train Loss: 0.2995 | Val Loss: 0.6452 | Val Corr: 0.5715 | Val RMSE: 0.8339\n",
      "   Epoch 60/100 | Train Loss: 0.2786 | Val Loss: 0.7086 | Val Corr: 0.5749 | Val RMSE: 0.9254\n",
      "Early stopping at epoch 63 (best epoch 48, best corr 0.5800)\n",
      "BEST epoch = 48 | BEST Corr = 0.5800 | RMSE@BEST = 0.8384\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_41973/468903869.py:7: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool LIP | Train Samples: 361 | Test Samples: 90\n",
      "   Epoch 1/100 | Train Loss: 0.8269 | Val Loss: 0.8277 | Val Corr: 0.2079 | Val RMSE: 1.0092\n",
      "   Epoch 10/100 | Train Loss: 0.6029 | Val Loss: 0.7228 | Val Corr: 0.4775 | Val RMSE: 0.9248\n",
      "   Epoch 20/100 | Train Loss: 0.4304 | Val Loss: 0.7797 | Val Corr: 0.5026 | Val RMSE: 0.9458\n",
      "   Epoch 30/100 | Train Loss: 0.3515 | Val Loss: 0.6697 | Val Corr: 0.5359 | Val RMSE: 0.8593\n",
      "Early stopping at epoch 39 (best epoch 24, best corr 0.5546)\n",
      "BEST epoch = 24 | BEST Corr = 0.5546 | RMSE@BEST = 0.8555\n",
      "----------------------------------------\n",
      "   [Saved] sorghum_T5000_LIP.json\n",
      "\n",
      "   >>> Processing Pooling Type: MAP | Species: bulls | Feature Count: T5000\n",
      "      Processing Trait: MS\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7988 | Val Loss: 0.8170 | Val Corr: 0.1388 | Val RMSE: 1.0633\n",
      "   Epoch 10/100 | Train Loss: 0.6276 | Val Loss: 0.7642 | Val Corr: 0.3791 | Val RMSE: 0.9675\n",
      "   Epoch 20/100 | Train Loss: 0.4996 | Val Loss: 0.8218 | Val Corr: 0.4078 | Val RMSE: 1.0460\n",
      "   Epoch 30/100 | Train Loss: 0.4099 | Val Loss: 0.7550 | Val Corr: 0.4138 | Val RMSE: 0.9732\n",
      "   Epoch 40/100 | Train Loss: 0.3679 | Val Loss: 0.7727 | Val Corr: 0.4169 | Val RMSE: 0.9875\n",
      "   Epoch 50/100 | Train Loss: 0.3215 | Val Loss: 0.7838 | Val Corr: 0.4132 | Val RMSE: 1.0129\n",
      "Early stopping at epoch 56 (best epoch 41, best corr 0.4341)\n",
      "BEST epoch = 41 | BEST Corr = 0.4341 | RMSE@BEST = 1.0284\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.8173 | Val Loss: 0.7659 | Val Corr: 0.1866 | Val RMSE: 0.9849\n",
      "   Epoch 10/100 | Train Loss: 0.6340 | Val Loss: 0.7468 | Val Corr: 0.4201 | Val RMSE: 0.9261\n",
      "   Epoch 20/100 | Train Loss: 0.5143 | Val Loss: 0.8033 | Val Corr: 0.4442 | Val RMSE: 0.9835\n",
      "   Epoch 30/100 | Train Loss: 0.4266 | Val Loss: 0.6702 | Val Corr: 0.4480 | Val RMSE: 0.8948\n",
      "Early stopping at epoch 39 (best epoch 24, best corr 0.4784)\n",
      "BEST epoch = 24 | BEST Corr = 0.4784 | RMSE@BEST = 0.8730\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7962 | Val Loss: 0.7623 | Val Corr: 0.0947 | Val RMSE: 0.9590\n",
      "   Epoch 10/100 | Train Loss: 0.6401 | Val Loss: 0.7370 | Val Corr: 0.3501 | Val RMSE: 0.9124\n",
      "   Epoch 20/100 | Train Loss: 0.5078 | Val Loss: 0.7624 | Val Corr: 0.3867 | Val RMSE: 0.9333\n",
      "   Epoch 30/100 | Train Loss: 0.4055 | Val Loss: 0.7307 | Val Corr: 0.4033 | Val RMSE: 0.9186\n",
      "Early stopping at epoch 40 (best epoch 25, best corr 0.4293)\n",
      "BEST epoch = 25 | BEST Corr = 0.4293 | RMSE@BEST = 0.9444\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.8075 | Val Loss: 0.7603 | Val Corr: 0.1333 | Val RMSE: 0.9885\n",
      "   Epoch 10/100 | Train Loss: 0.6542 | Val Loss: 0.7014 | Val Corr: 0.3665 | Val RMSE: 0.9261\n",
      "   Epoch 20/100 | Train Loss: 0.5046 | Val Loss: 0.7315 | Val Corr: 0.3955 | Val RMSE: 0.9419\n",
      "   Epoch 30/100 | Train Loss: 0.4240 | Val Loss: 0.6934 | Val Corr: 0.4054 | Val RMSE: 0.9305\n",
      "   Epoch 40/100 | Train Loss: 0.3647 | Val Loss: 0.7015 | Val Corr: 0.3889 | Val RMSE: 0.9467\n",
      "Early stopping at epoch 43 (best epoch 28, best corr 0.4194)\n",
      "BEST epoch = 28 | BEST Corr = 0.4194 | RMSE@BEST = 0.9136\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7995 | Val Loss: 0.7844 | Val Corr: 0.1388 | Val RMSE: 0.9885\n",
      "   Epoch 10/100 | Train Loss: 0.5774 | Val Loss: 0.7601 | Val Corr: 0.3184 | Val RMSE: 0.9507\n",
      "   Epoch 20/100 | Train Loss: 0.4858 | Val Loss: 0.7928 | Val Corr: 0.3478 | Val RMSE: 0.9809\n",
      "   Epoch 30/100 | Train Loss: 0.3926 | Val Loss: 0.7717 | Val Corr: 0.3625 | Val RMSE: 0.9596\n",
      "   Epoch 40/100 | Train Loss: 0.3669 | Val Loss: 0.7630 | Val Corr: 0.3595 | Val RMSE: 0.9552\n",
      "   Epoch 50/100 | Train Loss: 0.3083 | Val Loss: 0.7732 | Val Corr: 0.3444 | Val RMSE: 0.9723\n",
      "Early stopping at epoch 54 (best epoch 39, best corr 0.3691)\n",
      "BEST epoch = 39 | BEST Corr = 0.3691 | RMSE@BEST = 0.9555\n",
      "----------------------------------------\n",
      "      Processing Trait: NMSP\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.8149 | Val Loss: 0.8800 | Val Corr: 0.2725 | Val RMSE: 1.0099\n",
      "   Epoch 10/100 | Train Loss: 0.6382 | Val Loss: 0.6305 | Val Corr: 0.5240 | Val RMSE: 0.8248\n",
      "   Epoch 20/100 | Train Loss: 0.5052 | Val Loss: 0.5795 | Val Corr: 0.5978 | Val RMSE: 0.7678\n",
      "   Epoch 30/100 | Train Loss: 0.4199 | Val Loss: 0.5834 | Val Corr: 0.5912 | Val RMSE: 0.7532\n",
      "   Epoch 40/100 | Train Loss: 0.3679 | Val Loss: 0.5901 | Val Corr: 0.5959 | Val RMSE: 0.7461\n",
      "   Epoch 50/100 | Train Loss: 0.3402 | Val Loss: 0.5806 | Val Corr: 0.6080 | Val RMSE: 0.7669\n",
      "Early stopping at epoch 53 (best epoch 38, best corr 0.6139)\n",
      "BEST epoch = 38 | BEST Corr = 0.6139 | RMSE@BEST = 0.7369\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7865 | Val Loss: 0.8286 | Val Corr: 0.1089 | Val RMSE: 1.0514\n",
      "   Epoch 10/100 | Train Loss: 0.6024 | Val Loss: 0.7090 | Val Corr: 0.4472 | Val RMSE: 0.9446\n",
      "   Epoch 20/100 | Train Loss: 0.4826 | Val Loss: 0.7287 | Val Corr: 0.4843 | Val RMSE: 1.0152\n",
      "   Epoch 30/100 | Train Loss: 0.4061 | Val Loss: 0.6958 | Val Corr: 0.4956 | Val RMSE: 0.9648\n",
      "Early stopping at epoch 32 (best epoch 17, best corr 0.5366)\n",
      "BEST epoch = 17 | BEST Corr = 0.5366 | RMSE@BEST = 0.9210\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7574 | Val Loss: 0.7551 | Val Corr: 0.2704 | Val RMSE: 1.0060\n",
      "   Epoch 10/100 | Train Loss: 0.5744 | Val Loss: 0.6699 | Val Corr: 0.4647 | Val RMSE: 0.9092\n",
      "   Epoch 20/100 | Train Loss: 0.4445 | Val Loss: 0.6366 | Val Corr: 0.5465 | Val RMSE: 0.8578\n",
      "   Epoch 30/100 | Train Loss: 0.3716 | Val Loss: 0.6304 | Val Corr: 0.5454 | Val RMSE: 0.8580\n",
      "   Epoch 40/100 | Train Loss: 0.3475 | Val Loss: 0.6285 | Val Corr: 0.5516 | Val RMSE: 0.8343\n",
      "   Epoch 50/100 | Train Loss: 0.3013 | Val Loss: 0.6070 | Val Corr: 0.5635 | Val RMSE: 0.8262\n",
      "Early stopping at epoch 57 (best epoch 42, best corr 0.5830)\n",
      "BEST epoch = 42 | BEST Corr = 0.5830 | RMSE@BEST = 0.8044\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7816 | Val Loss: 0.7595 | Val Corr: 0.1170 | Val RMSE: 1.0009\n",
      "   Epoch 10/100 | Train Loss: 0.6237 | Val Loss: 0.6824 | Val Corr: 0.3882 | Val RMSE: 0.9286\n",
      "   Epoch 20/100 | Train Loss: 0.4918 | Val Loss: 0.6596 | Val Corr: 0.4798 | Val RMSE: 0.8895\n",
      "   Epoch 30/100 | Train Loss: 0.4113 | Val Loss: 0.6725 | Val Corr: 0.4497 | Val RMSE: 0.9198\n",
      "Early stopping at epoch 34 (best epoch 19, best corr 0.4904)\n",
      "BEST epoch = 19 | BEST Corr = 0.4904 | RMSE@BEST = 0.8894\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.8148 | Val Loss: 0.8195 | Val Corr: 0.1374 | Val RMSE: 1.0261\n",
      "   Epoch 10/100 | Train Loss: 0.5831 | Val Loss: 0.8270 | Val Corr: 0.2709 | Val RMSE: 1.0301\n",
      "   Epoch 20/100 | Train Loss: 0.4380 | Val Loss: 0.8063 | Val Corr: 0.2805 | Val RMSE: 1.0388\n",
      "   Epoch 30/100 | Train Loss: 0.3941 | Val Loss: 0.7777 | Val Corr: 0.2842 | Val RMSE: 1.0142\n",
      "Early stopping at epoch 40 (best epoch 25, best corr 0.3138)\n",
      "BEST epoch = 25 | BEST Corr = 0.3138 | RMSE@BEST = 1.0042\n",
      "----------------------------------------\n",
      "      Processing Trait: VE\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.8090 | Val Loss: 0.7525 | Val Corr: 0.1783 | Val RMSE: 0.9788\n",
      "   Epoch 10/100 | Train Loss: 0.6305 | Val Loss: 0.6843 | Val Corr: 0.3616 | Val RMSE: 0.9069\n",
      "   Epoch 20/100 | Train Loss: 0.5026 | Val Loss: 0.6863 | Val Corr: 0.3851 | Val RMSE: 0.9141\n",
      "   Epoch 30/100 | Train Loss: 0.4454 | Val Loss: 0.7044 | Val Corr: 0.3873 | Val RMSE: 0.9174\n",
      "   Epoch 40/100 | Train Loss: 0.3924 | Val Loss: 0.6876 | Val Corr: 0.3998 | Val RMSE: 0.9113\n",
      "   Epoch 50/100 | Train Loss: 0.3483 | Val Loss: 0.6721 | Val Corr: 0.3933 | Val RMSE: 0.9085\n",
      "Early stopping at epoch 52 (best epoch 37, best corr 0.4146)\n",
      "BEST epoch = 37 | BEST Corr = 0.4146 | RMSE@BEST = 0.9454\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.8031 | Val Loss: 0.7813 | Val Corr: 0.2683 | Val RMSE: 1.0404\n",
      "   Epoch 10/100 | Train Loss: 0.6096 | Val Loss: 0.7188 | Val Corr: 0.4522 | Val RMSE: 0.9965\n",
      "   Epoch 20/100 | Train Loss: 0.4864 | Val Loss: 0.7095 | Val Corr: 0.4337 | Val RMSE: 0.9748\n",
      "   Epoch 30/100 | Train Loss: 0.4186 | Val Loss: 0.6782 | Val Corr: 0.4869 | Val RMSE: 0.9452\n",
      "Early stopping at epoch 32 (best epoch 17, best corr 0.4972)\n",
      "BEST epoch = 17 | BEST Corr = 0.4972 | RMSE@BEST = 0.9578\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7965 | Val Loss: 0.8701 | Val Corr: 0.2835 | Val RMSE: 1.0597\n",
      "   Epoch 10/100 | Train Loss: 0.6140 | Val Loss: 0.7214 | Val Corr: 0.4152 | Val RMSE: 0.8850\n",
      "   Epoch 20/100 | Train Loss: 0.4881 | Val Loss: 0.7138 | Val Corr: 0.4167 | Val RMSE: 0.8914\n",
      "   Epoch 30/100 | Train Loss: 0.4318 | Val Loss: 0.7320 | Val Corr: 0.4031 | Val RMSE: 0.9245\n",
      "Early stopping at epoch 38 (best epoch 23, best corr 0.4493)\n",
      "BEST epoch = 23 | BEST Corr = 0.4493 | RMSE@BEST = 0.8942\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7974 | Val Loss: 0.8117 | Val Corr: 0.2713 | Val RMSE: 1.0317\n",
      "   Epoch 10/100 | Train Loss: 0.6311 | Val Loss: 0.7847 | Val Corr: 0.4348 | Val RMSE: 1.0273\n",
      "   Epoch 20/100 | Train Loss: 0.5203 | Val Loss: 0.8360 | Val Corr: 0.4164 | Val RMSE: 1.0860\n",
      "   Epoch 30/100 | Train Loss: 0.4528 | Val Loss: 0.7664 | Val Corr: 0.4307 | Val RMSE: 0.9916\n",
      "Early stopping at epoch 39 (best epoch 24, best corr 0.4627)\n",
      "BEST epoch = 24 | BEST Corr = 0.4627 | RMSE@BEST = 0.9961\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.8304 | Val Loss: 0.7287 | Val Corr: 0.1670 | Val RMSE: 0.9545\n",
      "   Epoch 10/100 | Train Loss: 0.6274 | Val Loss: 0.7060 | Val Corr: 0.3156 | Val RMSE: 0.9137\n",
      "   Epoch 20/100 | Train Loss: 0.4797 | Val Loss: 0.7021 | Val Corr: 0.3706 | Val RMSE: 0.9030\n",
      "   Epoch 30/100 | Train Loss: 0.4306 | Val Loss: 0.7186 | Val Corr: 0.3469 | Val RMSE: 0.9224\n",
      "   Epoch 40/100 | Train Loss: 0.3947 | Val Loss: 0.7006 | Val Corr: 0.3411 | Val RMSE: 0.9316\n",
      "Early stopping at epoch 41 (best epoch 26, best corr 0.3764)\n",
      "BEST epoch = 26 | BEST Corr = 0.3764 | RMSE@BEST = 0.8956\n",
      "----------------------------------------\n",
      "   [Saved] bulls_T5000_MAP.json\n",
      "\n",
      "   >>> Processing Pooling Type: AVG | Species: bulls | Feature Count: T5000\n",
      "      Processing Trait: MS\n",
      "Seed 0 | Pool AVG | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7842 | Val Loss: 0.8000 | Val Corr: 0.0744 | Val RMSE: 1.0454\n",
      "   Epoch 10/100 | Train Loss: 0.6219 | Val Loss: 0.9073 | Val Corr: 0.3569 | Val RMSE: 1.1755\n",
      "   Epoch 20/100 | Train Loss: 0.5064 | Val Loss: 0.7772 | Val Corr: 0.4060 | Val RMSE: 0.9735\n",
      "   Epoch 30/100 | Train Loss: 0.4180 | Val Loss: 0.7758 | Val Corr: 0.4121 | Val RMSE: 0.9951\n",
      "   Epoch 40/100 | Train Loss: 0.3656 | Val Loss: 0.7785 | Val Corr: 0.4325 | Val RMSE: 0.9943\n",
      "   Epoch 50/100 | Train Loss: 0.3276 | Val Loss: 0.7710 | Val Corr: 0.4231 | Val RMSE: 0.9825\n",
      "   Epoch 60/100 | Train Loss: 0.2999 | Val Loss: 0.8569 | Val Corr: 0.4159 | Val RMSE: 1.1081\n",
      "Early stopping at epoch 61 (best epoch 46, best corr 0.4407)\n",
      "BEST epoch = 46 | BEST Corr = 0.4407 | RMSE@BEST = 0.9866\n",
      "----------------------------------------\n",
      "Seed 1 | Pool AVG | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7928 | Val Loss: 0.7641 | Val Corr: 0.1585 | Val RMSE: 0.9888\n",
      "   Epoch 10/100 | Train Loss: 0.6212 | Val Loss: 0.7523 | Val Corr: 0.3843 | Val RMSE: 0.9447\n",
      "   Epoch 20/100 | Train Loss: 0.4823 | Val Loss: 0.7527 | Val Corr: 0.4066 | Val RMSE: 0.9470\n",
      "   Epoch 30/100 | Train Loss: 0.3939 | Val Loss: 0.8719 | Val Corr: 0.4123 | Val RMSE: 1.0614\n",
      "Early stopping at epoch 33 (best epoch 18, best corr 0.4270)\n",
      "BEST epoch = 18 | BEST Corr = 0.4270 | RMSE@BEST = 1.0612\n",
      "----------------------------------------\n",
      "Seed 2 | Pool AVG | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7849 | Val Loss: 0.7938 | Val Corr: 0.1625 | Val RMSE: 0.9880\n",
      "   Epoch 10/100 | Train Loss: 0.6045 | Val Loss: 0.8174 | Val Corr: 0.3656 | Val RMSE: 1.0536\n",
      "   Epoch 20/100 | Train Loss: 0.4870 | Val Loss: 0.7347 | Val Corr: 0.3980 | Val RMSE: 0.9217\n",
      "   Epoch 30/100 | Train Loss: 0.4326 | Val Loss: 0.7691 | Val Corr: 0.4165 | Val RMSE: 1.0164\n",
      "   Epoch 40/100 | Train Loss: 0.3778 | Val Loss: 0.7559 | Val Corr: 0.4272 | Val RMSE: 0.9811\n",
      "   Epoch 50/100 | Train Loss: 0.3496 | Val Loss: 0.7500 | Val Corr: 0.4341 | Val RMSE: 0.9892\n",
      "Early stopping at epoch 53 (best epoch 38, best corr 0.4463)\n",
      "BEST epoch = 38 | BEST Corr = 0.4463 | RMSE@BEST = 1.0129\n",
      "----------------------------------------\n",
      "Seed 3 | Pool AVG | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7835 | Val Loss: 0.7702 | Val Corr: 0.1211 | Val RMSE: 1.0065\n",
      "   Epoch 10/100 | Train Loss: 0.6308 | Val Loss: 0.7195 | Val Corr: 0.3434 | Val RMSE: 0.9347\n",
      "   Epoch 20/100 | Train Loss: 0.4953 | Val Loss: 0.7617 | Val Corr: 0.4110 | Val RMSE: 1.0310\n",
      "   Epoch 30/100 | Train Loss: 0.4062 | Val Loss: 0.8157 | Val Corr: 0.3857 | Val RMSE: 1.1067\n",
      "Early stopping at epoch 35 (best epoch 20, best corr 0.4110)\n",
      "BEST epoch = 20 | BEST Corr = 0.4110 | RMSE@BEST = 1.0310\n",
      "----------------------------------------\n",
      "Seed 4 | Pool AVG | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7758 | Val Loss: 0.7903 | Val Corr: 0.0740 | Val RMSE: 0.9977\n",
      "   Epoch 10/100 | Train Loss: 0.6061 | Val Loss: 0.7736 | Val Corr: 0.2841 | Val RMSE: 0.9717\n",
      "   Epoch 20/100 | Train Loss: 0.4564 | Val Loss: 0.7991 | Val Corr: 0.2725 | Val RMSE: 1.0031\n",
      "Early stopping at epoch 23 (best epoch 8, best corr 0.2917)\n",
      "BEST epoch = 8 | BEST Corr = 0.2917 | RMSE@BEST = 1.0199\n",
      "----------------------------------------\n",
      "      Processing Trait: NMSP\n",
      "Seed 0 | Pool AVG | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7614 | Val Loss: 0.7508 | Val Corr: 0.2184 | Val RMSE: 0.9287\n",
      "   Epoch 10/100 | Train Loss: 0.6020 | Val Loss: 0.9977 | Val Corr: 0.4453 | Val RMSE: 1.1591\n",
      "   Epoch 20/100 | Train Loss: 0.4616 | Val Loss: 0.8009 | Val Corr: 0.5398 | Val RMSE: 0.9559\n",
      "   Epoch 30/100 | Train Loss: 0.4040 | Val Loss: 0.6675 | Val Corr: 0.5376 | Val RMSE: 0.8223\n",
      "Early stopping at epoch 38 (best epoch 23, best corr 0.5609)\n",
      "BEST epoch = 23 | BEST Corr = 0.5609 | RMSE@BEST = 0.8659\n",
      "----------------------------------------\n",
      "Seed 1 | Pool AVG | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7370 | Val Loss: 0.7885 | Val Corr: 0.1404 | Val RMSE: 1.0499\n",
      "   Epoch 10/100 | Train Loss: 0.5754 | Val Loss: 1.2409 | Val Corr: 0.3733 | Val RMSE: 1.4397\n",
      "   Epoch 20/100 | Train Loss: 0.4521 | Val Loss: 0.8991 | Val Corr: 0.4354 | Val RMSE: 1.1181\n",
      "   Epoch 30/100 | Train Loss: 0.3662 | Val Loss: 0.7231 | Val Corr: 0.4671 | Val RMSE: 0.9485\n",
      "   Epoch 40/100 | Train Loss: 0.3387 | Val Loss: 0.7326 | Val Corr: 0.4672 | Val RMSE: 0.9595\n",
      "Early stopping at epoch 46 (best epoch 31, best corr 0.4766)\n",
      "BEST epoch = 31 | BEST Corr = 0.4766 | RMSE@BEST = 0.9658\n",
      "----------------------------------------\n",
      "Seed 2 | Pool AVG | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7535 | Val Loss: 0.7627 | Val Corr: 0.2437 | Val RMSE: 0.9942\n",
      "   Epoch 10/100 | Train Loss: 0.5775 | Val Loss: 0.7224 | Val Corr: 0.4417 | Val RMSE: 0.9012\n",
      "   Epoch 20/100 | Train Loss: 0.4460 | Val Loss: 0.7066 | Val Corr: 0.4864 | Val RMSE: 0.9014\n",
      "   Epoch 30/100 | Train Loss: 0.3716 | Val Loss: 0.6841 | Val Corr: 0.4953 | Val RMSE: 0.8805\n",
      "   Epoch 40/100 | Train Loss: 0.3477 | Val Loss: 0.7763 | Val Corr: 0.5167 | Val RMSE: 0.9664\n",
      "   Epoch 50/100 | Train Loss: 0.2979 | Val Loss: 0.7062 | Val Corr: 0.5197 | Val RMSE: 0.9058\n",
      "   Epoch 60/100 | Train Loss: 0.2861 | Val Loss: 0.7134 | Val Corr: 0.5108 | Val RMSE: 0.9140\n",
      "Early stopping at epoch 63 (best epoch 48, best corr 0.5309)\n",
      "BEST epoch = 48 | BEST Corr = 0.5309 | RMSE@BEST = 0.8641\n",
      "----------------------------------------\n",
      "Seed 3 | Pool AVG | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7494 | Val Loss: 0.7927 | Val Corr: 0.1124 | Val RMSE: 1.0029\n",
      "   Epoch 10/100 | Train Loss: 0.5955 | Val Loss: 0.7887 | Val Corr: 0.4094 | Val RMSE: 0.9581\n",
      "   Epoch 20/100 | Train Loss: 0.4596 | Val Loss: 0.8019 | Val Corr: 0.4497 | Val RMSE: 0.9936\n",
      "   Epoch 30/100 | Train Loss: 0.4001 | Val Loss: 0.9077 | Val Corr: 0.4676 | Val RMSE: 1.1228\n",
      "   Epoch 40/100 | Train Loss: 0.3532 | Val Loss: 0.7981 | Val Corr: 0.4820 | Val RMSE: 1.0029\n",
      "   Epoch 50/100 | Train Loss: 0.3119 | Val Loss: 0.8279 | Val Corr: 0.4872 | Val RMSE: 1.0392\n",
      "Early stopping at epoch 52 (best epoch 37, best corr 0.4903)\n",
      "BEST epoch = 37 | BEST Corr = 0.4903 | RMSE@BEST = 1.1625\n",
      "----------------------------------------\n",
      "Seed 4 | Pool AVG | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7489 | Val Loss: 0.7771 | Val Corr: 0.1794 | Val RMSE: 1.0225\n",
      "   Epoch 10/100 | Train Loss: 0.5671 | Val Loss: 1.0999 | Val Corr: 0.2801 | Val RMSE: 1.2879\n",
      "   Epoch 20/100 | Train Loss: 0.4199 | Val Loss: 1.0063 | Val Corr: 0.3013 | Val RMSE: 1.2462\n",
      "   Epoch 30/100 | Train Loss: 0.3706 | Val Loss: 0.9413 | Val Corr: 0.2999 | Val RMSE: 1.1698\n",
      "   Epoch 40/100 | Train Loss: 0.3317 | Val Loss: 0.8516 | Val Corr: 0.3352 | Val RMSE: 1.0810\n",
      "   Epoch 50/100 | Train Loss: 0.2877 | Val Loss: 0.7613 | Val Corr: 0.3299 | Val RMSE: 1.0285\n",
      "   Epoch 60/100 | Train Loss: 0.2693 | Val Loss: 0.8480 | Val Corr: 0.3453 | Val RMSE: 1.0799\n",
      "   Epoch 70/100 | Train Loss: 0.2769 | Val Loss: 0.8277 | Val Corr: 0.3375 | Val RMSE: 1.0619\n",
      "Early stopping at epoch 77 (best epoch 62, best corr 0.3577)\n",
      "BEST epoch = 62 | BEST Corr = 0.3577 | RMSE@BEST = 1.0748\n",
      "----------------------------------------\n",
      "      Processing Trait: VE\n",
      "Seed 0 | Pool AVG | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7858 | Val Loss: 0.7640 | Val Corr: 0.2108 | Val RMSE: 0.9704\n",
      "   Epoch 10/100 | Train Loss: 0.5914 | Val Loss: 0.6883 | Val Corr: 0.3853 | Val RMSE: 0.9119\n",
      "   Epoch 20/100 | Train Loss: 0.4815 | Val Loss: 0.8163 | Val Corr: 0.3898 | Val RMSE: 1.0344\n",
      "   Epoch 30/100 | Train Loss: 0.4049 | Val Loss: 0.7381 | Val Corr: 0.4279 | Val RMSE: 0.9495\n",
      "   Epoch 40/100 | Train Loss: 0.3793 | Val Loss: 0.7729 | Val Corr: 0.3956 | Val RMSE: 1.0049\n",
      "Early stopping at epoch 46 (best epoch 31, best corr 0.4368)\n",
      "BEST epoch = 31 | BEST Corr = 0.4368 | RMSE@BEST = 0.9248\n",
      "----------------------------------------\n",
      "Seed 1 | Pool AVG | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7680 | Val Loss: 0.7978 | Val Corr: 0.2773 | Val RMSE: 1.0563\n",
      "   Epoch 10/100 | Train Loss: 0.5589 | Val Loss: 0.7692 | Val Corr: 0.4669 | Val RMSE: 0.9827\n",
      "   Epoch 20/100 | Train Loss: 0.4712 | Val Loss: 0.7296 | Val Corr: 0.4869 | Val RMSE: 0.9811\n",
      "   Epoch 30/100 | Train Loss: 0.3845 | Val Loss: 0.7410 | Val Corr: 0.4885 | Val RMSE: 0.9976\n",
      "   Epoch 40/100 | Train Loss: 0.3552 | Val Loss: 0.7551 | Val Corr: 0.4714 | Val RMSE: 0.9842\n",
      "   Epoch 50/100 | Train Loss: 0.3312 | Val Loss: 0.7468 | Val Corr: 0.4798 | Val RMSE: 0.9700\n",
      "Early stopping at epoch 60 (best epoch 45, best corr 0.4898)\n",
      "BEST epoch = 45 | BEST Corr = 0.4898 | RMSE@BEST = 1.0208\n",
      "----------------------------------------\n",
      "Seed 2 | Pool AVG | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7758 | Val Loss: 0.8027 | Val Corr: 0.2210 | Val RMSE: 0.9737\n",
      "   Epoch 10/100 | Train Loss: 0.5895 | Val Loss: 0.7854 | Val Corr: 0.3749 | Val RMSE: 0.9736\n",
      "   Epoch 20/100 | Train Loss: 0.4745 | Val Loss: 0.7888 | Val Corr: 0.4260 | Val RMSE: 1.0021\n",
      "   Epoch 30/100 | Train Loss: 0.4165 | Val Loss: 0.7934 | Val Corr: 0.3659 | Val RMSE: 1.0037\n",
      "Early stopping at epoch 35 (best epoch 20, best corr 0.4260)\n",
      "BEST epoch = 20 | BEST Corr = 0.4260 | RMSE@BEST = 1.0021\n",
      "----------------------------------------\n",
      "Seed 3 | Pool AVG | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7626 | Val Loss: 0.8335 | Val Corr: 0.2368 | Val RMSE: 1.0452\n",
      "   Epoch 10/100 | Train Loss: 0.6052 | Val Loss: 0.7705 | Val Corr: 0.4376 | Val RMSE: 0.9585\n",
      "   Epoch 20/100 | Train Loss: 0.5085 | Val Loss: 0.9214 | Val Corr: 0.4346 | Val RMSE: 1.1036\n",
      "   Epoch 30/100 | Train Loss: 0.4268 | Val Loss: 0.7806 | Val Corr: 0.4458 | Val RMSE: 0.9830\n",
      "   Epoch 40/100 | Train Loss: 0.3760 | Val Loss: 0.7773 | Val Corr: 0.4580 | Val RMSE: 0.9758\n",
      "   Epoch 50/100 | Train Loss: 0.3361 | Val Loss: 0.8629 | Val Corr: 0.4401 | Val RMSE: 1.0680\n",
      "Early stopping at epoch 57 (best epoch 42, best corr 0.4628)\n",
      "BEST epoch = 42 | BEST Corr = 0.4628 | RMSE@BEST = 0.9611\n",
      "----------------------------------------\n",
      "Seed 4 | Pool AVG | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7849 | Val Loss: 0.7169 | Val Corr: 0.2075 | Val RMSE: 0.9410\n",
      "   Epoch 10/100 | Train Loss: 0.5918 | Val Loss: 0.9760 | Val Corr: 0.3628 | Val RMSE: 1.1845\n",
      "   Epoch 20/100 | Train Loss: 0.4529 | Val Loss: 0.7844 | Val Corr: 0.3413 | Val RMSE: 1.0039\n",
      "   Epoch 30/100 | Train Loss: 0.3920 | Val Loss: 0.8254 | Val Corr: 0.3551 | Val RMSE: 1.0391\n",
      "Early stopping at epoch 31 (best epoch 16, best corr 0.3793)\n",
      "BEST epoch = 16 | BEST Corr = 0.3793 | RMSE@BEST = 1.0190\n",
      "----------------------------------------\n",
      "   [Saved] bulls_T5000_AVG.json\n",
      "\n",
      "   >>> Processing Pooling Type: MAX | Species: bulls | Feature Count: T5000\n",
      "      Processing Trait: MS\n",
      "Seed 0 | Pool MAX | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7856 | Val Loss: 0.8009 | Val Corr: -0.0266 | Val RMSE: 1.0475\n",
      "   Epoch 10/100 | Train Loss: 0.7490 | Val Loss: 0.8263 | Val Corr: 0.2658 | Val RMSE: 1.0443\n",
      "   Epoch 20/100 | Train Loss: 0.5955 | Val Loss: 0.8649 | Val Corr: 0.3490 | Val RMSE: 1.0493\n",
      "   Epoch 30/100 | Train Loss: 0.4907 | Val Loss: 0.8003 | Val Corr: 0.3723 | Val RMSE: 0.9894\n",
      "   Epoch 40/100 | Train Loss: 0.4142 | Val Loss: 0.7933 | Val Corr: 0.4250 | Val RMSE: 0.9977\n",
      "   Epoch 50/100 | Train Loss: 0.3778 | Val Loss: 0.8024 | Val Corr: 0.4033 | Val RMSE: 1.0303\n",
      "Early stopping at epoch 55 (best epoch 40, best corr 0.4250)\n",
      "BEST epoch = 40 | BEST Corr = 0.4250 | RMSE@BEST = 0.9977\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAX | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7929 | Val Loss: 0.7661 | Val Corr: 0.0820 | Val RMSE: 0.9920\n",
      "   Epoch 10/100 | Train Loss: 0.7449 | Val Loss: 0.7503 | Val Corr: 0.3339 | Val RMSE: 0.9751\n",
      "   Epoch 20/100 | Train Loss: 0.5848 | Val Loss: 0.7137 | Val Corr: 0.4141 | Val RMSE: 0.9127\n",
      "   Epoch 30/100 | Train Loss: 0.4615 | Val Loss: 0.8233 | Val Corr: 0.4341 | Val RMSE: 1.0232\n",
      "   Epoch 40/100 | Train Loss: 0.4082 | Val Loss: 0.8200 | Val Corr: 0.4368 | Val RMSE: 1.0366\n",
      "Early stopping at epoch 41 (best epoch 26, best corr 0.4630)\n",
      "BEST epoch = 26 | BEST Corr = 0.4630 | RMSE@BEST = 0.8914\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAX | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7837 | Val Loss: 0.7674 | Val Corr: 0.0025 | Val RMSE: 0.9608\n",
      "   Epoch 10/100 | Train Loss: 0.7646 | Val Loss: 0.7579 | Val Corr: 0.2653 | Val RMSE: 0.9540\n",
      "   Epoch 20/100 | Train Loss: 0.6150 | Val Loss: 0.7400 | Val Corr: 0.3962 | Val RMSE: 0.9169\n",
      "   Epoch 30/100 | Train Loss: 0.4914 | Val Loss: 0.7609 | Val Corr: 0.4263 | Val RMSE: 0.9368\n",
      "   Epoch 40/100 | Train Loss: 0.4116 | Val Loss: 0.7772 | Val Corr: 0.4218 | Val RMSE: 0.9761\n",
      "   Epoch 50/100 | Train Loss: 0.3614 | Val Loss: 0.8413 | Val Corr: 0.4182 | Val RMSE: 1.0549\n",
      "Early stopping at epoch 54 (best epoch 39, best corr 0.4337)\n",
      "BEST epoch = 39 | BEST Corr = 0.4337 | RMSE@BEST = 0.9924\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAX | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7841 | Val Loss: 0.7791 | Val Corr: 0.0915 | Val RMSE: 1.0144\n",
      "   Epoch 10/100 | Train Loss: 0.7665 | Val Loss: 0.7611 | Val Corr: 0.2194 | Val RMSE: 0.9985\n",
      "   Epoch 20/100 | Train Loss: 0.6400 | Val Loss: 0.6996 | Val Corr: 0.3846 | Val RMSE: 0.9186\n",
      "   Epoch 30/100 | Train Loss: 0.4955 | Val Loss: 0.6985 | Val Corr: 0.4382 | Val RMSE: 0.9195\n",
      "   Epoch 40/100 | Train Loss: 0.4252 | Val Loss: 0.7124 | Val Corr: 0.4302 | Val RMSE: 0.9394\n",
      "Early stopping at epoch 47 (best epoch 32, best corr 0.4537)\n",
      "BEST epoch = 32 | BEST Corr = 0.4537 | RMSE@BEST = 0.9337\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAX | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7773 | Val Loss: 0.7825 | Val Corr: -0.0711 | Val RMSE: 0.9950\n",
      "   Epoch 10/100 | Train Loss: 0.7276 | Val Loss: 0.8639 | Val Corr: 0.1915 | Val RMSE: 1.0556\n",
      "   Epoch 20/100 | Train Loss: 0.5520 | Val Loss: 0.9454 | Val Corr: 0.2378 | Val RMSE: 1.1479\n",
      "   Epoch 30/100 | Train Loss: 0.4662 | Val Loss: 0.8420 | Val Corr: 0.2737 | Val RMSE: 1.0445\n",
      "   Epoch 40/100 | Train Loss: 0.3931 | Val Loss: 0.8403 | Val Corr: 0.2951 | Val RMSE: 1.0696\n",
      "   Epoch 50/100 | Train Loss: 0.3502 | Val Loss: 0.8636 | Val Corr: 0.3051 | Val RMSE: 1.0682\n",
      "   Epoch 60/100 | Train Loss: 0.3161 | Val Loss: 0.8532 | Val Corr: 0.2944 | Val RMSE: 1.0658\n",
      "Early stopping at epoch 64 (best epoch 49, best corr 0.3117)\n",
      "BEST epoch = 49 | BEST Corr = 0.3117 | RMSE@BEST = 1.0987\n",
      "----------------------------------------\n",
      "      Processing Trait: NMSP\n",
      "Seed 0 | Pool MAX | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7631 | Val Loss: 0.7492 | Val Corr: 0.0162 | Val RMSE: 0.9299\n",
      "   Epoch 10/100 | Train Loss: 0.6935 | Val Loss: 0.7086 | Val Corr: 0.4249 | Val RMSE: 0.8955\n",
      "   Epoch 20/100 | Train Loss: 0.5641 | Val Loss: 0.6324 | Val Corr: 0.4844 | Val RMSE: 0.8622\n",
      "   Epoch 30/100 | Train Loss: 0.4646 | Val Loss: 0.6286 | Val Corr: 0.5087 | Val RMSE: 0.8145\n",
      "   Epoch 40/100 | Train Loss: 0.3956 | Val Loss: 0.6150 | Val Corr: 0.5347 | Val RMSE: 0.8054\n",
      "   Epoch 50/100 | Train Loss: 0.3443 | Val Loss: 0.6420 | Val Corr: 0.5566 | Val RMSE: 0.8232\n",
      "Early stopping at epoch 57 (best epoch 42, best corr 0.5619)\n",
      "BEST epoch = 42 | BEST Corr = 0.5619 | RMSE@BEST = 0.8002\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAX | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7513 | Val Loss: 0.8519 | Val Corr: 0.1122 | Val RMSE: 1.0662\n",
      "   Epoch 10/100 | Train Loss: 0.6469 | Val Loss: 1.0026 | Val Corr: 0.3266 | Val RMSE: 1.1857\n",
      "   Epoch 20/100 | Train Loss: 0.5402 | Val Loss: 1.2752 | Val Corr: 0.4659 | Val RMSE: 1.4596\n",
      "   Epoch 30/100 | Train Loss: 0.4016 | Val Loss: 0.8154 | Val Corr: 0.4818 | Val RMSE: 1.0143\n",
      "Early stopping at epoch 33 (best epoch 18, best corr 0.4877)\n",
      "BEST epoch = 18 | BEST Corr = 0.4877 | RMSE@BEST = 1.2486\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAX | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7514 | Val Loss: 0.7977 | Val Corr: -0.0220 | Val RMSE: 0.9864\n",
      "   Epoch 10/100 | Train Loss: 0.6996 | Val Loss: 0.8342 | Val Corr: 0.3386 | Val RMSE: 0.9953\n",
      "   Epoch 20/100 | Train Loss: 0.5544 | Val Loss: 0.7640 | Val Corr: 0.4251 | Val RMSE: 0.9291\n",
      "   Epoch 30/100 | Train Loss: 0.4480 | Val Loss: 0.7743 | Val Corr: 0.4886 | Val RMSE: 0.9774\n",
      "   Epoch 40/100 | Train Loss: 0.3774 | Val Loss: 0.8990 | Val Corr: 0.5314 | Val RMSE: 1.1163\n",
      "   Epoch 50/100 | Train Loss: 0.3333 | Val Loss: 0.7592 | Val Corr: 0.5380 | Val RMSE: 0.9611\n",
      "   Epoch 60/100 | Train Loss: 0.3165 | Val Loss: 0.7047 | Val Corr: 0.5519 | Val RMSE: 0.9209\n",
      "Early stopping at epoch 68 (best epoch 53, best corr 0.5622)\n",
      "BEST epoch = 53 | BEST Corr = 0.5622 | RMSE@BEST = 0.8944\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAX | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7540 | Val Loss: 0.8390 | Val Corr: 0.0723 | Val RMSE: 1.0285\n",
      "   Epoch 10/100 | Train Loss: 0.7045 | Val Loss: 0.9370 | Val Corr: 0.3300 | Val RMSE: 1.1104\n",
      "   Epoch 20/100 | Train Loss: 0.5557 | Val Loss: 1.1716 | Val Corr: 0.4066 | Val RMSE: 1.3385\n",
      "   Epoch 30/100 | Train Loss: 0.4552 | Val Loss: 0.8287 | Val Corr: 0.4777 | Val RMSE: 1.0058\n",
      "   Epoch 40/100 | Train Loss: 0.3998 | Val Loss: 0.9797 | Val Corr: 0.4709 | Val RMSE: 1.1749\n",
      "   Epoch 50/100 | Train Loss: 0.3454 | Val Loss: 0.7647 | Val Corr: 0.4987 | Val RMSE: 0.9899\n",
      "   Epoch 60/100 | Train Loss: 0.3380 | Val Loss: 0.7673 | Val Corr: 0.4896 | Val RMSE: 0.9968\n",
      "Early stopping at epoch 68 (best epoch 53, best corr 0.4989)\n",
      "BEST epoch = 53 | BEST Corr = 0.4989 | RMSE@BEST = 0.9736\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAX | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7473 | Val Loss: 0.7970 | Val Corr: -0.1229 | Val RMSE: 1.0247\n",
      "   Epoch 10/100 | Train Loss: 0.6563 | Val Loss: 0.8308 | Val Corr: 0.2489 | Val RMSE: 1.0254\n",
      "   Epoch 20/100 | Train Loss: 0.5196 | Val Loss: 1.0529 | Val Corr: 0.2836 | Val RMSE: 1.2662\n",
      "   Epoch 30/100 | Train Loss: 0.4039 | Val Loss: 0.8327 | Val Corr: 0.3138 | Val RMSE: 1.0820\n",
      "   Epoch 40/100 | Train Loss: 0.3447 | Val Loss: 0.9088 | Val Corr: 0.3224 | Val RMSE: 1.1764\n",
      "   Epoch 50/100 | Train Loss: 0.3187 | Val Loss: 0.9448 | Val Corr: 0.3100 | Val RMSE: 1.2086\n",
      "Early stopping at epoch 56 (best epoch 41, best corr 0.3308)\n",
      "BEST epoch = 41 | BEST Corr = 0.3308 | RMSE@BEST = 1.0822\n",
      "----------------------------------------\n",
      "      Processing Trait: VE\n",
      "Seed 0 | Pool MAX | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7825 | Val Loss: 0.7612 | Val Corr: 0.0387 | Val RMSE: 0.9723\n",
      "   Epoch 10/100 | Train Loss: 0.6815 | Val Loss: 0.7350 | Val Corr: 0.3041 | Val RMSE: 0.9770\n",
      "   Epoch 20/100 | Train Loss: 0.5353 | Val Loss: 0.7059 | Val Corr: 0.3668 | Val RMSE: 0.9117\n",
      "   Epoch 30/100 | Train Loss: 0.4506 | Val Loss: 0.7811 | Val Corr: 0.3877 | Val RMSE: 1.0086\n",
      "   Epoch 40/100 | Train Loss: 0.3969 | Val Loss: 0.7900 | Val Corr: 0.3878 | Val RMSE: 1.0213\n",
      "   Epoch 50/100 | Train Loss: 0.3530 | Val Loss: 0.7629 | Val Corr: 0.4010 | Val RMSE: 0.9992\n",
      "Early stopping at epoch 60 (best epoch 45, best corr 0.4110)\n",
      "BEST epoch = 45 | BEST Corr = 0.4110 | RMSE@BEST = 1.0512\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAX | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7704 | Val Loss: 0.8202 | Val Corr: 0.1525 | Val RMSE: 1.0636\n",
      "   Epoch 10/100 | Train Loss: 0.6654 | Val Loss: 0.9891 | Val Corr: 0.4322 | Val RMSE: 1.2005\n",
      "   Epoch 20/100 | Train Loss: 0.5462 | Val Loss: 0.8615 | Val Corr: 0.4661 | Val RMSE: 1.0782\n",
      "   Epoch 30/100 | Train Loss: 0.4406 | Val Loss: 0.7161 | Val Corr: 0.4832 | Val RMSE: 0.9491\n",
      "   Epoch 40/100 | Train Loss: 0.3712 | Val Loss: 0.7584 | Val Corr: 0.4747 | Val RMSE: 0.9980\n",
      "Early stopping at epoch 45 (best epoch 30, best corr 0.4832)\n",
      "BEST epoch = 30 | BEST Corr = 0.4832 | RMSE@BEST = 0.9491\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAX | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7750 | Val Loss: 0.8071 | Val Corr: 0.0288 | Val RMSE: 0.9700\n",
      "   Epoch 10/100 | Train Loss: 0.7112 | Val Loss: 0.7658 | Val Corr: 0.3888 | Val RMSE: 0.9282\n",
      "   Epoch 20/100 | Train Loss: 0.5728 | Val Loss: 0.7151 | Val Corr: 0.4292 | Val RMSE: 0.8875\n",
      "   Epoch 30/100 | Train Loss: 0.4694 | Val Loss: 0.7318 | Val Corr: 0.4304 | Val RMSE: 0.9091\n",
      "Early stopping at epoch 32 (best epoch 17, best corr 0.4486)\n",
      "BEST epoch = 17 | BEST Corr = 0.4486 | RMSE@BEST = 0.9245\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAX | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7644 | Val Loss: 0.8428 | Val Corr: 0.1608 | Val RMSE: 1.0516\n",
      "   Epoch 10/100 | Train Loss: 0.7049 | Val Loss: 0.8732 | Val Corr: 0.3631 | Val RMSE: 1.0658\n",
      "   Epoch 20/100 | Train Loss: 0.5674 | Val Loss: 0.7896 | Val Corr: 0.3977 | Val RMSE: 0.9783\n",
      "   Epoch 30/100 | Train Loss: 0.4774 | Val Loss: 0.8427 | Val Corr: 0.3985 | Val RMSE: 1.0354\n",
      "Early stopping at epoch 32 (best epoch 17, best corr 0.4374)\n",
      "BEST epoch = 17 | BEST Corr = 0.4374 | RMSE@BEST = 0.9622\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAX | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7920 | Val Loss: 0.7235 | Val Corr: 0.1129 | Val RMSE: 0.9461\n",
      "   Epoch 10/100 | Train Loss: 0.6947 | Val Loss: 0.7366 | Val Corr: 0.2848 | Val RMSE: 0.9530\n",
      "   Epoch 20/100 | Train Loss: 0.5394 | Val Loss: 0.7891 | Val Corr: 0.3326 | Val RMSE: 1.0056\n",
      "   Epoch 30/100 | Train Loss: 0.4556 | Val Loss: 0.8440 | Val Corr: 0.3274 | Val RMSE: 1.0724\n",
      "Early stopping at epoch 40 (best epoch 25, best corr 0.3624)\n",
      "BEST epoch = 25 | BEST Corr = 0.3624 | RMSE@BEST = 1.0068\n",
      "----------------------------------------\n",
      "   [Saved] bulls_T5000_MAX.json\n",
      "\n",
      "   >>> Processing Pooling Type: LIP | Species: bulls | Feature Count: T5000\n",
      "      Processing Trait: MS\n",
      "Seed 0 | Pool LIP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7753 | Val Loss: 0.8024 | Val Corr: 0.1345 | Val RMSE: 1.0507\n",
      "   Epoch 10/100 | Train Loss: 0.6698 | Val Loss: 0.9898 | Val Corr: 0.3529 | Val RMSE: 1.1731\n",
      "   Epoch 20/100 | Train Loss: 0.4898 | Val Loss: 0.7997 | Val Corr: 0.3979 | Val RMSE: 0.9932\n",
      "   Epoch 30/100 | Train Loss: 0.4181 | Val Loss: 0.8403 | Val Corr: 0.4087 | Val RMSE: 1.0339\n",
      "   Epoch 40/100 | Train Loss: 0.3644 | Val Loss: 0.8085 | Val Corr: 0.4167 | Val RMSE: 1.0146\n",
      "   Epoch 50/100 | Train Loss: 0.3262 | Val Loss: 0.7957 | Val Corr: 0.4188 | Val RMSE: 1.0211\n",
      "   Epoch 60/100 | Train Loss: 0.3008 | Val Loss: 0.7920 | Val Corr: 0.4184 | Val RMSE: 0.9993\n",
      "   Epoch 70/100 | Train Loss: 0.2748 | Val Loss: 0.7899 | Val Corr: 0.4296 | Val RMSE: 1.0012\n",
      "   Epoch 80/100 | Train Loss: 0.2763 | Val Loss: 0.8066 | Val Corr: 0.4363 | Val RMSE: 1.0223\n",
      "   Epoch 90/100 | Train Loss: 0.2725 | Val Loss: 0.8039 | Val Corr: 0.4330 | Val RMSE: 1.0157\n",
      "Early stopping at epoch 95 (best epoch 80, best corr 0.4363)\n",
      "BEST epoch = 80 | BEST Corr = 0.4363 | RMSE@BEST = 1.0223\n",
      "----------------------------------------\n",
      "Seed 1 | Pool LIP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7853 | Val Loss: 0.7700 | Val Corr: 0.0598 | Val RMSE: 0.9913\n",
      "   Epoch 10/100 | Train Loss: 0.6686 | Val Loss: 0.7137 | Val Corr: 0.4101 | Val RMSE: 0.9245\n",
      "   Epoch 20/100 | Train Loss: 0.4833 | Val Loss: 0.7259 | Val Corr: 0.4396 | Val RMSE: 0.9063\n",
      "   Epoch 30/100 | Train Loss: 0.3935 | Val Loss: 0.7510 | Val Corr: 0.4294 | Val RMSE: 0.9454\n",
      "Early stopping at epoch 32 (best epoch 17, best corr 0.4502)\n",
      "BEST epoch = 17 | BEST Corr = 0.4502 | RMSE@BEST = 0.9011\n",
      "----------------------------------------\n",
      "Seed 2 | Pool LIP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7821 | Val Loss: 0.7755 | Val Corr: 0.1001 | Val RMSE: 0.9712\n",
      "   Epoch 10/100 | Train Loss: 0.6652 | Val Loss: 0.7351 | Val Corr: 0.3609 | Val RMSE: 0.9116\n",
      "   Epoch 20/100 | Train Loss: 0.5209 | Val Loss: 0.7744 | Val Corr: 0.4264 | Val RMSE: 0.9533\n",
      "   Epoch 30/100 | Train Loss: 0.4230 | Val Loss: 0.7281 | Val Corr: 0.4411 | Val RMSE: 0.9487\n",
      "   Epoch 40/100 | Train Loss: 0.3735 | Val Loss: 0.8816 | Val Corr: 0.4537 | Val RMSE: 1.0929\n",
      "Early stopping at epoch 41 (best epoch 26, best corr 0.4606)\n",
      "BEST epoch = 26 | BEST Corr = 0.4606 | RMSE@BEST = 0.9381\n",
      "----------------------------------------\n",
      "Seed 3 | Pool LIP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7892 | Val Loss: 0.7610 | Val Corr: 0.1008 | Val RMSE: 0.9928\n",
      "   Epoch 10/100 | Train Loss: 0.6960 | Val Loss: 0.7822 | Val Corr: 0.3181 | Val RMSE: 0.9913\n",
      "   Epoch 20/100 | Train Loss: 0.5225 | Val Loss: 0.6925 | Val Corr: 0.3996 | Val RMSE: 0.9260\n",
      "   Epoch 30/100 | Train Loss: 0.4290 | Val Loss: 0.6964 | Val Corr: 0.4444 | Val RMSE: 0.9194\n",
      "   Epoch 40/100 | Train Loss: 0.3845 | Val Loss: 0.7442 | Val Corr: 0.4299 | Val RMSE: 1.0198\n",
      "   Epoch 50/100 | Train Loss: 0.3442 | Val Loss: 0.7246 | Val Corr: 0.4434 | Val RMSE: 0.9725\n",
      "Early stopping at epoch 52 (best epoch 37, best corr 0.4502)\n",
      "BEST epoch = 37 | BEST Corr = 0.4502 | RMSE@BEST = 1.0880\n",
      "----------------------------------------\n",
      "Seed 4 | Pool LIP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7776 | Val Loss: 0.7843 | Val Corr: 0.0016 | Val RMSE: 0.9953\n",
      "   Epoch 10/100 | Train Loss: 0.6180 | Val Loss: 0.9540 | Val Corr: 0.2602 | Val RMSE: 1.1391\n",
      "   Epoch 20/100 | Train Loss: 0.4879 | Val Loss: 0.8257 | Val Corr: 0.3341 | Val RMSE: 1.0997\n",
      "   Epoch 30/100 | Train Loss: 0.4019 | Val Loss: 0.7971 | Val Corr: 0.3099 | Val RMSE: 1.0404\n",
      "   Epoch 40/100 | Train Loss: 0.3420 | Val Loss: 0.7942 | Val Corr: 0.3537 | Val RMSE: 1.0402\n",
      "   Epoch 50/100 | Train Loss: 0.3182 | Val Loss: 0.7663 | Val Corr: 0.3536 | Val RMSE: 1.0307\n",
      "   Epoch 60/100 | Train Loss: 0.2784 | Val Loss: 0.7789 | Val Corr: 0.3425 | Val RMSE: 1.0315\n",
      "Early stopping at epoch 66 (best epoch 51, best corr 0.3580)\n",
      "BEST epoch = 51 | BEST Corr = 0.3580 | RMSE@BEST = 1.0242\n",
      "----------------------------------------\n",
      "      Processing Trait: NMSP\n",
      "Seed 0 | Pool LIP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7540 | Val Loss: 0.7319 | Val Corr: 0.2082 | Val RMSE: 0.9266\n",
      "   Epoch 10/100 | Train Loss: 0.6112 | Val Loss: 0.6880 | Val Corr: 0.4380 | Val RMSE: 0.8508\n",
      "   Epoch 20/100 | Train Loss: 0.4586 | Val Loss: 0.7047 | Val Corr: 0.5401 | Val RMSE: 0.8576\n",
      "   Epoch 30/100 | Train Loss: 0.3943 | Val Loss: 0.6116 | Val Corr: 0.5404 | Val RMSE: 0.7918\n",
      "   Epoch 40/100 | Train Loss: 0.3359 | Val Loss: 0.6374 | Val Corr: 0.5556 | Val RMSE: 0.8425\n",
      "Early stopping at epoch 49 (best epoch 34, best corr 0.5728)\n",
      "BEST epoch = 34 | BEST Corr = 0.5728 | RMSE@BEST = 0.7923\n",
      "----------------------------------------\n",
      "Seed 1 | Pool LIP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7332 | Val Loss: 0.7961 | Val Corr: 0.0613 | Val RMSE: 1.0487\n",
      "   Epoch 10/100 | Train Loss: 0.5790 | Val Loss: 0.8453 | Val Corr: 0.4057 | Val RMSE: 1.1701\n",
      "   Epoch 20/100 | Train Loss: 0.4352 | Val Loss: 0.7486 | Val Corr: 0.4901 | Val RMSE: 1.0147\n",
      "   Epoch 30/100 | Train Loss: 0.3453 | Val Loss: 0.8096 | Val Corr: 0.5319 | Val RMSE: 1.0772\n",
      "   Epoch 40/100 | Train Loss: 0.3077 | Val Loss: 0.7794 | Val Corr: 0.5215 | Val RMSE: 1.0304\n",
      "   Epoch 50/100 | Train Loss: 0.2810 | Val Loss: 0.7070 | Val Corr: 0.5164 | Val RMSE: 0.9515\n",
      "   Epoch 60/100 | Train Loss: 0.2555 | Val Loss: 0.7480 | Val Corr: 0.5273 | Val RMSE: 0.9907\n",
      "Early stopping at epoch 63 (best epoch 48, best corr 0.5422)\n",
      "BEST epoch = 48 | BEST Corr = 0.5422 | RMSE@BEST = 0.9486\n",
      "----------------------------------------\n",
      "Seed 2 | Pool LIP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7556 | Val Loss: 0.7627 | Val Corr: 0.0892 | Val RMSE: 1.0115\n",
      "   Epoch 10/100 | Train Loss: 0.6333 | Val Loss: 0.8793 | Val Corr: 0.3695 | Val RMSE: 1.1713\n",
      "   Epoch 20/100 | Train Loss: 0.4522 | Val Loss: 0.7145 | Val Corr: 0.4902 | Val RMSE: 0.9491\n",
      "   Epoch 30/100 | Train Loss: 0.3823 | Val Loss: 0.7051 | Val Corr: 0.5119 | Val RMSE: 0.9186\n",
      "   Epoch 40/100 | Train Loss: 0.3275 | Val Loss: 0.7251 | Val Corr: 0.5115 | Val RMSE: 0.9558\n",
      "Early stopping at epoch 48 (best epoch 33, best corr 0.5301)\n",
      "BEST epoch = 33 | BEST Corr = 0.5301 | RMSE@BEST = 0.9245\n",
      "----------------------------------------\n",
      "Seed 3 | Pool LIP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7535 | Val Loss: 0.7546 | Val Corr: 0.0395 | Val RMSE: 1.0110\n",
      "   Epoch 10/100 | Train Loss: 0.6244 | Val Loss: 0.7994 | Val Corr: 0.3930 | Val RMSE: 0.9694\n",
      "   Epoch 20/100 | Train Loss: 0.4751 | Val Loss: 0.7558 | Val Corr: 0.4783 | Val RMSE: 0.9356\n",
      "   Epoch 30/100 | Train Loss: 0.3824 | Val Loss: 0.6528 | Val Corr: 0.5139 | Val RMSE: 0.9029\n",
      "   Epoch 40/100 | Train Loss: 0.3346 | Val Loss: 0.6564 | Val Corr: 0.5212 | Val RMSE: 0.8918\n",
      "   Epoch 50/100 | Train Loss: 0.3059 | Val Loss: 0.6779 | Val Corr: 0.5144 | Val RMSE: 0.8983\n",
      "Early stopping at epoch 54 (best epoch 39, best corr 0.5306)\n",
      "BEST epoch = 39 | BEST Corr = 0.5306 | RMSE@BEST = 0.9437\n",
      "----------------------------------------\n",
      "Seed 4 | Pool LIP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7455 | Val Loss: 0.7790 | Val Corr: 0.0957 | Val RMSE: 1.0221\n",
      "   Epoch 10/100 | Train Loss: 0.5889 | Val Loss: 0.7477 | Val Corr: 0.2432 | Val RMSE: 1.0038\n",
      "   Epoch 20/100 | Train Loss: 0.4055 | Val Loss: 0.8359 | Val Corr: 0.3142 | Val RMSE: 1.0790\n",
      "   Epoch 30/100 | Train Loss: 0.3675 | Val Loss: 0.7655 | Val Corr: 0.3574 | Val RMSE: 1.0254\n",
      "   Epoch 40/100 | Train Loss: 0.3262 | Val Loss: 0.8336 | Val Corr: 0.3444 | Val RMSE: 1.1179\n",
      "Early stopping at epoch 43 (best epoch 28, best corr 0.3634)\n",
      "BEST epoch = 28 | BEST Corr = 0.3634 | RMSE@BEST = 1.0133\n",
      "----------------------------------------\n",
      "      Processing Trait: VE\n",
      "Seed 0 | Pool LIP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7738 | Val Loss: 0.7620 | Val Corr: 0.2090 | Val RMSE: 0.9699\n",
      "   Epoch 10/100 | Train Loss: 0.6138 | Val Loss: 0.7073 | Val Corr: 0.3432 | Val RMSE: 0.9149\n",
      "   Epoch 20/100 | Train Loss: 0.4590 | Val Loss: 0.8096 | Val Corr: 0.3579 | Val RMSE: 1.0464\n",
      "   Epoch 30/100 | Train Loss: 0.4010 | Val Loss: 0.8000 | Val Corr: 0.3677 | Val RMSE: 1.0236\n",
      "Early stopping at epoch 31 (best epoch 16, best corr 0.3836)\n",
      "BEST epoch = 16 | BEST Corr = 0.3836 | RMSE@BEST = 0.9421\n",
      "----------------------------------------\n",
      "Seed 1 | Pool LIP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7641 | Val Loss: 0.7955 | Val Corr: 0.2514 | Val RMSE: 1.0641\n",
      "   Epoch 10/100 | Train Loss: 0.6037 | Val Loss: 0.7206 | Val Corr: 0.4403 | Val RMSE: 0.9647\n",
      "   Epoch 20/100 | Train Loss: 0.4745 | Val Loss: 0.8293 | Val Corr: 0.4635 | Val RMSE: 1.0440\n",
      "   Epoch 30/100 | Train Loss: 0.3954 | Val Loss: 0.9126 | Val Corr: 0.4923 | Val RMSE: 1.1681\n",
      "   Epoch 40/100 | Train Loss: 0.3491 | Val Loss: 0.9644 | Val Corr: 0.4716 | Val RMSE: 1.2361\n",
      "Early stopping at epoch 43 (best epoch 28, best corr 0.5098)\n",
      "BEST epoch = 28 | BEST Corr = 0.5098 | RMSE@BEST = 1.1025\n",
      "----------------------------------------\n",
      "Seed 2 | Pool LIP | Train Samples: 1206 | Test Samples: 302\n",
      "   Epoch 1/100 | Train Loss: 0.7712 | Val Loss: 0.7993 | Val Corr: 0.2066 | Val RMSE: 0.9668\n",
      "   Epoch 10/100 | Train Loss: 0.6198 | Val Loss: 0.7832 | Val Corr: 0.4215 | Val RMSE: 0.9570\n",
      "   Epoch 20/100 | Train Loss: 0.4847 | Val Loss: 0.7155 | Val Corr: 0.4288 | Val RMSE: 0.9210\n",
      "Early stopping at epoch 26 (best epoch 11, best corr 0.4426)\n",
      "BEST epoch = 11 | BEST Corr = 0.4426 | RMSE@BEST = 0.9380\n",
      "----------------------------------------\n",
      "Seed 3 | Pool LIP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7664 | Val Loss: 0.8238 | Val Corr: 0.1669 | Val RMSE: 1.0636\n",
      "   Epoch 10/100 | Train Loss: 0.6161 | Val Loss: 0.7746 | Val Corr: 0.4501 | Val RMSE: 1.0224\n",
      "   Epoch 20/100 | Train Loss: 0.4984 | Val Loss: 0.8151 | Val Corr: 0.4607 | Val RMSE: 1.0500\n",
      "   Epoch 30/100 | Train Loss: 0.4191 | Val Loss: 0.7653 | Val Corr: 0.4613 | Val RMSE: 0.9581\n",
      "Early stopping at epoch 40 (best epoch 25, best corr 0.4771)\n",
      "BEST epoch = 25 | BEST Corr = 0.4771 | RMSE@BEST = 0.9531\n",
      "----------------------------------------\n",
      "Seed 4 | Pool LIP | Train Samples: 1207 | Test Samples: 301\n",
      "   Epoch 1/100 | Train Loss: 0.7816 | Val Loss: 0.7257 | Val Corr: 0.1911 | Val RMSE: 0.9531\n",
      "   Epoch 10/100 | Train Loss: 0.6139 | Val Loss: 0.6519 | Val Corr: 0.3726 | Val RMSE: 0.8768\n",
      "   Epoch 20/100 | Train Loss: 0.4738 | Val Loss: 0.6760 | Val Corr: 0.3674 | Val RMSE: 0.9229\n",
      "   Epoch 30/100 | Train Loss: 0.4055 | Val Loss: 0.8126 | Val Corr: 0.3729 | Val RMSE: 1.0819\n",
      "   Epoch 40/100 | Train Loss: 0.3566 | Val Loss: 0.7788 | Val Corr: 0.3804 | Val RMSE: 1.0307\n",
      "Early stopping at epoch 41 (best epoch 26, best corr 0.4165)\n",
      "BEST epoch = 26 | BEST Corr = 0.4165 | RMSE@BEST = 0.9145\n",
      "----------------------------------------\n",
      "   [Saved] bulls_T5000_LIP.json\n"
     ]
    }
   ],
   "source": [
    "# [3] Ablation study pipeline: (1) LD threshold sweep and (2) pooling strategy sweep; trains EBMGP across seeds/species/traits and saves mean metrics to JSON\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# --- Configuration ---\n",
    "learning_rate = 0.0005\n",
    "epochs = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Optimal Feature Counts per Species (Used for both ablations)\n",
    "species_optimal_counts = {\n",
    "    \"rice\": 5000,\n",
    "    \"sorghum\": 5000,\n",
    "    # \"soybean\": 3000,\n",
    "    \"bulls\": 5000\n",
    "}\n",
    "\n",
    "species_config = {\n",
    "    \"rice\": {\n",
    "        \"label_path\": \"./data/rice_pheno.csv\",\n",
    "        \"geno_path\": \"./data/ricerawgeno.csv\",\n",
    "        \"traits\": ['SW', 'FLW', 'AC', 'PH', 'SNPP']\n",
    "    },\n",
    "    \"sorghum\": {\n",
    "        \"label_path\": \"./data/sorghum_pheno.csv\",\n",
    "        \"geno_path\": \"./data/sorghumrawgeno.csv\",\n",
    "        \"traits\": ['HT', 'MO', 'YLD']\n",
    "    },\n",
    "    # \"soybean\": {\n",
    "    #     \"label_path\": \"./data/soybean_pheno.csv\",\n",
    "    #     \"geno_path\": \"./data/soybeanrawgeno.csv\",\n",
    "    #     \"traits\": ['protein', 'Steartic', 'R8', 'SdWgt', 'Yield']\n",
    "    # },\n",
    "    \"bulls\": {\n",
    "        \"label_path\": \"./data/bulls_pheno.csv\",\n",
    "        \"geno_path\": \"./data/bullsrawgeno.csv\",\n",
    "        \"traits\": ['MS', 'NMSP', 'VE']\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Experiment 1: LD Threshold Ablation ---\n",
    "def run_ld_ablation():\n",
    "    print(\"\\n\" + \"#\"*60)\n",
    "    print(\"RUNNING EXPERIMENT: LD THRESHOLD ABLATION\")\n",
    "    print(\"#\"*60)\n",
    "    \n",
    "    ld_thresholds = [0.2, 0.4, 0.6, 0.8]\n",
    "    \n",
    "    for species, optimal_count in species_optimal_counts.items():\n",
    "        config = species_config[species]\n",
    "        sel_num = optimal_count\n",
    "        T_folder = f\"T{optimal_count}\"\n",
    "        \n",
    "        for ld_thresh in ld_thresholds:\n",
    "            print(f\"\\n   >>> Processing LD Threshold: {ld_thresh} | Species: {species} | Feature Count: {T_folder}\")\n",
    "            \n",
    "            results_data = {\n",
    "                \"species\": species,\n",
    "                \"feature_count\": optimal_count,\n",
    "                \"ld_threshold\": ld_thresh,\n",
    "                \"pooling_type\": \"MAP\", # Default for LD ablation\n",
    "                \"mean_corrs\": {},\n",
    "                \"mean_RMSE\": {}\n",
    "            }\n",
    "\n",
    "            for trait_idx, trait_name in enumerate(config[\"traits\"]):\n",
    "                data_path = f\"./EN/{species}/{T_folder}{trait_name}\"\n",
    "                \n",
    "                if not os.path.exists(f\"{data_path}0.csv\"):\n",
    "                    continue\n",
    "\n",
    "                corrs, RMSE = [], []\n",
    "                print(f\"      Processing Trait: {trait_name}\")\n",
    "                \n",
    "                for seed in range(5):\n",
    "                    try:\n",
    "                        # Uses default pooling_type=\"MAP\"\n",
    "                        fold_corrs, fold_RMSE, _, _ = train_and_evaluate(\n",
    "                            trait_idx, data_path, config[\"label_path\"], config[\"geno_path\"], \n",
    "                            device, learning_rate, epochs, seed, sel_num,\n",
    "                            ld_threshold=ld_thresh, pooling_type=\"MAP\"\n",
    "                        )\n",
    "                        corrs.append(fold_corrs)\n",
    "                        RMSE.append(fold_RMSE)\n",
    "                    except Exception as e:\n",
    "                        print(f\"         [Error] Seed {seed}: {e}\")\n",
    "\n",
    "                if corrs:\n",
    "                    results_data[\"mean_corrs\"][trait_name] = float(np.mean(corrs))\n",
    "                    results_data[\"mean_RMSE\"][trait_name] = float(np.mean(RMSE))\n",
    "\n",
    "            # Save JSON\n",
    "            json_filename = f\"{species}_{T_folder}_LD{ld_thresh}.json\"\n",
    "            with open(json_filename, \"w\") as f:\n",
    "                json.dump(results_data, f, indent=4)\n",
    "            print(f\"   [Saved] {json_filename}\")\n",
    "\n",
    "\n",
    "# --- Experiment 2: Pooling Strategy Ablation ---\n",
    "def run_pooling_ablation():\n",
    "    print(\"\\n\" + \"#\"*60)\n",
    "    print(\"RUNNING EXPERIMENT: POOLING STRATEGY ABLATION\")\n",
    "    print(\"#\"*60)\n",
    "    \n",
    "    pooling_types = [\"MAP\", \"AVG\", \"MAX\", \"LIP\"]\n",
    "    \n",
    "    for species, optimal_count in species_optimal_counts.items():\n",
    "        config = species_config[species]\n",
    "        sel_num = optimal_count\n",
    "        T_folder = f\"T{optimal_count}\"\n",
    "        \n",
    "        for p_type in pooling_types:\n",
    "            print(f\"\\n   >>> Processing Pooling Type: {p_type} | Species: {species} | Feature Count: {T_folder}\")\n",
    "            \n",
    "            results_data = {\n",
    "                \"species\": species,\n",
    "                \"feature_count\": optimal_count,\n",
    "                \"ld_threshold\": 0.8, # Default for Pooling ablation\n",
    "                \"pooling_type\": p_type,\n",
    "                \"mean_corrs\": {},\n",
    "                \"mean_RMSE\": {}\n",
    "            }\n",
    "\n",
    "            for trait_idx, trait_name in enumerate(config[\"traits\"]):\n",
    "                data_path = f\"./EN/{species}/{T_folder}{trait_name}\"\n",
    "                \n",
    "                if not os.path.exists(f\"{data_path}0.csv\"):\n",
    "                    continue\n",
    "\n",
    "                corrs, RMSE = [], []\n",
    "                print(f\"      Processing Trait: {trait_name}\")\n",
    "                \n",
    "                for seed in range(5):\n",
    "                    try:\n",
    "                        # Uses default ld_threshold=0.8\n",
    "                        fold_corrs, fold_RMSE, _, _ = train_and_evaluate(\n",
    "                            trait_idx, data_path, config[\"label_path\"], config[\"geno_path\"], \n",
    "                            device, learning_rate, epochs, seed, sel_num,\n",
    "                            ld_threshold=0.8, pooling_type=p_type\n",
    "                        )\n",
    "                        corrs.append(fold_corrs)\n",
    "                        RMSE.append(fold_RMSE)\n",
    "                    except Exception as e:\n",
    "                        print(f\"         [Error] Seed {seed}: {e}\")\n",
    "\n",
    "                if corrs:\n",
    "                    results_data[\"mean_corrs\"][trait_name] = float(np.mean(corrs))\n",
    "                    results_data[\"mean_RMSE\"][trait_name] = float(np.mean(RMSE))\n",
    "\n",
    "            # Save JSON\n",
    "            json_filename = f\"{species}_{T_folder}_{p_type}.json\"\n",
    "            with open(json_filename, \"w\") as f:\n",
    "                json.dump(results_data, f, indent=4)\n",
    "            print(f\"   [Saved] {json_filename}\")\n",
    "\n",
    "def main():\n",
    "    # You can comment out one of these lines if you only want to run one experiment\n",
    "    run_ld_ablation()\n",
    "    run_pooling_ablation()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27b0b53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "RUNNING BENCHMARKING (GBLUP, Bayes, SOTA-Tree)\n",
      "############################################################\n",
      "\n",
      "   >>> Benchmarking Model: GBLUP | Species: rice | Feature Count: T5000\n",
      "      Processing Trait: SW... Done (0.5s) | Avg R2: 0.3005 | Avg Corr: 0.5545\n",
      "      Processing Trait: FLW... Done (0.5s) | Avg R2: 0.1459 | Avg Corr: 0.4046\n",
      "      Processing Trait: AC... Done (0.5s) | Avg R2: 0.1200 | Avg Corr: 0.3844\n",
      "      Processing Trait: PH... Done (0.5s) | Avg R2: 0.1841 | Avg Corr: 0.4412\n",
      "      Processing Trait: SNPP... Done (0.5s) | Avg R2: 0.0905 | Avg Corr: 0.3434\n",
      "   [Saved] rice_T5000_GBLUP.json\n",
      "\n",
      "   >>> Benchmarking Model: BayesB | Species: rice | Feature Count: T5000\n",
      "      Processing Trait: SW... Done (0.5s) | Avg R2: 0.2871 | Avg Corr: 0.5463\n",
      "      Processing Trait: FLW... Done (0.6s) | Avg R2: 0.1367 | Avg Corr: 0.4012\n",
      "      Processing Trait: AC... Done (0.5s) | Avg R2: 0.1309 | Avg Corr: 0.3851\n",
      "      Processing Trait: PH... Done (0.6s) | Avg R2: 0.1945 | Avg Corr: 0.4493\n",
      "      Processing Trait: SNPP... Done (0.6s) | Avg R2: 0.0875 | Avg Corr: 0.3442\n",
      "   [Saved] rice_T5000_BayesB.json\n",
      "\n",
      "   >>> Benchmarking Model: LightGBM | Species: rice | Feature Count: T5000\n",
      "      Processing Trait: SW... Done (0.7s) | Avg R2: 0.2692 | Avg Corr: 0.5345\n",
      "      Processing Trait: FLW... Done (0.8s) | Avg R2: -0.0294 | Avg Corr: 0.3386\n",
      "      Processing Trait: AC... Done (0.8s) | Avg R2: 0.1156 | Avg Corr: 0.3944\n",
      "      Processing Trait: PH... Done (0.8s) | Avg R2: 0.1635 | Avg Corr: 0.4504\n",
      "      Processing Trait: SNPP... Done (0.8s) | Avg R2: 0.0658 | Avg Corr: 0.3681\n",
      "   [Saved] rice_T5000_LightGBM.json\n",
      "\n",
      "   >>> Benchmarking Model: GBLUP | Species: sorghum | Feature Count: T5000\n",
      "      Processing Trait: HT... Done (5.2s) | Avg R2: 0.3139 | Avg Corr: 0.5658\n",
      "      Processing Trait: MO... Done (5.1s) | Avg R2: 0.1094 | Avg Corr: 0.3768\n",
      "      Processing Trait: YLD... Done (5.2s) | Avg R2: 0.3106 | Avg Corr: 0.5721\n",
      "   [Saved] sorghum_T5000_GBLUP.json\n",
      "\n",
      "   >>> Benchmarking Model: BayesB | Species: sorghum | Feature Count: T5000\n",
      "      Processing Trait: HT... Done (13.5s) | Avg R2: 0.2332 | Avg Corr: 0.5279\n",
      "      Processing Trait: MO... Done (13.4s) | Avg R2: -0.0346 | Avg Corr: 0.3228\n",
      "      Processing Trait: YLD... Done (13.4s) | Avg R2: 0.2665 | Avg Corr: 0.5543\n",
      "   [Saved] sorghum_T5000_BayesB.json\n",
      "\n",
      "   >>> Benchmarking Model: LightGBM | Species: sorghum | Feature Count: T5000\n",
      "      Processing Trait: HT... Done (18.5s) | Avg R2: 0.2879 | Avg Corr: 0.5477\n",
      "      Processing Trait: MO... Done (18.2s) | Avg R2: 0.0321 | Avg Corr: 0.3154\n",
      "      Processing Trait: YLD... Done (19.0s) | Avg R2: 0.2950 | Avg Corr: 0.5546\n",
      "   [Saved] sorghum_T5000_LightGBM.json\n",
      "\n",
      "   >>> Benchmarking Model: GBLUP | Species: bulls | Feature Count: T5000\n",
      "      Processing Trait: MS... Done (2.9s) | Avg R2: 0.1932 | Avg Corr: 0.4495\n",
      "      Processing Trait: NMSP... Done (2.8s) | Avg R2: 0.3118 | Avg Corr: 0.5723\n",
      "      Processing Trait: VE... Done (2.8s) | Avg R2: 0.2046 | Avg Corr: 0.4654\n",
      "   [Saved] bulls_T5000_GBLUP.json\n",
      "\n",
      "   >>> Benchmarking Model: BayesB | Species: bulls | Feature Count: T5000\n",
      "      Processing Trait: MS... Done (2.7s) | Avg R2: 0.1894 | Avg Corr: 0.4497\n",
      "      Processing Trait: NMSP... Done (2.6s) | Avg R2: 0.3193 | Avg Corr: 0.5727\n",
      "      Processing Trait: VE... Done (2.6s) | Avg R2: 0.2087 | Avg Corr: 0.4667\n",
      "   [Saved] bulls_T5000_BayesB.json\n",
      "\n",
      "   >>> Benchmarking Model: LightGBM | Species: bulls | Feature Count: T5000\n",
      "      Processing Trait: MS... Done (3.8s) | Avg R2: 0.1399 | Avg Corr: 0.3871\n",
      "      Processing Trait: NMSP... Done (3.6s) | Avg R2: 0.1409 | Avg Corr: 0.3815\n",
      "      Processing Trait: VE... Done (3.5s) | Avg R2: 0.1059 | Avg Corr: 0.3379\n",
      "   [Saved] bulls_T5000_LightGBM.json\n"
     ]
    }
   ],
   "source": [
    "#Benchy (Not Main?)\n",
    "# [2.2/3] Baseline models benchmark: load same EN-selected SNPs, encode numerically (0/1/2), reproduce CV splits, and compare Ridge/BayesianRidge/HGBRegressor against DL\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, BayesianRidge\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATA LOADER FOR NUMERICAL MODELS\n",
    "# ==========================================\n",
    "class BenchmarkDataLoader:\n",
    "    \"\"\"\n",
    "    Extracts NUMERICAL genotype matrices (0,1,2) matching the exact \n",
    "    feature selection and CV splits of the Deep Learning pipeline.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, label_path, geno_path, trait_idx, seed, sel_num):\n",
    "        # 1. Load Pre-selected Features (Elastic Net Output)\n",
    "        # We must load the exact same features used in the DL pipeline\n",
    "        cs = pd.read_csv(f\"{data_path}{seed}.csv\").sort_values(by='cs', ascending=False)\n",
    "        Top = sorted(cs.index[:sel_num])\n",
    "        \n",
    "        # 2. Load Raw Genotype Data\n",
    "        Rawgeno = pd.read_csv(geno_path, low_memory=False)\n",
    "        # Drop the first row (SNP index row) usually present in your format\n",
    "        Rawgeno = Rawgeno.iloc[1:].reset_index(drop=True)\n",
    "        \n",
    "        # Filter for valid indices\n",
    "        Top = [i for i in Top if i in Rawgeno.index]\n",
    "        geno = Rawgeno.loc[Top].copy()\n",
    "\n",
    "        # 3. Convert H/M/L to 0/1/2 (Numerical Encoding)\n",
    "        # Note: Your DL pipeline does this inside 'calculate_LD'. \n",
    "        # We assume columns before the last 2 are samples.\n",
    "        geno_vals = geno.iloc[:, :-2].values\n",
    "        \n",
    "        # Vectorized replacement for speed\n",
    "        X_num = np.select(\n",
    "            [geno_vals == 'H', geno_vals == 'M', geno_vals == 'L'],\n",
    "            [0, 1, 2],\n",
    "            default=1 # Default to heterozygous/mean if missing/unknown\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        # Transpose: (SNPs, Samples) -> (Samples, SNPs)\n",
    "        self.X = X_num.T\n",
    "        \n",
    "        # 4. Load Labels\n",
    "        annos = pd.read_csv(label_path, index_col=0).iloc[:, [trait_idx]]\n",
    "        annos = annos.fillna(annos.mean())\n",
    "        \n",
    "        # Standardize labels (Critical for convergence in Ridge/Bayes)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.y = self.scaler.fit_transform(annos).flatten().astype(np.float32)\n",
    "        \n",
    "        # 5. Reproduce Exact CV Splits\n",
    "        # Must use same random_state=27 as DL pipeline\n",
    "        self.kfold = KFold(n_splits=5, shuffle=True, random_state=27)\n",
    "        self.current_seed = seed\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"Returns X_train, X_test, y_train, y_test for the specific seed.\"\"\"\n",
    "        # Iterate to find the split matching the current seed\n",
    "        for i, (train_idx, val_idx) in enumerate(self.kfold.split(self.X, self.y)):\n",
    "            if i == self.current_seed:\n",
    "                return (\n",
    "                    self.X[train_idx], \n",
    "                    self.X[val_idx], \n",
    "                    self.y[train_idx], \n",
    "                    self.y[val_idx]\n",
    "                )\n",
    "        return None, None, None, None\n",
    "\n",
    "# ==========================================\n",
    "# 2. MODEL DEFINITIONS\n",
    "# ==========================================\n",
    "\n",
    "def run_gblup_proxy(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    GBLUP is mathematically equivalent to Ridge Regression (L2).\n",
    "    We use RidgeCV to automatically tune the regularization parameter (alpha).\n",
    "    This is 'Research Grade' because it optimizes hyperparams internally.\n",
    "    \"\"\"\n",
    "    # Alphas to search: logarithmic scale 0.1 to 1000\n",
    "    alphas = np.logspace(-1, 3, 10) \n",
    "    model = RidgeCV(alphas=alphas)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "def run_bayes_proxy(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    BayesianRidge is a robust proxy for BayesC/BayesB.\n",
    "    It infers precision parameters from data, handling 'large p, small n'.\n",
    "    \"\"\"\n",
    "    model = BayesianRidge()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "def run_sota_tabular(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    HistGradientBoosting is Scikit-Learn's implementation of LightGBM.\n",
    "    It handles non-linearities and epistasis better than linear models.\n",
    "    \"\"\"\n",
    "    model = HistGradientBoostingRegressor(\n",
    "        max_iter=100, \n",
    "        max_depth=5, \n",
    "        learning_rate=0.1, \n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "# ==========================================\n",
    "# 3. BENCHMARKING ENGINE\n",
    "# ==========================================\n",
    "\n",
    "def run_benchmarks():\n",
    "    print(\"\\n\" + \"#\"*60)\n",
    "    print(\"RUNNING BENCHMARKING (GBLUP, Bayes, SOTA-Tree)\")\n",
    "    print(\"#\"*60)\n",
    "\n",
    "    # Models to evaluate\n",
    "    benchmark_models = {\n",
    "        \"GBLUP\": run_gblup_proxy,\n",
    "        \"BayesB\": run_bayes_proxy,\n",
    "        \"LightGBM\": run_sota_tabular\n",
    "    }\n",
    "\n",
    "    # Configuration (Reusing your species_config and optimal counts)\n",
    "    # Ensure these variables exist in your notebook context\n",
    "    global species_config, species_optimal_counts\n",
    "    \n",
    "    for species, optimal_count in species_optimal_counts.items():\n",
    "        config = species_config[species]\n",
    "        sel_num = optimal_count\n",
    "        T_folder = f\"T{optimal_count}\"\n",
    "        \n",
    "        # We iterate over models first to save organized JSONs\n",
    "        for model_name, model_func in benchmark_models.items():\n",
    "            print(f\"\\n   >>> Benchmarking Model: {model_name} | Species: {species} | Feature Count: {T_folder}\")\n",
    "            \n",
    "            results_data = {\n",
    "                \"species\": species,\n",
    "                \"feature_count\": optimal_count,\n",
    "                \"model\": model_name,\n",
    "                \"mean_corrs\": {},\n",
    "                \"mean_RMSE\": {},\n",
    "                \"mean_R2\": {}\n",
    "            }\n",
    "\n",
    "            for trait_idx, trait_name in enumerate(config[\"traits\"]):\n",
    "                data_path = f\"./EN/{species}/{T_folder}{trait_name}\"\n",
    "                \n",
    "                # Check if feature selection file exists\n",
    "                if not os.path.exists(f\"{data_path}0.csv\"):\n",
    "                    print(f\"      [Skip] Trait {trait_name}: Feature file not found.\")\n",
    "                    continue\n",
    "\n",
    "                corrs, rmses, r2s = [], [], []\n",
    "                print(f\"      Processing Trait: {trait_name}...\", end=\"\")\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                for seed in range(5):\n",
    "                    try:\n",
    "                        # 1. Load Data\n",
    "                        loader = BenchmarkDataLoader(\n",
    "                            data_path, config[\"label_path\"], config[\"geno_path\"], \n",
    "                            trait_idx, seed, sel_num\n",
    "                        )\n",
    "                        X_train, X_test, y_train, y_test = loader.get_data()\n",
    "                        \n",
    "                        # 2. Train & Predict\n",
    "                        preds = model_func(X_train, y_train, X_test)\n",
    "                        \n",
    "                        # 3. Evaluate\n",
    "                        # Pearson returns (corr, p-value), we take [0]\n",
    "                        p_corr = pearsonr(y_test, preds)[0]\n",
    "                        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "                        r2 = r2_score(y_test, preds)\n",
    "                        \n",
    "                        corrs.append(p_corr)\n",
    "                        rmses.append(rmse)\n",
    "                        r2s.append(r2)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"\\n         [Error] Seed {seed}: {e}\")\n",
    "                \n",
    "                # Aggregate results\n",
    "                if corrs:\n",
    "                    mean_r2 = float(np.mean(r2s))\n",
    "                    mean_corr = float(np.mean(corrs))\n",
    "                    results_data[\"mean_corrs\"][trait_name] = mean_corr\n",
    "                    results_data[\"mean_RMSE\"][trait_name] = float(np.mean(rmses))\n",
    "                    results_data[\"mean_R2\"][trait_name] = mean_r2\n",
    "                    \n",
    "                    print(f\" Done ({time.time()-start_time:.1f}s) | Avg R2: {mean_r2:.4f} | Avg Corr: {mean_corr:.4f}\")\n",
    "\n",
    "            # Save results to JSON\n",
    "            json_filename = f\"{species}_{T_folder}_{model_name}.json\"\n",
    "            with open(json_filename, \"w\") as f:\n",
    "                json.dump(results_data, f, indent=4)\n",
    "            print(f\"   [Saved] {json_filename}\")\n",
    "\n",
    "# Run the benchmark\n",
    "if __name__ == \"__main__\":\n",
    "    run_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b8087ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== Loading Data for RICE ===============\n",
      "   Phenotype loaded: (413, 35)\n",
      "   Applying Rice ID fix...\n",
      "   Genotype loaded (Transposed): (413, 5000)\n",
      "   Matched Samples: 413\n",
      "   [Warning] No valid traits found to plot for rice\n",
      "\n",
      "--- Analyzing Selection Files for RICE ---\n",
      "Trait           File    Max_Score  Non_Zero_Markers                                          Top_Markers\n",
      "   SW   T5000SW0.csv 26886.431578              5353  idx:1055(26886); idx:27530(26173); idx:29488(23325)\n",
      "  FLW  T5000FLW0.csv 27553.540050              5418 idx:29488(27554); idx:25613(27361); idx:16868(24507)\n",
      "   AC   T5000AC1.csv 22321.722834              5640  idx:20657(22322); idx:23796(14764); idx:8921(13020)\n",
      "   PH   T5000PH4.csv 18873.765509              5429   idx:670(18874); idx:10340(18236); idx:12061(17900)\n",
      " SNPP T5000SNPP2.csv 46072.571731              5472 idx:16868(46073); idx:16172(34201); idx:30309(27803)\n",
      "   [Saved] Manhattan plots for rice traits.\n",
      "\n",
      "=============== Loading Data for SORGHUM ===============\n",
      "   Phenotype loaded: (451, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41973/2441041819.py:98: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  geno = pd.read_csv(config['geno_path'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Genotype loaded (Transposed): (451, 56299)\n",
      "   Matched Samples: 451\n",
      "   [Saved] Distribution plot: sorghum_pheno_dist.png\n",
      "\n",
      "--- Analyzing Selection Files for SORGHUM ---\n",
      "Trait          File    Max_Score  Non_Zero_Markers                                    Top_Markers\n",
      "   HT  T5000HT1.csv 11166.573731              5374  idx:nan(11167); idx:nan(10293); idx:nan(9629)\n",
      "   MO  T5000MO1.csv 11999.699549              5227 idx:nan(12000); idx:nan(11389); idx:nan(11361)\n",
      "  YLD T5000YLD0.csv 17536.456323              5369 idx:nan(17536); idx:nan(15555); idx:nan(15093)\n",
      "   [Saved] Manhattan plots for sorghum traits.\n",
      "\n",
      "=============== Loading Data for BULLS ===============\n",
      "   Phenotype loaded: (1508, 3)\n",
      "   Genotype loaded (Transposed): (1508, 9000)\n",
      "   Matched Samples: 1508\n",
      "   [Saved] Distribution plot: bulls_pheno_dist.png\n",
      "\n",
      "--- Analyzing Selection Files for BULLS ---\n",
      "Trait           File    Max_Score  Non_Zero_Markers                                    Top_Markers\n",
      "   MS   T5000MS1.csv 18887.730782              5386 idx:nan(18888); idx:nan(17107); idx:nan(16473)\n",
      " NMSP T5000NMSP0.csv 27498.878246              5432 idx:nan(27499); idx:nan(19416); idx:nan(15442)\n",
      "   VE   T5000VE2.csv 22260.664549              5278 idx:nan(22261); idx:nan(21571); idx:nan(20112)\n",
      "   [Saved] Manhattan plots for bulls traits.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# --- Configuration ---\n",
    "species_config = {\n",
    "    \"rice\": {\n",
    "        \"label_path\": \"./data/rice_pheno.csv\",\n",
    "        \"geno_path\": \"./data/ricerawgeno.csv\",\n",
    "        \"traits\": ['SW', 'FLW', 'AC', 'PH', 'SNPP'],\n",
    "        \"id_col_pheno\": \"NSFTVID\",\n",
    "        \"fix_rice_ids\": True\n",
    "    },\n",
    "    \"sorghum\": {\n",
    "        \"label_path\": \"./data/sorghum_pheno.csv\",\n",
    "        \"geno_path\": \"./data/sorghumrawgeno.csv\",\n",
    "        \"traits\": ['HT', 'MO', 'YLD'],\n",
    "        \"id_col_pheno\": \"ID\",\n",
    "        \"fix_rice_ids\": False\n",
    "    },\n",
    "    \"bulls\": {\n",
    "        \"label_path\": \"./data/bulls_pheno.csv\",\n",
    "        \"geno_path\": \"./data/bullsrawgeno.csv\",\n",
    "        \"traits\": ['MS', 'NMSP', 'VE'],\n",
    "        \"id_col_pheno\": \"ID\",\n",
    "        \"fix_rice_ids\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Optimal Feature Counts\n",
    "species_optimal_counts = {\n",
    "    \"rice\": 5000,\n",
    "    \"sorghum\": 5000,\n",
    "    \"bulls\": 5000\n",
    "}\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def clean_rice_ids(geno_id):\n",
    "    \"\"\"Specific cleaner for Rice Genotype IDs (removes suffixes).\"\"\"\n",
    "    if isinstance(geno_id, str) and '_' in geno_id:\n",
    "        return geno_id.rsplit('_', 1)[0]\n",
    "    return geno_id\n",
    "\n",
    "def plot_distributions(species, df, traits):\n",
    "    \"\"\"Plots histograms for the traits defined in config.\"\"\"\n",
    "    valid_traits = [t for t in traits if t in df.columns]\n",
    "    \n",
    "    if not valid_traits:\n",
    "        print(f\"   [Warning] No valid traits found to plot for {species}\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    for i, trait in enumerate(valid_traits):\n",
    "        plt.subplot(1, len(valid_traits), i+1)\n",
    "        sns.histplot(df[trait].dropna(), kde=True)\n",
    "        plt.title(f\"{species}: {trait}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    out_path = f\"{species}_pheno_dist.png\"\n",
    "    plt.savefig(out_path)\n",
    "    print(f\"   [Saved] Distribution plot: {out_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def load_and_align_data(species, config):\n",
    "    print(f\"\\n{'='*15} Loading Data for {species.upper()} {'='*15}\")\n",
    "    \n",
    "    # 1. Load Phenotype\n",
    "    if not os.path.exists(config['label_path']):\n",
    "        print(f\"[Missing] Phenotype file not found: {config['label_path']}\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        pheno = pd.read_csv(config['label_path'])\n",
    "        # Ensure ID is string and strip whitespace\n",
    "        pheno[config['id_col_pheno']] = pheno[config['id_col_pheno']].astype(str).str.strip()\n",
    "        pheno.set_index(config['id_col_pheno'], inplace=True)\n",
    "        print(f\"   Phenotype loaded: {pheno.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   [Error] Loading phenotype: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # 2. Load Genotype\n",
    "    if not os.path.exists(config['geno_path']):\n",
    "        print(f\"[Missing] Genotype file not found: {config['geno_path']}\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        # Read header only first to identify columns\n",
    "        geno_preview = pd.read_csv(config['geno_path'], nrows=1)\n",
    "        meta_cols = [c for c in geno_preview.columns if c.lower() in ['chrom', 'pos', 'chromosome', 'position']]\n",
    "        sample_cols = [c for c in geno_preview.columns if c not in meta_cols]\n",
    "        \n",
    "        # Load full file\n",
    "        geno = pd.read_csv(config['geno_path'])\n",
    "        geno_samples = geno[sample_cols]\n",
    "\n",
    "        # Apply Rice Fix if needed\n",
    "        if config['fix_rice_ids']:\n",
    "            print(\"   Applying Rice ID fix...\")\n",
    "            new_names = {c: clean_rice_ids(c) for c in sample_cols}\n",
    "            geno_samples = geno_samples.rename(columns=new_names)\n",
    "\n",
    "        # Transpose\n",
    "        geno_t = geno_samples.T\n",
    "        geno_t.index = geno_t.index.astype(str).str.strip()\n",
    "        print(f\"   Genotype loaded (Transposed): {geno_t.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   [Error] Loading genotype: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # 3. Align\n",
    "    common = geno_t.index.intersection(pheno.index)\n",
    "    \n",
    "    # --- DEBUG INFO ---\n",
    "    if len(common) == 0:\n",
    "        print(f\"\\n   !!! [DEBUG: ID Mismatch Check] !!!\")\n",
    "        print(f\"   First 3 Genotype IDs: {list(geno_t.index[:3])}\")\n",
    "        print(f\"   First 3 Phenotype IDs: {list(pheno.index[:3])}\")\n",
    "        print(f\"   [Warning] No matching IDs found! Skipping plots.\\n\")\n",
    "        return None, None\n",
    "    else:\n",
    "        print(f\"   Matched Samples: {len(common)}\")\n",
    "        \n",
    "    pheno_aligned = pheno.loc[common]\n",
    "    \n",
    "    # Plot Trait Distributions\n",
    "    plot_distributions(species, pheno_aligned, config['traits'])\n",
    "    \n",
    "    return pheno_aligned, geno_t.loc[common]\n",
    "\n",
    "def analyze_feature_selection(species_name, config, optimal_count):\n",
    "    \"\"\"\n",
    "    Analyzes the T* CSV files in ./EN/{species}/\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Analyzing Selection Files for {species_name.upper()} ---\")\n",
    "    \n",
    "    base_dir = f\"./EN/{species_name}/\"\n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"   [Skip] Directory not found: {base_dir}\")\n",
    "        return\n",
    "\n",
    "    summary_stats = []\n",
    "    \n",
    "    for trait in config['traits']:\n",
    "        # Pattern: ./EN/rice/T5000SW*.csv\n",
    "        file_pattern = os.path.join(base_dir, f\"T{optimal_count}{trait}*.csv\")\n",
    "        files = glob.glob(file_pattern)\n",
    "        \n",
    "        if not files:\n",
    "            print(f\"   No files found for trait {trait}\")\n",
    "            continue\n",
    "            \n",
    "        # Analyze the first file found\n",
    "        for filepath in files[:1]: \n",
    "            try:\n",
    "                df = pd.read_csv(filepath)\n",
    "                \n",
    "                # --- FIX: Ensure columns are numeric ---\n",
    "                df['cs'] = pd.to_numeric(df['cs'], errors='coerce').fillna(0)\n",
    "                df['index'] = pd.to_numeric(df['index'], errors='coerce')\n",
    "                \n",
    "                # Basic Stats\n",
    "                max_score = df['cs'].max()\n",
    "                non_zero = (df['cs'] > 0).sum()\n",
    "                \n",
    "                # Top hits\n",
    "                top = df.sort_values('cs', ascending=False).head(3)\n",
    "                \n",
    "                # --- FIX: explicit float cast in f-string ---\n",
    "                top_str = \"; \".join([\n",
    "                    f\"idx:{float(r['index']):.0f}({float(r['cs']):.0f})\" \n",
    "                    for _, r in top.iterrows()\n",
    "                ])\n",
    "                \n",
    "                summary_stats.append({\n",
    "                    'Trait': trait,\n",
    "                    'File': os.path.basename(filepath),\n",
    "                    'Max_Score': max_score,\n",
    "                    'Non_Zero_Markers': non_zero,\n",
    "                    'Top_Markers': top_str\n",
    "                })\n",
    "                \n",
    "                # Manhattan Plot\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.scatter(df['index'], df['cs'], s=10, alpha=0.6)\n",
    "                plt.title(f\"Feature Scores: {species_name} - {trait}\")\n",
    "                plt.xlabel(\"SNP Index\")\n",
    "                plt.ylabel(\"Score\")\n",
    "                plt.savefig(f\"{species_name}_{trait}_manhattan.png\")\n",
    "                plt.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   Error reading {filepath}: {e}\")\n",
    "\n",
    "    if summary_stats:\n",
    "        print(pd.DataFrame(summary_stats).to_string(index=False))\n",
    "        print(f\"   [Saved] Manhattan plots for {species_name} traits.\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "def main():\n",
    "    if not os.path.exists(\"./data\"): \n",
    "        os.makedirs(\"./data\")\n",
    "        print(\"Created ./data folder (place your CSVs here)\")\n",
    "    \n",
    "    for species, config in species_config.items():\n",
    "        # 1. Check Data Alignment\n",
    "        load_and_align_data(species, config)\n",
    "        \n",
    "        # 2. Analyze Feature Selection Outputs\n",
    "        if species in species_optimal_counts:\n",
    "            analyze_feature_selection(species, config, species_optimal_counts[species])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01403f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Benchmark for T5000...\n",
      "------------------------------------------------------------\n",
      "Processing Species: RICE\n",
      "  > Trait: FLW\n",
      "    Results for FLW (Avg over 5 seeds):\n",
      "      GBLUP     : 0.0000\n",
      "      BayesB    : 0.0000\n",
      "      XGBoost   : 0.0000\n",
      "      KNN       : 0.0000\n",
      "  > Trait: PH\n",
      "    Results for PH (Avg over 5 seeds):\n",
      "      GBLUP     : 0.0000\n",
      "      BayesB    : 0.0000\n",
      "      XGBoost   : 0.0000\n",
      "      KNN       : 0.0000\n",
      "  > Trait: SNPP\n",
      "    Results for SNPP (Avg over 5 seeds):\n",
      "      GBLUP     : 0.0000\n",
      "      BayesB    : 0.0000\n",
      "      XGBoost   : 0.0000\n",
      "      KNN       : 0.0000\n",
      "  > Trait: SW\n",
      "    Results for SW (Avg over 5 seeds):\n",
      "      GBLUP     : 0.0000\n",
      "      BayesB    : 0.0000\n",
      "      XGBoost   : 0.0000\n",
      "      KNN       : 0.0000\n",
      "  > Trait: AC\n",
      "    Results for AC (Avg over 5 seeds):\n",
      "      GBLUP     : 0.0000\n",
      "      BayesB    : 0.0000\n",
      "      XGBoost   : 0.0000\n",
      "      KNN       : 0.0000\n",
      "------------------------------------------------------------\n",
      "Processing Species: SORGHUM\n",
      "  > Trait: HT\n",
      "    Results for HT (Avg over 5 seeds):\n",
      "      GBLUP     : 0.0105\n",
      "      BayesB    : 0.0000\n",
      "      XGBoost   : -0.0133\n",
      "      KNN       : 0.0000\n",
      "  > Trait: YLD\n",
      "    Results for YLD (Avg over 5 seeds):\n",
      "      GBLUP     : 0.0057\n",
      "      BayesB    : 0.0000\n",
      "      XGBoost   : 0.0074\n",
      "      KNN       : 0.0000\n",
      "  > Trait: MO\n",
      "    Results for MO (Avg over 5 seeds):\n",
      "      GBLUP     : 0.0120\n",
      "      BayesB    : 0.0000\n",
      "      XGBoost   : -0.0059\n",
      "      KNN       : 0.0000\n",
      "------------------------------------------------------------\n",
      "Processing Species: BULLS\n",
      "  > Trait: MS\n",
      "    Results for MS (Avg over 5 seeds):\n",
      "      GBLUP     : 0.0306\n",
      "      BayesB    : 0.0000\n",
      "      XGBoost   : 0.0112\n",
      "      KNN       : 0.0000\n",
      "  > Trait: VE\n",
      "    Results for VE (Avg over 5 seeds):\n",
      "      GBLUP     : 0.0296\n",
      "      BayesB    : 0.0000\n",
      "      XGBoost   : 0.0238\n",
      "      KNN       : 0.0000\n",
      "  > Trait: NMSP\n",
      "    Results for NMSP (Avg over 5 seeds):\n",
      "      GBLUP     : 0.0176\n",
      "      BayesB    : 0.0000\n",
      "      XGBoost   : 0.0051\n",
      "      KNN       : 0.0000\n",
      "------------------------------------------------------------\n",
      "\n",
      "Benchmark Complete. Saved to 'benchmark_results_proper.csv'\n"
     ]
    }
   ],
   "source": [
    "# [2.2/3] Genomic baseline benchmarks: GBLUP (GRM+KRR), BayesB-proxy (ARD), XGBoost, and KNN; cross-validated Pearson correlation and CSV export\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import ARDRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==========================================\n",
    "# 1. MODEL DEFINITIONS \n",
    "# ==========================================\n",
    "\n",
    "class GBLUP:\n",
    "    \"\"\"Genomic Best Linear Unbiased Prediction via Kernel Ridge.\"\"\"\n",
    "    def __init__(self):\n",
    "        # alpha=1.0 corresponds to lambda in ridge regression\n",
    "        self.model = KernelRidge(alpha=1.0, kernel='precomputed')\n",
    "        self.X_train = None\n",
    "\n",
    "    def compute_grm(self, X):\n",
    "        \"\"\"VanRaden Genomic Relationship Matrix\"\"\"\n",
    "        X = np.array(X)\n",
    "        p_freq = np.mean(X, axis=0) / 2\n",
    "        Z = X - 2 * p_freq\n",
    "        scale = 2 * np.sum(p_freq * (1 - p_freq))\n",
    "        # Add small jitter to scale to prevent division by zero\n",
    "        if scale == 0: scale = 1 \n",
    "        K = np.dot(Z, Z.T) / scale\n",
    "        return K\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        K_train = self.compute_grm(X)\n",
    "        self.model.fit(K_train, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # We must compute kernel between Test and Train samples\n",
    "        X_train_np = np.array(self.X_train)\n",
    "        X_test_np = np.array(X_test)\n",
    "        \n",
    "        p_freq = np.mean(X_train_np, axis=0) / 2\n",
    "        scale = 2 * np.sum(p_freq * (1 - p_freq))\n",
    "        if scale == 0: scale = 1\n",
    "        \n",
    "        Z_train = X_train_np - 2 * p_freq\n",
    "        Z_test = X_test_np - 2 * p_freq\n",
    "        \n",
    "        K_test = np.dot(Z_test, Z_train.T) / scale\n",
    "        return self.model.predict(K_test)\n",
    "\n",
    "class BayesB_Proxy:\n",
    "    \"\"\"Approximation of BayesB using ARD Regression (Sparsity inducing).\"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = ARDRegression(max_iter=300)\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "class XGBoostGenomic:\n",
    "    \"\"\"XGBoost optimized for dense SNP data.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = XGBRegressor(\n",
    "            n_estimators=300, max_depth=6, learning_rate=0.05, \n",
    "            subsample=0.8, n_jobs=-1, verbosity=0\n",
    "        )\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "class GenomicKNN:\n",
    "    def __init__(self):\n",
    "        self.model = KNeighborsRegressor(n_neighbors=20, metric='correlation')\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# ==========================================\n",
    "# 2. DATA LOADING LOGIC (Specific to your folder)\n",
    "# ==========================================\n",
    "\n",
    "TRAIT_MAP = {\n",
    "    'rice': ['FLW', 'PH', 'SNPP', 'SW', 'AC'],\n",
    "    'sorghum': ['HT', 'YLD', 'MO'],\n",
    "    # 'soybean': ['protein', 'Yield', 'SdWgt', 'Steartic', 'R8'],\n",
    "    'bulls': ['MS', 'VE', 'NMSP']\n",
    "}\n",
    "\n",
    "PHENO_COL_MAP = {\n",
    "    \"rice\": {\n",
    "        \"FLW\": \"Flag leaf width\",\n",
    "        \"PH\": \"Plant height\",\n",
    "        \"SNPP\": \"Seed number per panicle\",\n",
    "        \"SW\": \"Seed width\",\n",
    "        \"AC\": \"Amylose content\",\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_dataset(species, trait, num_snps, seed):\n",
    "    \"\"\"\n",
    "    Loads X from ./EN/{species}/T{num_snps}{trait}{seed}.csv\n",
    "    Loads y from ./data/{species}_pheno.csv\n",
    "    \"\"\"\n",
    "    # 1. Construct Path for Genotypes (X)\n",
    "    # Note: Using your file list logic (e.g., ./EN/rice/T5000FLW0.csv)\n",
    "    en_path = f\"./EN/{species}/T{num_snps}{trait}{seed}.csv\"\n",
    "    \n",
    "    if not os.path.exists(en_path):\n",
    "        # Fallback for capitalization mismatches often seen in folders\n",
    "        # (e.g. 'protein' vs 'Protein')\n",
    "        possible_files = glob.glob(f\"./EN/{species}/T{num_snps}*{seed}.csv\")\n",
    "        # Try to find case-insensitive match\n",
    "        match = next((f for f in possible_files if trait.lower() in f.lower()), None)\n",
    "        if match:\n",
    "            en_path = match\n",
    "        else:\n",
    "            print(f\"  [Warning] File not found: {en_path}\")\n",
    "            return None, None\n",
    "\n",
    "    # Load Genotypes (Assumed to be purely numeric, no headers or simple headers)\n",
    "    # Your previous code implies these are just SNP matrices.\n",
    "    try:\n",
    "        X = pd.read_csv(en_path)\n",
    "        # Drop non-numeric columns if they sneak in (like IDs)\n",
    "        X = X.select_dtypes(include=[np.number]).values\n",
    "    except Exception as e:\n",
    "        print(f\"  [Error] Could not read X: {e}\")\n",
    "        return None, None\n",
    "    # 2. Load Phenotypes (y)\n",
    "    pheno_path = f\"./data/{species}_pheno.csv\"\n",
    "    if not os.path.exists(pheno_path):  # optional: helps when using attachments in this environment\n",
    "        pheno_path = f\"/mnt/data/{species}_pheno.csv\"\n",
    "\n",
    "    try:\n",
    "        df_pheno = pd.read_csv(pheno_path)\n",
    "\n",
    "        # ---- THE FIX: map rice short codes -> real column names ----\n",
    "        lookup_trait = PHENO_COL_MAP.get(species, {}).get(trait, trait)\n",
    "\n",
    "        cols = df_pheno.columns\n",
    "        target_col = next((c for c in cols if c.strip().lower() == lookup_trait.strip().lower()), None)\n",
    "\n",
    "        if target_col is None:\n",
    "            print(f\"  [Error] Trait '{trait}' not found in {pheno_path}\")\n",
    "            return None, None\n",
    "\n",
    "        y = df_pheno[target_col].values\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [Error] Could not read Phenotype: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # 3. Simple Alignment Check\n",
    "    # We assume X and y are already aligned by row index as is standard in these prep bundles\n",
    "    min_len = min(len(X), len(y))\n",
    "    return X[:min_len], y[:min_len]\n",
    "\n",
    "# ==========================================\n",
    "# 3. MAIN BENCHMARK LOOP\n",
    "# ==========================================\n",
    "\n",
    "def run_benchmarks():\n",
    "    # Only run for T5000 for now to save time, as indicated by your output folder focus\n",
    "    NUM_SNPS = 5000 \n",
    "    \n",
    "    # Define models\n",
    "    models = {\n",
    "        \"GBLUP\": GBLUP(),\n",
    "        \"BayesB\": BayesB_Proxy(),\n",
    "        \"XGBoost\": XGBoostGenomic(),\n",
    "        \"KNN\": GenomicKNN()\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    print(f\"Starting Benchmark for T{NUM_SNPS}...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for species, traits in TRAIT_MAP.items():\n",
    "        print(f\"Processing Species: {species.upper()}\")\n",
    "        \n",
    "        for trait in traits:\n",
    "            print(f\"  > Trait: {trait}\")\n",
    "            \n",
    "            trait_corrs = {m: [] for m in models}\n",
    "            \n",
    "            # Loop through Seeds 0 to 4\n",
    "            for seed in range(5):\n",
    "                X, y = load_dataset(species, trait, NUM_SNPS, seed)\n",
    "                \n",
    "                if X is None:\n",
    "                    continue\n",
    "\n",
    "                # 5-Fold CV on this specific Seed's dataset\n",
    "                # Note: Usually \"Seed 0\" file implies a specific split, \n",
    "                # but we will run a quick CV to get robust metrics.\n",
    "                kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "                \n",
    "                # We aggregate predictions for this seed to calculate one correlation\n",
    "                # Or average correlations across folds. Let's average folds.\n",
    "                \n",
    "                for name, model in models.items():\n",
    "                    fold_scores = []\n",
    "                    \n",
    "                    for train_idx, val_idx in kf.split(X):\n",
    "                        X_train, X_val = X[train_idx], X[val_idx]\n",
    "                        y_train, y_val = y[train_idx], y[val_idx]\n",
    "                        \n",
    "                        # Scale Y\n",
    "                        scaler = StandardScaler()\n",
    "                        y_train = scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "                        \n",
    "                        try:\n",
    "                            model.fit(X_train, y_train)\n",
    "                            preds = model.predict(X_val)\n",
    "                            \n",
    "                            # Inverse scale preds for fair comparison (optional, but good for MSE)\n",
    "                            # For Pearson Corr, scaling doesn't matter.\n",
    "                            \n",
    "                            # Pearson Correlation\n",
    "                            if len(np.unique(preds)) > 1: # Avoid constant output errors\n",
    "                                corr, _ = pearsonr(y_val, preds)\n",
    "                                fold_scores.append(corr)\n",
    "                            else:\n",
    "                                fold_scores.append(0.0)\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            # print(f\"    [Error] {name} failed: {e}\")\n",
    "                            fold_scores.append(0.0)\n",
    "                    \n",
    "                    avg_fold_score = np.mean(fold_scores)\n",
    "                    trait_corrs[name].append(avg_fold_score)\n",
    "\n",
    "            # Summarize Trait Results\n",
    "            print(f\"    Results for {trait} (Avg over 5 seeds):\")\n",
    "            for name in models:\n",
    "                final_score = np.mean(trait_corrs[name])\n",
    "                print(f\"      {name:<10}: {final_score:.4f}\")\n",
    "                \n",
    "                # Save to results list for CSV export\n",
    "                results.append({\n",
    "                    \"Species\": species,\n",
    "                    \"Trait\": trait,\n",
    "                    \"Model\": name,\n",
    "                    \"Pearson_Corr\": final_score\n",
    "                })\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    # Export\n",
    "    df_res = pd.DataFrame(results)\n",
    "    df_res.to_csv(\"benchmark_results_proper.csv\", index=False)\n",
    "    print(\"\\nBenchmark Complete. Saved to 'benchmark_results_proper.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e6527fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "# Assuming EBMGP, DNADataset, setup_seed, train_epoch, evaluate_epoch, PearsonCorrCoef\n",
    "# are defined in previous cells (Cell 1-3).\n",
    "\n",
    "def train_and_evaluate(trait_idx, species, data_path, label_path, geno_path, device,\n",
    "                       learning_rate, epochs, seed, sel_num, ld_threshold=0.8, pooling_type=\"MAP\"):\n",
    "    \"\"\"\n",
    "    Standard training & evaluation loop.\n",
    "    Returns: (mean_pearson_corr, history_dict, final_predictions, final_targets)\n",
    "    \"\"\"\n",
    "    setup_seed(seed)\n",
    "\n",
    "    # 1. Create Datasets & Loaders\n",
    "    traindataset = DNADataset(data_path, label_path, geno_path, trait_idx, seed, sel_num, ld_threshold, is_training=True)\n",
    "    testdataset = DNADataset(data_path, label_path, geno_path, trait_idx, seed, sel_num, ld_threshold, is_training=False)\n",
    "\n",
    "    bs = 32\n",
    "    train_loader = DataLoader(traindataset, batch_size=bs, shuffle=True)\n",
    "    # Important: Shuffle=False for test to keep order for scatter plots\n",
    "    test_loader = DataLoader(testdataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "    # 2. Model, Optimizer, Scheduler, Loss\n",
    "    model = EBMGP(pooling_type=pooling_type).to(device)\n",
    "    \n",
    "    # Hyperparameters from paper\n",
    "    lr = learning_rate\n",
    "    weight_decay = 1e-5\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Cosine Annealing Scheduler\n",
    "    steps = math.ceil(len(traindataset) / bs) * epochs - 1\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=steps)\n",
    "\n",
    "    loss_fn = nn.L1Loss() # MAE Loss\n",
    "    pearson = PearsonCorrCoef().to(device)\n",
    "\n",
    "    # Data capture containers\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    best_test_corr = -1.0\n",
    "    final_preds = []\n",
    "    final_targets = []\n",
    "\n",
    "    # 3. Training Loop\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Train\n",
    "        t_loss = train_epoch(model, train_loader, optimizer, loss_fn, scheduler, device)\n",
    "        history['train_loss'].append(t_loss)\n",
    "        \n",
    "        # Validate\n",
    "        v_loss, vp, vt = evaluate_epoch(model, test_loader, loss_fn, pearson, device)\n",
    "        history['val_loss'].append(v_loss)\n",
    "        test_corr = pearson(vp, vt).item()\n",
    "        \n",
    "        # Save predictions from the final epoch\n",
    "        if epoch == epochs:\n",
    "            final_preds = vp.cpu().numpy().flatten()\n",
    "            final_targets = vt.cpu().numpy().flatten()\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs:\n",
    "            print(f\"   Epoch {epoch}/{epochs} | Train Loss: {t_loss:.4f} | Val Loss: {v_loss:.4f} | Val Corr: {test_corr:.4f}\")\n",
    "\n",
    "    # Final Evaluation on Test Set\n",
    "    mean_corr = pearson(vp, vt).item()\n",
    "    print(f\"Finished. Final Test Correlation: {mean_corr:.4f}\")\n",
    "    \n",
    "    return mean_corr, history, final_preds, final_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1495389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating LD Ablation Curves...\n",
      "Saved: LD_Ablation_Curves.png\n",
      "\n",
      "Generating Benchmark Heatmap...\n",
      "Saved: Benchmark_Heatmap.png\n",
      "\n",
      "Generating Pooling Comparison...\n",
      "Saved: Pooling_Comparison_Bar.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1308.12x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1537.87x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Set global style\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "def load_results_from_json(pattern):\n",
    "    \"\"\"Helper to load all JSONs matching a pattern into a DataFrame\"\"\"\n",
    "    data = []\n",
    "    files = glob.glob(pattern)\n",
    "    for f in files:\n",
    "        with open(f, 'r') as file:\n",
    "            content = json.load(file)\n",
    "            # Flatten the 'mean_corrs' dict into rows\n",
    "            for trait, score in content.get('mean_corrs', {}).items():\n",
    "                row = content.copy()\n",
    "                del row['mean_corrs'] # remove dict\n",
    "                if 'mean_RMSE' in row: del row['mean_RMSE']\n",
    "                row['Trait'] = trait\n",
    "                row['Pearson_Corr'] = score\n",
    "                data.append(row)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. LD Ablation Curves (Line Plot)\n",
    "# ---------------------------------------------------------\n",
    "print(\"Generating LD Ablation Curves...\")\n",
    "df_ld = load_results_from_json(\"*_LD*.json\")\n",
    "\n",
    "if not df_ld.empty:\n",
    "    # Filter for MAP pooling if mixed\n",
    "    if 'pooling_type' in df_ld.columns:\n",
    "        df_ld = df_ld[df_ld['pooling_type'] == 'MAP']\n",
    "    \n",
    "    g = sns.relplot(\n",
    "        data=df_ld, \n",
    "        x=\"ld_threshold\", y=\"Pearson_Corr\", \n",
    "        hue=\"Trait\", col=\"species\", \n",
    "        kind=\"line\", marker=\"o\", linewidth=2.5,\n",
    "        height=4, aspect=1, facet_kws={'sharey': False}\n",
    "    )\n",
    "    g.set_titles(\"{col_name}\")\n",
    "    g.set_axis_labels(\"LD Threshold ()\", \"Pearson Correlation\")\n",
    "    plt.savefig(\"LD_Ablation_Curves.png\", dpi=300, bbox_inches='tight')\n",
    "    print(\"Saved: LD_Ablation_Curves.png\")\n",
    "else:\n",
    "    print(\"No LD ablation JSON files found.\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Benchmark Heatmap (Method x Trait)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nGenerating Benchmark Heatmap...\")\n",
    "# Load EBMGP results (assuming LD0.8 MAP is the standard EBMGP)\n",
    "df_ebmgp = load_results_from_json(\"*_LD0.8.json\")\n",
    "if not df_ebmgp.empty:\n",
    "    df_ebmgp['Model'] = 'EBMGP'\n",
    "    df_ebmgp = df_ebmgp[['species', 'Trait', 'Model', 'Pearson_Corr']]\n",
    "\n",
    "# Load Benchmark results\n",
    "df_bench = load_results_from_json(\"*_GBLUP.json\") \n",
    "df_bench = pd.concat([df_bench, load_results_from_json(\"*_BayesB.json\")])\n",
    "df_bench = pd.concat([df_bench, load_results_from_json(\"*_LightGBM.json\")])\n",
    "\n",
    "if not df_bench.empty and not df_ebmgp.empty:\n",
    "    df_final = pd.concat([df_ebmgp, df_bench], ignore_index=True)\n",
    "    \n",
    "    # Create a pivot table for the heatmap\n",
    "    # Pivot: Index=Trait(Species), Columns=Model, Values=Corr\n",
    "    df_final['Label'] = df_final['species'].str.capitalize() + \" - \" + df_final['Trait']\n",
    "    pivot_df = df_final.pivot_table(index='Label', columns='Model', values='Pearson_Corr')\n",
    "    \n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.heatmap(pivot_df, annot=True, cmap=\"viridis\", fmt=\".3f\", linewidths=.5)\n",
    "    plt.title(\"Pearson Correlation: Methods vs Traits\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Benchmark_Heatmap.png\", dpi=300)\n",
    "    print(\"Saved: Benchmark_Heatmap.png\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Pooling Comparison (Bar Chart)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nGenerating Pooling Comparison...\")\n",
    "# We look for files that look like species_T5000_TYPE.json where TYPE is MAP, AVG, etc.\n",
    "# Note: Reuse logic from run_pooling_ablation naming convention\n",
    "df_pool = pd.DataFrame()\n",
    "for p_type in [\"MAP\", \"AVG\", \"MAX\", \"LIP\"]:\n",
    "    temp = load_results_from_json(f\"*_{p_type}.json\")\n",
    "    temp['Pooling'] = p_type\n",
    "    df_pool = pd.concat([df_pool, temp])\n",
    "\n",
    "if not df_pool.empty:\n",
    "    g = sns.catplot(\n",
    "        data=df_pool, \n",
    "        x=\"Trait\", y=\"Pearson_Corr\", hue=\"Pooling\", col=\"species\",\n",
    "        kind=\"bar\", height=4, aspect=1.2, sharey=False\n",
    "    )\n",
    "    g.set_axis_labels(\"Trait\", \"Pearson Correlation\")\n",
    "    g.set_titles(\"{col_name}\")\n",
    "    plt.savefig(\"Pooling_Comparison_Bar.png\", dpi=300, bbox_inches='tight')\n",
    "    print(\"Saved: Pooling_Comparison_Bar.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dded6061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Diagnostics for rice - SW...\n",
      "   Epoch 10/100 | Train Loss: 0.6724 | Val Loss: 0.6198 | Val Corr: 0.6238\n",
      "   Epoch 20/100 | Train Loss: 0.5759 | Val Loss: 0.5259 | Val Corr: 0.6747\n",
      "   Epoch 30/100 | Train Loss: 0.5152 | Val Loss: 0.5731 | Val Corr: 0.6571\n",
      "   Epoch 40/100 | Train Loss: 0.4526 | Val Loss: 0.5335 | Val Corr: 0.6465\n",
      "   Epoch 50/100 | Train Loss: 0.4180 | Val Loss: 0.5733 | Val Corr: 0.6150\n",
      "   Epoch 60/100 | Train Loss: 0.4071 | Val Loss: 0.6127 | Val Corr: 0.6118\n",
      "   Epoch 70/100 | Train Loss: 0.3965 | Val Loss: 0.5773 | Val Corr: 0.6114\n",
      "   Epoch 80/100 | Train Loss: 0.3845 | Val Loss: 0.5386 | Val Corr: 0.6174\n",
      "   Epoch 90/100 | Train Loss: 0.3982 | Val Loss: 0.5685 | Val Corr: 0.6264\n",
      "   Epoch 100/100 | Train Loss: 0.3809 | Val Loss: 0.5285 | Val Corr: 0.6238\n",
      "Finished. Final Test Correlation: 0.6238\n",
      "   Saved rice_SW_loss_curve.png\n",
      "   Saved rice_SW_scatter.png\n",
      "Generating Diagnostics for sorghum - YLD...\n",
      "   Epoch 10/100 | Train Loss: 0.5679 | Val Loss: 0.6775 | Val Corr: 0.5687\n",
      "   Epoch 20/100 | Train Loss: 0.4216 | Val Loss: 0.7140 | Val Corr: 0.5442\n",
      "   Epoch 30/100 | Train Loss: 0.3983 | Val Loss: 0.6743 | Val Corr: 0.5296\n",
      "   Epoch 40/100 | Train Loss: 0.3293 | Val Loss: 0.6640 | Val Corr: 0.5559\n",
      "   Epoch 50/100 | Train Loss: 0.3020 | Val Loss: 0.6669 | Val Corr: 0.5687\n",
      "   Epoch 60/100 | Train Loss: 0.3031 | Val Loss: 0.6513 | Val Corr: 0.5760\n",
      "   Epoch 70/100 | Train Loss: 0.2637 | Val Loss: 0.6533 | Val Corr: 0.5804\n",
      "   Epoch 80/100 | Train Loss: 0.2647 | Val Loss: 0.6523 | Val Corr: 0.5834\n",
      "   Epoch 90/100 | Train Loss: 0.2396 | Val Loss: 0.6537 | Val Corr: 0.5825\n",
      "   Epoch 100/100 | Train Loss: 0.2315 | Val Loss: 0.6549 | Val Corr: 0.5814\n",
      "Finished. Final Test Correlation: 0.5814\n",
      "   Saved sorghum_YLD_loss_curve.png\n",
      "   Saved sorghum_YLD_scatter.png\n",
      "Generating Diagnostics for bulls - NMSP...\n",
      "   Epoch 10/100 | Train Loss: 0.6480 | Val Loss: 0.6897 | Val Corr: 0.4562\n",
      "   Epoch 20/100 | Train Loss: 0.4948 | Val Loss: 0.6243 | Val Corr: 0.5258\n",
      "   Epoch 30/100 | Train Loss: 0.4237 | Val Loss: 0.5982 | Val Corr: 0.5482\n",
      "   Epoch 40/100 | Train Loss: 0.3814 | Val Loss: 0.6042 | Val Corr: 0.5467\n",
      "   Epoch 50/100 | Train Loss: 0.3391 | Val Loss: 0.5937 | Val Corr: 0.5729\n",
      "   Epoch 60/100 | Train Loss: 0.3260 | Val Loss: 0.6058 | Val Corr: 0.5445\n",
      "   Epoch 70/100 | Train Loss: 0.3019 | Val Loss: 0.5977 | Val Corr: 0.5442\n",
      "   Epoch 80/100 | Train Loss: 0.2942 | Val Loss: 0.6036 | Val Corr: 0.5436\n",
      "   Epoch 90/100 | Train Loss: 0.3027 | Val Loss: 0.6007 | Val Corr: 0.5458\n",
      "   Epoch 100/100 | Train Loss: 0.2935 | Val Loss: 0.6027 | Val Corr: 0.5464\n",
      "Finished. Final Test Correlation: 0.5464\n",
      "   Saved bulls_NMSP_loss_curve.png\n",
      "   Saved bulls_NMSP_scatter.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# --- FIX SECTION START ---\n",
    "# REDEFINED evaluate_epoch to handle batches with token_type_ids\n",
    "def evaluate_epoch(model, loader, loss_fn, metric_fn, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # Move all tensors in the batch to the device\n",
    "            batch = [t.to(device) for t in batch]\n",
    "            \n",
    "            # HANDLE BATCH UNPACKING\n",
    "            if len(batch) == 3:\n",
    "                # Case: [input_ids, token_type_ids, labels]\n",
    "                b_input_ids, b_token_types, b_labels = batch\n",
    "                outputs = model(b_input_ids, token_type_ids=b_token_types)\n",
    "                target = b_labels\n",
    "            elif len(batch) == 2:\n",
    "                # Case: [input_ids, labels] - Attempt to generate dummy types or pass single\n",
    "                b_input_ids, b_labels = batch\n",
    "                target = b_labels\n",
    "                try:\n",
    "                    outputs = model(b_input_ids)\n",
    "                except TypeError:\n",
    "                    # If model strictly demands token_type_ids but loader didn't give them,\n",
    "                    # generate zeros (common for single-sequence BERT models)\n",
    "                    b_token_types = torch.zeros_like(b_input_ids, dtype=torch.long)\n",
    "                    outputs = model(b_input_ids, token_type_ids=b_token_types)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected batch size: {len(batch)}\")\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, target)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Store raw tensors\n",
    "            all_preds.append(outputs)\n",
    "            all_targets.append(target)\n",
    "\n",
    "    # Concatenate lists into single Tensors immediately\n",
    "    if len(all_preds) > 0:\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_targets = torch.cat(all_targets, dim=0)\n",
    "    else:\n",
    "        all_preds = torch.tensor([], device=device)\n",
    "        all_targets = torch.tensor([], device=device)\n",
    "    \n",
    "    return running_loss / len(loader), all_preds, all_targets\n",
    "# --- FIX SECTION END ---\n",
    "\n",
    "\n",
    "# Configuration for the \"Hero\" plots\n",
    "hero_plots = [\n",
    "    {\"species\": \"rice\", \"trait\": \"SW\", \"seed\": 0},\n",
    "    {\"species\": \"sorghum\", \"trait\": \"YLD\", \"seed\": 0},\n",
    "    {\"species\": \"bulls\", \"trait\": \"NMSP\", \"seed\": 0}\n",
    "]\n",
    "\n",
    "for item in hero_plots:\n",
    "    sp = item['species']\n",
    "    tr = item['trait']\n",
    "    sd = item['seed']\n",
    "    \n",
    "    print(f\"Generating Diagnostics for {sp} - {tr}...\")\n",
    "    \n",
    "    t_folder = \"T5000\" \n",
    "    data_path = f\"./EN/{sp}/{t_folder}{tr}\"\n",
    "    \n",
    "    config = species_config[sp]\n",
    "    trait_idx = config['traits'].index(tr)\n",
    "    \n",
    "    if not os.path.exists(f\"{data_path}{sd}.csv\"):\n",
    "        print(f\"Skipping {sp}-{tr}: File {data_path}{sd}.csv not found.\")\n",
    "        continue\n",
    "\n",
    "    # RUN TRAINING\n",
    "    # This will now use the IMPROVED evaluate_epoch above\n",
    "    corr, history, preds, targets = train_and_evaluate(\n",
    "        trait_idx=trait_idx,\n",
    "        species=sp,\n",
    "        data_path=data_path,\n",
    "        label_path=config[\"label_path\"],\n",
    "        geno_path=config[\"geno_path\"],\n",
    "        device=device,\n",
    "        learning_rate=0.0005,\n",
    "        epochs=100,\n",
    "        seed=sd,\n",
    "        sel_num=5000,\n",
    "        ld_threshold=0.8,\n",
    "        pooling_type=\"MAP\"\n",
    "    )\n",
    "    \n",
    "    # --- PLOTTING DATA PREP ---\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        targets = targets.detach().cpu().numpy()\n",
    "    elif isinstance(preds, list):\n",
    "        preds = torch.cat(preds).detach().cpu().numpy()\n",
    "        targets = torch.cat(targets).detach().cpu().numpy()\n",
    "        \n",
    "    preds = preds.flatten()\n",
    "    targets = targets.flatten()\n",
    "    # --------------------------\n",
    "    \n",
    "    # 1. Plot Loss Curve\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title(f\"Training Convergence: {sp.capitalize()} {tr}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"L1 Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    loss_filename = f\"{sp}_{tr}_loss_curve.png\"\n",
    "    plt.savefig(loss_filename, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"   Saved {loss_filename}\")\n",
    "    \n",
    "    # 2. Plot Scatter (Pred vs Obs)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    \n",
    "    d_min = min(np.min(preds), np.min(targets))\n",
    "    d_max = max(np.max(preds), np.max(targets))\n",
    "    pad = (d_max - d_min) * 0.1\n",
    "    \n",
    "    plt.scatter(targets, preds, alpha=0.6, edgecolors='w', s=50)\n",
    "    plt.plot([d_min-pad, d_max+pad], [d_min-pad, d_max+pad], 'r--', label=\"Perfect Fit\")\n",
    "    \n",
    "    plt.title(f\"{sp.capitalize()} {tr} (r={corr:.3f})\")\n",
    "    plt.xlabel(\"Observed Phenotype (Standardized)\")\n",
    "    plt.ylabel(\"Predicted Phenotype\")\n",
    "    plt.xlim(d_min-pad, d_max+pad)\n",
    "    plt.ylim(d_min-pad, d_max+pad)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    scat_filename = f\"{sp}_{tr}_scatter.png\"\n",
    "    plt.savefig(scat_filename, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"   Saved {scat_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa9bbe11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Manhattan Plots...\n",
      "  Saved rice_AC_manhattan.png\n",
      "  Saved rice_FLW_manhattan.png\n",
      "  Saved rice_PH_manhattan.png\n",
      "  Saved rice_SNPP_manhattan.png\n",
      "  Saved rice_SW_manhattan.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the specific traits you listed in the LaTeX images section\n",
    "target_plots = [\n",
    "    (\"rice\", \"AC\"),\n",
    "    (\"rice\", \"FLW\"),\n",
    "    (\"rice\", \"PH\"),\n",
    "    (\"rice\", \"SNPP\"),\n",
    "    (\"rice\", \"SW\")\n",
    "]\n",
    "\n",
    "print(\"Generating Manhattan Plots...\")\n",
    "\n",
    "for species, trait in target_plots:\n",
    "    # Construct pattern to find the file (e.g., ./EN/rice/T5000AC0.csv)\n",
    "    # Using T5000 as default based on your text\n",
    "    file_pattern = f\"./EN/{species}/T5000{trait}0.csv\" \n",
    "    \n",
    "    files = glob.glob(file_pattern)\n",
    "    if not files:\n",
    "        print(f\"  [Warning] No file found for {species} - {trait}\")\n",
    "        continue\n",
    "        \n",
    "    # Use the first seed (0) found\n",
    "    filepath = files[0]\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        # 'index' is the SNP location, 'cs' is the Elastic Net absolute coefficient\n",
    "        plt.scatter(df['index'], df['cs'], c='navy', alpha=0.6, s=15, edgecolors='none')\n",
    "        \n",
    "        plt.title(f\"Feature Importance (Elastic Net): {species.capitalize()} - {trait}\")\n",
    "        plt.xlabel(\"SNP Index\")\n",
    "        plt.ylabel(\"Selection Score (Abs Coeff)\")\n",
    "        \n",
    "        # Add a baseline threshold line for visual context\n",
    "        plt.axhline(y=0, color='grey', linewidth=0.5)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        out_name = f\"{species}_{trait}_manhattan.png\"\n",
    "        plt.savefig(out_name, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"  Saved {out_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error plotting {species}-{trait}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7a668cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: figs/rice_phenotypes_grid.png\n",
      "Saved: figs/sorghum_phenotypes_grid.png\n",
      "Saved: figs/bulls_phenotypes_grid.png\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OUTDIR = Path(\"figs\")\n",
    "OUTDIR.mkdir(exist_ok=True)\n",
    "\n",
    "def plot_grid(name, df, cols, ncols=3, bins=30):\n",
    "    k = len(cols)\n",
    "    nrows = math.ceil(k / ncols)\n",
    "\n",
    "    plt.figure(figsize=(4*ncols, 3.2*nrows))\n",
    "    for i, col in enumerate(cols, 1):\n",
    "        ax = plt.subplot(nrows, ncols, i)\n",
    "        x = df[col].dropna()\n",
    "        ax.hist(x, bins=bins)\n",
    "        ax.set_title(col, fontsize=10)\n",
    "        ax.set_xlabel(col, fontsize=9)\n",
    "        ax.set_ylabel(\"count\", fontsize=9)\n",
    "\n",
    "    plt.suptitle(f\"{name}: phenotype distributions (targets)\", y=1.02, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    out = OUTDIR / f\"{name}_phenotypes_grid.png\"\n",
    "    plt.savefig(out, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved:\", out)\n",
    "\n",
    "# Load\n",
    "rice = pd.read_csv(\"./data/rice_pheno.csv\", index_col=0)\n",
    "sorghum = pd.read_csv(\"./data/sorghum_pheno.csv\", index_col=0)\n",
    "bulls = pd.read_csv(\"./data/bulls_pheno.csv\", index_col=0)\n",
    "\n",
    "# Choose target traits\n",
    "rice_targets = [\n",
    "    \"Seed width\",              # SW\n",
    "    \"Flag leaf width\",         # FLW\n",
    "    \"Plant height\",            # PH\n",
    "    \"Amylose content\",         # AC\n",
    "    \"Seed number per panicle\", # SNPP\n",
    "]\n",
    "sorghum_targets = [\"MO\", \"YLD\", \"HT\"]\n",
    "bulls_targets = [\"MS\", \"NMSP\", \"VE\"]\n",
    "\n",
    "plot_grid(\"rice\", rice, rice_targets, ncols=3)\n",
    "plot_grid(\"sorghum\", sorghum, sorghum_targets, ncols=3)\n",
    "plot_grid(\"bulls\", bulls, bulls_targets, ncols=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "554efa0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RICE COLUMNS ---\n",
      "['Flowering time at Arkansas', 'Flowering time at Faridpur', 'Flowering time at Aberdeen', 'FT ratio of Arkansas/Aberdeen', 'FT ratio of Faridpur/Aberdeen', 'Culm habit', 'Leaf pubescence', 'Flag leaf length', 'Flag leaf width', 'Awn presence', 'Panicle number per plant', 'Plant height', 'Panicle length', 'Primary panicle branch number', 'Seed number per panicle', 'Florets per panicle', 'Panicle fertility', 'Seed length', 'Seed width', 'Seed volume', 'Seed surface area', 'Brown rice seed length', 'Brown rice seed width', 'Brown rice surface area', 'Brown rice volume', 'Seed length/width ratio', 'Brown rice length/width ratio', 'Straighthead suseptability ', 'Blast resistance', 'Amylose content', 'Alkali spreading value', 'Protein content', 'Year07Flowering time at Arkansas', 'Year06Flowering time at Arkansas', 'Sub-population']\n",
      "\n",
      "\n",
      "--- SORGHUM COLUMNS ---\n",
      "['MO', 'YLD', 'HT']\n",
      "\n",
      "\n",
      "--- BULLS COLUMNS ---\n",
      "['MS', 'NMSP', 'VE']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datasets = {\n",
    "    \"rice\": \"./data/rice_pheno.csv\",\n",
    "    \"sorghum\": \"./data/sorghum_pheno.csv\",\n",
    "    \"bulls\": \"./data/bulls_pheno.csv\",\n",
    "}\n",
    "\n",
    "for name, path in datasets.items():\n",
    "    df = pd.read_csv(path, index_col=0, nrows=0) # nrows=0 reads only the header (very fast)\n",
    "    print(f\"--- {name.upper()} COLUMNS ---\")\n",
    "    print(df.columns.tolist())\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
