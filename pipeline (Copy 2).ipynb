{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d3c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2.2] Setup: import libraries used across dataset loading, DL model, training/eval, and benchmarks\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "from itertools import accumulate\n",
    "from typing import Optional, List, Tuple\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import PearsonCorrCoef\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from transformers import BertPreTrainedModel\n",
    "from transformers.modeling_utils import ModuleUtilsMixin\n",
    "from transformers.models.bert.modeling_bert import (\n",
    "    BertConfig, \n",
    "    BertEmbeddings, \n",
    "    BertEncoder, \n",
    "    BertLayer, \n",
    "    BertAttention\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7900a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2.1/2.3] Data pipeline: load EN-selected SNPs, compute LD-gap tokens, build sequences, standardize labels, and create KFold split-based PyTorch Dataset\n",
    "class DNADataset(Dataset):\n",
    "    # Updated __init__ to accept ld_threshold\n",
    "    def __init__(self, data_path, label_path, geno_path, trait, seed, sel_num, ld_threshold=0.8, is_training=True):\n",
    "        cs = pd.read_csv(f\"{data_path}{seed}.csv\").sort_values(by='cs', ascending=False)\n",
    "        Top = sorted(cs.index[:sel_num])  \n",
    "        Rawgeno = pd.read_csv(geno_path)\n",
    "\n",
    "        # DROP the first row (SNP index row)\n",
    "        Rawgeno = Rawgeno.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "        Top = [i for i in Top if i in Rawgeno.index]\n",
    "\n",
    "        geno = Rawgeno.loc[Top].copy()\n",
    "\n",
    "        # explicitly separate columns\n",
    "        geno_cols = [c for c in geno.columns if c not in ['chrom']]\n",
    "\n",
    "        LD = self.calculate_LD(geno)\n",
    "        \n",
    "        # Pass the variable threshold here\n",
    "        geno['gap'] = self.assign_gap_labels(LD, ld_threshold)\n",
    "        \n",
    "        geno = geno.drop(columns=geno.columns[[-3, -2]])  \n",
    "        lines = self.generate_geno_sequences(geno)\n",
    "        annos = pd.read_csv(label_path, index_col=0).iloc[:, [trait]]\n",
    "        annos = annos.fillna(annos.mean()) \n",
    "        annos = StandardScaler().fit_transform(annos).astype(np.float32)\n",
    "\n",
    "        kfold = KFold(n_splits=5, shuffle=True, random_state=27)\n",
    "        for i, (train_idx, val_idx) in enumerate(kfold.split(lines, annos)):\n",
    "            if i == seed:\n",
    "                train_lines, val_lines = lines[train_idx], lines[val_idx]\n",
    "                train_annos, val_annos = annos[train_idx], annos[val_idx]                \n",
    "                break\n",
    "\n",
    "        train_seqs, train_type_ids = self.process_sequences(train_lines)\n",
    "        val_seqs, val_type_ids = self.process_sequences(val_lines)\n",
    "\n",
    "        if is_training:\n",
    "            self.seqs, self.type_ids, self.annos = train_seqs, train_type_ids, train_annos\n",
    "        else:\n",
    "            self.seqs, self.type_ids, self.annos = val_seqs, val_type_ids, val_annos\n",
    "\n",
    "    def calculate_LD(self, geno):\n",
    "        geno_values = np.select(\n",
    "            [geno.iloc[:, :-2].values == 'H', geno.iloc[:, :-2].values == 'M', geno.iloc[:, :-2].values == 'L'],\n",
    "            [0, 1, 2],\n",
    "            default=-1\n",
    "        ) \n",
    "\n",
    "        a, b = geno_values[:-1], geno_values[1:]  \n",
    "        var_a, var_b = np.var(a, axis=1), np.var(b, axis=1)\n",
    "        mean_a, mean_b = np.mean(a, axis=1), np.mean(b, axis=1)\n",
    "        d = np.mean((a - mean_a[:, None]) * (b - mean_b[:, None]), axis=1)\n",
    "        \n",
    "        LD = np.where((var_a == 0) | (var_b == 0), 0, (d ** 2) / (var_a * var_b))\n",
    "        LD = np.append(LD, -1)  \n",
    "\n",
    "        chrom = sorted(set(geno['chrom']))\n",
    "        index = list(accumulate([len(geno.groupby('chrom').get_group(i)) for i in chrom])) \n",
    "        for idx in index:\n",
    "            LD[idx - 1] = -1        \n",
    "        return LD\n",
    "\n",
    "    # Updated to accept threshold parameter\n",
    "    def assign_gap_labels(self, LD, threshold):\n",
    "        return np.where(LD == -1, 'N', np.where(LD >= threshold, 'J', 'Y'))\n",
    "\n",
    "    def generate_geno_sequences(self, geno):\n",
    "        lines = []\n",
    "        for i in range(geno.shape[1] - 1):\n",
    "            geno.iloc[:, i] = geno.iloc[:, i] + geno['gap']\n",
    "            lines.append(''.join(geno.iloc[:, i]))\n",
    "        return np.stack(lines, axis=0)\n",
    "\n",
    "    def process_sequences(self, lines):\n",
    "        vocabs = {f\"{a}{b}\": i + 1 for i, (a, b) in enumerate([(\"H\", \"J\"), (\"H\", \"Y\"), (\"H\", \"N\"), (\"L\", \"J\"), (\"L\", \"Y\"), (\"L\", \"N\"), (\"M\", \"J\"), (\"M\", \"Y\"), (\"M\", \"N\")])}\n",
    "        type_vocabs = {\"J\": 1, \"Y\": 2, \"N\": 3}\n",
    "\n",
    "        seqs, type_ids = [], []\n",
    "        for raw_seq in lines:\n",
    "            seq, type_id = [], []\n",
    "            for i in range(0, len(raw_seq), 2):\n",
    "                seq.append(vocabs[raw_seq[i:i + 2]])\n",
    "                type_id.append(type_vocabs[raw_seq[i + 1]])\n",
    "            seqs.append(seq)\n",
    "            type_ids.append(type_id)\n",
    "\n",
    "        return np.asarray(seqs), np.asarray(type_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annos)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq = torch.tensor(self.seqs[index], dtype=torch.float32)\n",
    "        type_ids = torch.tensor(self.type_ids[index], dtype=torch.float32)\n",
    "        annos = torch.tensor(self.annos[index], dtype=torch.float32)\n",
    "        return seq, type_ids, annos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adbe71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2.2] Model definitions: EBMGP architecture + pooling modules (MAP/AVG/MAX/LIP) to support pooling ablation experiments\n",
    "class soft_pool1d(nn.Module):\n",
    "    def __init__(self,  kernel_size=2):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride= kernel_size\n",
    "    def forward(self, x):\n",
    "        e_x = torch.sum(torch.exp(x),dim=1,keepdim=True)\n",
    "        return F.avg_pool1d(x.mul(e_x), self.kernel_size, stride=self.stride).mul_(self.kernel_size).div_(F.avg_pool1d(e_x, self.kernel_size, stride=self.stride).mul_(self.kernel_size))\n",
    "\n",
    "def lip1d(x, logit, kernel=3, stride=2, padding=1):\n",
    "    weight = logit.exp()\n",
    "    return F.avg_pool1d(x*weight, kernel, stride, padding)/F.avg_pool1d(weight, kernel, stride, padding)\n",
    "    \n",
    "class LIP(nn.Module):\n",
    "    # Updated to accept pool_size to match AttentionPool's stride/reduction behavior\n",
    "    def __init__(self, channels, pool_size=2):\n",
    "        super(LIP, self).__init__()\n",
    "        self.pool_size = pool_size\n",
    "        self.logit = nn.Sequential(\n",
    "                nn.Conv1d(channels, channels, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(channels, affine=True),\n",
    "                nn.ReLU(),\n",
    "        )\n",
    "    def init_layer(self):\n",
    "        self.logit[0].weight.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass pool_size as kernel and stride to mimic other pooling layers\n",
    "        # padding is set to pool_size // 2 to maintain consistency\n",
    "        frac = lip1d(x, self.logit(x), kernel=self.pool_size, stride=self.pool_size, padding=self.pool_size//2)\n",
    "        return frac\n",
    "\n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, dim, pool_size=2,dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.pool_size = pool_size\n",
    "        self.attn_dropout = nn.Dropout(dropout_prob)\n",
    "        self.to_attn_logits1 = nn.Conv2d(dim, dim, 1, bias=False)\n",
    "        self.to_attn_logits2 = nn.ModuleList([nn.Conv1d(dim, dim, 1, bias=False) for _ in range(pool_size)])\n",
    "        self.BN = nn.BatchNorm1d(dim)\n",
    "    def forward(self, x):\n",
    "        b, s, n = x.shape  \n",
    "        remainder = n % self.pool_size\n",
    "        needs_padding = remainder > 0      \n",
    "        if needs_padding:\n",
    "            x = F.pad(x, (0, (self.pool_size-remainder)), value=0) \n",
    "        x = x.unfold(-1,self.pool_size,self.pool_size) \n",
    "        outx = []\n",
    "        i = 0\n",
    "        for conv in self.to_attn_logits2:                \n",
    "            nx = x[:,:,:,i]\n",
    "            nx = self.BN(nx)\n",
    "            logit = conv(nx)           \n",
    "            outx.append(logit)\n",
    "            i+=1 \n",
    "        outx = torch.stack(outx, dim=-1)       \n",
    "        logits =  self.to_attn_logits1(outx)\n",
    "        logits = self.attn_dropout(logits)             \n",
    "        attn = logits.softmax(dim=-1)             \n",
    "        outs = (outx * attn).sum(dim=-1)\n",
    "        return outs\n",
    "  \n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(1.702 * x) * x\n",
    "        \n",
    "def ConvBlock(dim, dimout, kernel_size = 1,stride=2):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm1d(dim),\n",
    "        GELU(),\n",
    "        nn.Conv1d(dim,  dimout, kernel_size,stride=stride, padding = kernel_size // 2)\n",
    "    )\n",
    "\n",
    "class EBMGP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 10,\n",
    "        type_vocab_size: int = 4,\n",
    "        hidden_size: int = 64,\n",
    "        num_layers: int = 1,\n",
    "        num_attention_heads: int = 8,\n",
    "        intermediate_size: int = 256,\n",
    "        hidden_act: str = \"gelu\",\n",
    "        dropout_rate: float = 0.3,\n",
    "        pooling_type: str = \"MAP\" # New Argument\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = BertConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_hidden_layers=num_layers,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            intermediate_size=intermediate_size,\n",
    "            hidden_act=hidden_act,\n",
    "            type_vocab_size=type_vocab_size,\n",
    "            max_position_embeddings=5000, \n",
    "            dropout_rate = dropout_rate,\n",
    "        )\n",
    "\n",
    "        self.embeddings = BertEmbeddings(self.config)\n",
    "\n",
    "        # Factory method to select pooling layer\n",
    "        def get_pool(dim, size):\n",
    "            if pooling_type == \"MAP\":\n",
    "                return AttentionPool(dim, pool_size=size)\n",
    "            elif pooling_type == \"AVG\":\n",
    "                return nn.AvgPool1d(kernel_size=size, stride=size)\n",
    "            elif pooling_type == \"MAX\":\n",
    "                return nn.MaxPool1d(kernel_size=size, stride=size)\n",
    "            elif pooling_type == \"LIP\":\n",
    "                return LIP(dim, pool_size=size)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown pooling type: {pooling_type}\")\n",
    "\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            ConvBlock(hidden_size, 64,30,stride=2),\n",
    "            get_pool(64, 1), # Replaced hardcoded AttentionPool\n",
    "            nn.Dropout(dropout_rate), \n",
    "\n",
    "            nn.BatchNorm1d(64),\n",
    "            ConvBlock(64, 64, 3,stride=2),            \n",
    "            get_pool(64, 2), # Replaced hardcoded AttentionPool\n",
    "            nn.Dropout(dropout_rate), \n",
    "\n",
    "            nn.BatchNorm1d(64),\n",
    "            ConvBlock(64, 64, 30,stride=2),           \n",
    "            get_pool(64, 3), # Replaced hardcoded AttentionPool\n",
    "            nn.Dropout(dropout_rate), \n",
    "\n",
    "            nn.BatchNorm1d(64),\n",
    "            ConvBlock(64, 64, 3,stride=2),            \n",
    "            get_pool(64, 2), # Replaced hardcoded AttentionPool\n",
    "            nn.Dropout(dropout_rate), \n",
    "\n",
    "            nn.BatchNorm1d(64),\n",
    "            ConvBlock(64, 64, 30,stride=2),           \n",
    "            get_pool(64, 3), # Replaced hardcoded AttentionPool\n",
    "            nn.Dropout(dropout_rate), \n",
    "        )\n",
    "          \n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),       \n",
    "            Rearrange('b c l -> b (c l)'), \n",
    "            nn.Linear(64, 1),              \n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        token_type_ids: torch.Tensor,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "        output_hidden_states: bool = False,\n",
    "        return_dict: bool = False,\n",
    "    ):\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids.long(),\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids.long(),\n",
    "        )\n",
    "        x = embedding_output.permute(0, 2, 1) \n",
    "        x = self.convs(x)\n",
    "        logits = self.predictor(x) \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cc86b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2.3] Training & evaluation: reproducibility seed, epoch loops, metrics (Pearson/RMSE/R2), and a train_and_evaluate wrapper used by experiments\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, loss_fn, scheduler, device):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for batch_idx, (seqs, type_ids, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        seqs, type_ids, labels = seqs.to(device), type_ids.to(device), labels.to(device)\n",
    "        predict = model(seqs, type_ids)\n",
    "        loss = loss_fn(predict, labels)\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "\n",
    "def evaluate_epoch(model, test_loader, loss_fn, pearson, device):\n",
    "    model.eval()\n",
    "    valid_loss = []\n",
    "    vp, vt = [], []\n",
    "    with torch.no_grad():\n",
    "        for seqs, type_ids, labels in test_loader:\n",
    "            seqs, type_ids, labels = seqs.to(device), type_ids.to(device), labels.to(device)\n",
    "            pred = model(seqs, type_ids)\n",
    "            val_loss = loss_fn(pred, labels)\n",
    "            valid_loss.append(val_loss.item())\n",
    "            vp.extend(pred.squeeze().cpu().numpy())\n",
    "            vt.extend(labels.squeeze().cpu().numpy())\n",
    "    return np.mean(valid_loss), vp, vt\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Added pooling_type argument\n",
    "def train_and_evaluate(trait, data_path, label_path, geno_path, device, learning_rate, epochs, seed, sel_num, ld_threshold=0.8, pooling_type=\"MAP\"):\n",
    "    setup_seed(3407)\n",
    "\n",
    "    loss_fn = nn.L1Loss()\n",
    "    bs = 32\n",
    "    \n",
    "    # Dataset init (reusing your existing modified Dataset class)\n",
    "    traindataset = DNADataset(data_path, label_path, geno_path, trait, seed, sel_num, ld_threshold=ld_threshold, is_training=True)\n",
    "    testdataset = DNADataset(data_path, label_path, geno_path, trait, seed, sel_num, ld_threshold=ld_threshold, is_training=False)\n",
    "\n",
    "    train_loader = DataLoader(traindataset, batch_size=bs, shuffle=True)\n",
    "    test_loader = DataLoader(testdataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "    print(f\"Seed {seed} | Pool {pooling_type} | Train Samples: {len(traindataset)} | Test Samples: {len(testdataset)}\")\n",
    "\n",
    "    # Initialize model with pooling_type\n",
    "    model = EBMGP(pooling_type=pooling_type).to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    steps = math.ceil(len(traindataset) / bs) * epochs - 1\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=steps)\n",
    "    pearson = PearsonCorrCoef().to(device)\n",
    "\n",
    "    corrs, RMSE, pred, obser = [], [], [], []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, loss_fn, scheduler, device)\n",
    "        \n",
    "        if epoch == epochs:\n",
    "            valid_loss, vp, vt = evaluate_epoch(model, test_loader, loss_fn, pearson, device)\n",
    "            \n",
    "            valMSE = mean_squared_error(vp, vt)\n",
    "            val_r2 = r2_score(vt, vp) \n",
    "            v_corr = pearson(torch.tensor(vp).to(device), torch.tensor(vt).to(device))\n",
    "            \n",
    "            print(f\"   > Final Train Loss: {train_loss:.4f} | Final Val Loss: {valid_loss:.4f}\")\n",
    "            print(f\"   > R2 Score: {val_r2:.4f} | Corr: {v_corr.item():.4f}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            RMSE.append(valMSE)\n",
    "            corrs.append(v_corr.item())\n",
    "            pred.extend(vp)\n",
    "            obser.extend(vt)\n",
    "            \n",
    "    return np.mean(corrs), np.mean(RMSE), pred, obser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf009a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3] Ablation study pipeline: (1) LD threshold sweep and (2) pooling strategy sweep; trains EBMGP across seeds/species/traits and saves mean metrics to JSON\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# --- Configuration ---\n",
    "learning_rate = 0.0005\n",
    "epochs = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Optimal Feature Counts per Species (Used for both ablations)\n",
    "species_optimal_counts = {\n",
    "    \"rice\": 5000,\n",
    "    \"sorghum\": 5000,\n",
    "    # \"soybean\": 3000,\n",
    "    \"bulls\": 5000\n",
    "}\n",
    "\n",
    "species_config = {\n",
    "    \"rice\": {\n",
    "        \"label_path\": \"./data/rice_pheno.csv\",\n",
    "        \"geno_path\": \"./data/ricerawgeno.csv\",\n",
    "        \"traits\": ['SW', 'FLW', 'AC', 'PH', 'SNPP']\n",
    "    },\n",
    "    \"sorghum\": {\n",
    "        \"label_path\": \"./data/sorghum_pheno.csv\",\n",
    "        \"geno_path\": \"./data/sorghumrawgeno.csv\",\n",
    "        \"traits\": ['HT', 'MO', 'YLD']\n",
    "    }\n",
    "    # \"soybean\": {\n",
    "    #     \"label_path\": \"./data/soybean_pheno.csv\",\n",
    "    #     \"geno_path\": \"./data/soybeanrawgeno.csv\",\n",
    "    #     \"traits\": ['protein', 'Steartic', 'R8', 'SdWgt', 'Yield']\n",
    "    # },\n",
    "    \"bulls\": {\n",
    "        \"label_path\": \"./data/bulls_pheno.csv\",\n",
    "        \"geno_path\": \"./data/bullsrawgeno.csv\",\n",
    "        \"traits\": ['MS', 'NMSP', 'VE']\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Experiment 1: LD Threshold Ablation ---\n",
    "def run_ld_ablation():\n",
    "    print(\"\\n\" + \"#\"*60)\n",
    "    print(\"RUNNING EXPERIMENT: LD THRESHOLD ABLATION\")\n",
    "    print(\"#\"*60)\n",
    "    \n",
    "    ld_thresholds = [0.2, 0.4, 0.6, 0.8]\n",
    "    \n",
    "    for species, optimal_count in species_optimal_counts.items():\n",
    "        config = species_config[species]\n",
    "        sel_num = optimal_count\n",
    "        T_folder = f\"T{optimal_count}\"\n",
    "        \n",
    "        for ld_thresh in ld_thresholds:\n",
    "            print(f\"\\n   >>> Processing LD Threshold: {ld_thresh} | Species: {species} | Feature Count: {T_folder}\")\n",
    "            \n",
    "            results_data = {\n",
    "                \"species\": species,\n",
    "                \"feature_count\": optimal_count,\n",
    "                \"ld_threshold\": ld_thresh,\n",
    "                \"pooling_type\": \"MAP\", # Default for LD ablation\n",
    "                \"mean_corrs\": {},\n",
    "                \"mean_RMSE\": {}\n",
    "            }\n",
    "\n",
    "            for trait_idx, trait_name in enumerate(config[\"traits\"]):\n",
    "                data_path = f\"./EN/{species}/{T_folder}{trait_name}\"\n",
    "                \n",
    "                if not os.path.exists(f\"{data_path}0.csv\"):\n",
    "                    continue\n",
    "\n",
    "                corrs, RMSE = [], []\n",
    "                print(f\"      Processing Trait: {trait_name}\")\n",
    "                \n",
    "                for seed in range(5):\n",
    "                    try:\n",
    "                        # Uses default pooling_type=\"MAP\"\n",
    "                        fold_corrs, fold_RMSE, _, _ = train_and_evaluate(\n",
    "                            trait_idx, data_path, config[\"label_path\"], config[\"geno_path\"], \n",
    "                            device, learning_rate, epochs, seed, sel_num,\n",
    "                            ld_threshold=ld_thresh, pooling_type=\"MAP\"\n",
    "                        )\n",
    "                        corrs.append(fold_corrs)\n",
    "                        RMSE.append(fold_RMSE)\n",
    "                    except Exception as e:\n",
    "                        print(f\"         [Error] Seed {seed}: {e}\")\n",
    "\n",
    "                if corrs:\n",
    "                    results_data[\"mean_corrs\"][trait_name] = float(np.mean(corrs))\n",
    "                    results_data[\"mean_RMSE\"][trait_name] = float(np.mean(RMSE))\n",
    "\n",
    "            # Save JSON\n",
    "            json_filename = f\"{species}_{T_folder}_LD{ld_thresh}.json\"\n",
    "            with open(json_filename, \"w\") as f:\n",
    "                json.dump(results_data, f, indent=4)\n",
    "            print(f\"   [Saved] {json_filename}\")\n",
    "\n",
    "\n",
    "# --- Experiment 2: Pooling Strategy Ablation ---\n",
    "def run_pooling_ablation():\n",
    "    print(\"\\n\" + \"#\"*60)\n",
    "    print(\"RUNNING EXPERIMENT: POOLING STRATEGY ABLATION\")\n",
    "    print(\"#\"*60)\n",
    "    \n",
    "    pooling_types = [\"MAP\", \"AVG\", \"MAX\", \"LIP\"]\n",
    "    \n",
    "    for species, optimal_count in species_optimal_counts.items():\n",
    "        config = species_config[species]\n",
    "        sel_num = optimal_count\n",
    "        T_folder = f\"T{optimal_count}\"\n",
    "        \n",
    "        for p_type in pooling_types:\n",
    "            print(f\"\\n   >>> Processing Pooling Type: {p_type} | Species: {species} | Feature Count: {T_folder}\")\n",
    "            \n",
    "            results_data = {\n",
    "                \"species\": species,\n",
    "                \"feature_count\": optimal_count,\n",
    "                \"ld_threshold\": 0.8, # Default for Pooling ablation\n",
    "                \"pooling_type\": p_type,\n",
    "                \"mean_corrs\": {},\n",
    "                \"mean_RMSE\": {}\n",
    "            }\n",
    "\n",
    "            for trait_idx, trait_name in enumerate(config[\"traits\"]):\n",
    "                data_path = f\"./EN/{species}/{T_folder}{trait_name}\"\n",
    "                \n",
    "                if not os.path.exists(f\"{data_path}0.csv\"):\n",
    "                    continue\n",
    "\n",
    "                corrs, RMSE = [], []\n",
    "                print(f\"      Processing Trait: {trait_name}\")\n",
    "                \n",
    "                for seed in range(5):\n",
    "                    try:\n",
    "                        # Uses default ld_threshold=0.8\n",
    "                        fold_corrs, fold_RMSE, _, _ = train_and_evaluate(\n",
    "                            trait_idx, data_path, config[\"label_path\"], config[\"geno_path\"], \n",
    "                            device, learning_rate, epochs, seed, sel_num,\n",
    "                            ld_threshold=0.8, pooling_type=p_type\n",
    "                        )\n",
    "                        corrs.append(fold_corrs)\n",
    "                        RMSE.append(fold_RMSE)\n",
    "                    except Exception as e:\n",
    "                        print(f\"         [Error] Seed {seed}: {e}\")\n",
    "\n",
    "                if corrs:\n",
    "                    results_data[\"mean_corrs\"][trait_name] = float(np.mean(corrs))\n",
    "                    results_data[\"mean_RMSE\"][trait_name] = float(np.mean(RMSE))\n",
    "\n",
    "            # Save JSON\n",
    "            json_filename = f\"{species}_{T_folder}_{p_type}.json\"\n",
    "            with open(json_filename, \"w\") as f:\n",
    "                json.dump(results_data, f, indent=4)\n",
    "            print(f\"   [Saved] {json_filename}\")\n",
    "\n",
    "def main():\n",
    "    # You can comment out one of these lines if you only want to run one experiment\n",
    "    run_ld_ablation()\n",
    "    run_pooling_ablation()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b0b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Benchy (Not Main?)\n",
    "# [2.2/3] Baseline models benchmark: load same EN-selected SNPs, encode numerically (0/1/2), reproduce CV splits, and compare Ridge/BayesianRidge/HGBRegressor against DL\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, BayesianRidge\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATA LOADER FOR NUMERICAL MODELS\n",
    "# ==========================================\n",
    "class BenchmarkDataLoader:\n",
    "    \"\"\"\n",
    "    Extracts NUMERICAL genotype matrices (0,1,2) matching the exact \n",
    "    feature selection and CV splits of the Deep Learning pipeline.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, label_path, geno_path, trait_idx, seed, sel_num):\n",
    "        # 1. Load Pre-selected Features (Elastic Net Output)\n",
    "        # We must load the exact same features used in the DL pipeline\n",
    "        cs = pd.read_csv(f\"{data_path}{seed}.csv\").sort_values(by='cs', ascending=False)\n",
    "        Top = sorted(cs.index[:sel_num])\n",
    "        \n",
    "        # 2. Load Raw Genotype Data\n",
    "        Rawgeno = pd.read_csv(geno_path, low_memory=False)\n",
    "        # Drop the first row (SNP index row) usually present in your format\n",
    "        Rawgeno = Rawgeno.iloc[1:].reset_index(drop=True)\n",
    "        \n",
    "        # Filter for valid indices\n",
    "        Top = [i for i in Top if i in Rawgeno.index]\n",
    "        geno = Rawgeno.loc[Top].copy()\n",
    "\n",
    "        # 3. Convert H/M/L to 0/1/2 (Numerical Encoding)\n",
    "        # Note: Your DL pipeline does this inside 'calculate_LD'. \n",
    "        # We assume columns before the last 2 are samples.\n",
    "        geno_vals = geno.iloc[:, :-2].values\n",
    "        \n",
    "        # Vectorized replacement for speed\n",
    "        X_num = np.select(\n",
    "            [geno_vals == 'H', geno_vals == 'M', geno_vals == 'L'],\n",
    "            [0, 1, 2],\n",
    "            default=1 # Default to heterozygous/mean if missing/unknown\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        # Transpose: (SNPs, Samples) -> (Samples, SNPs)\n",
    "        self.X = X_num.T\n",
    "        \n",
    "        # 4. Load Labels\n",
    "        annos = pd.read_csv(label_path, index_col=0).iloc[:, [trait_idx]]\n",
    "        annos = annos.fillna(annos.mean())\n",
    "        \n",
    "        # Standardize labels (Critical for convergence in Ridge/Bayes)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.y = self.scaler.fit_transform(annos).flatten().astype(np.float32)\n",
    "        \n",
    "        # 5. Reproduce Exact CV Splits\n",
    "        # Must use same random_state=27 as DL pipeline\n",
    "        self.kfold = KFold(n_splits=5, shuffle=True, random_state=27)\n",
    "        self.current_seed = seed\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"Returns X_train, X_test, y_train, y_test for the specific seed.\"\"\"\n",
    "        # Iterate to find the split matching the current seed\n",
    "        for i, (train_idx, val_idx) in enumerate(self.kfold.split(self.X, self.y)):\n",
    "            if i == self.current_seed:\n",
    "                return (\n",
    "                    self.X[train_idx], \n",
    "                    self.X[val_idx], \n",
    "                    self.y[train_idx], \n",
    "                    self.y[val_idx]\n",
    "                )\n",
    "        return None, None, None, None\n",
    "\n",
    "# ==========================================\n",
    "# 2. MODEL DEFINITIONS\n",
    "# ==========================================\n",
    "\n",
    "def run_gblup_proxy(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    GBLUP is mathematically equivalent to Ridge Regression (L2).\n",
    "    We use RidgeCV to automatically tune the regularization parameter (alpha).\n",
    "    This is 'Research Grade' because it optimizes hyperparams internally.\n",
    "    \"\"\"\n",
    "    # Alphas to search: logarithmic scale 0.1 to 1000\n",
    "    alphas = np.logspace(-1, 3, 10) \n",
    "    model = RidgeCV(alphas=alphas)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "def run_bayes_proxy(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    BayesianRidge is a robust proxy for BayesC/BayesB.\n",
    "    It infers precision parameters from data, handling 'large p, small n'.\n",
    "    \"\"\"\n",
    "    model = BayesianRidge()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "def run_sota_tabular(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    HistGradientBoosting is Scikit-Learn's implementation of LightGBM.\n",
    "    It handles non-linearities and epistasis better than linear models.\n",
    "    \"\"\"\n",
    "    model = HistGradientBoostingRegressor(\n",
    "        max_iter=100, \n",
    "        max_depth=5, \n",
    "        learning_rate=0.1, \n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "# ==========================================\n",
    "# 3. BENCHMARKING ENGINE\n",
    "# ==========================================\n",
    "\n",
    "def run_benchmarks():\n",
    "    print(\"\\n\" + \"#\"*60)\n",
    "    print(\"RUNNING BENCHMARKING (GBLUP, Bayes, SOTA-Tree)\")\n",
    "    print(\"#\"*60)\n",
    "\n",
    "    # Models to evaluate\n",
    "    benchmark_models = {\n",
    "        \"GBLUP\": run_gblup_proxy,\n",
    "        \"BayesB\": run_bayes_proxy,\n",
    "        \"LightGBM\": run_sota_tabular\n",
    "    }\n",
    "\n",
    "    # Configuration (Reusing your species_config and optimal counts)\n",
    "    # Ensure these variables exist in your notebook context\n",
    "    global species_config, species_optimal_counts\n",
    "    \n",
    "    for species, optimal_count in species_optimal_counts.items():\n",
    "        config = species_config[species]\n",
    "        sel_num = optimal_count\n",
    "        T_folder = f\"T{optimal_count}\"\n",
    "        \n",
    "        # We iterate over models first to save organized JSONs\n",
    "        for model_name, model_func in benchmark_models.items():\n",
    "            print(f\"\\n   >>> Benchmarking Model: {model_name} | Species: {species} | Feature Count: {T_folder}\")\n",
    "            \n",
    "            results_data = {\n",
    "                \"species\": species,\n",
    "                \"feature_count\": optimal_count,\n",
    "                \"model\": model_name,\n",
    "                \"mean_corrs\": {},\n",
    "                \"mean_RMSE\": {},\n",
    "                \"mean_R2\": {}\n",
    "            }\n",
    "\n",
    "            for trait_idx, trait_name in enumerate(config[\"traits\"]):\n",
    "                data_path = f\"./EN/{species}/{T_folder}{trait_name}\"\n",
    "                \n",
    "                # Check if feature selection file exists\n",
    "                if not os.path.exists(f\"{data_path}0.csv\"):\n",
    "                    print(f\"      [Skip] Trait {trait_name}: Feature file not found.\")\n",
    "                    continue\n",
    "\n",
    "                corrs, rmses, r2s = [], [], []\n",
    "                print(f\"      Processing Trait: {trait_name}...\", end=\"\")\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                for seed in range(5):\n",
    "                    try:\n",
    "                        # 1. Load Data\n",
    "                        loader = BenchmarkDataLoader(\n",
    "                            data_path, config[\"label_path\"], config[\"geno_path\"], \n",
    "                            trait_idx, seed, sel_num\n",
    "                        )\n",
    "                        X_train, X_test, y_train, y_test = loader.get_data()\n",
    "                        \n",
    "                        # 2. Train & Predict\n",
    "                        preds = model_func(X_train, y_train, X_test)\n",
    "                        \n",
    "                        # 3. Evaluate\n",
    "                        # Pearson returns (corr, p-value), we take [0]\n",
    "                        p_corr = pearsonr(y_test, preds)[0]\n",
    "                        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "                        r2 = r2_score(y_test, preds)\n",
    "                        \n",
    "                        corrs.append(p_corr)\n",
    "                        rmses.append(rmse)\n",
    "                        r2s.append(r2)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"\\n         [Error] Seed {seed}: {e}\")\n",
    "                \n",
    "                # Aggregate results\n",
    "                if corrs:\n",
    "                    mean_r2 = float(np.mean(r2s))\n",
    "                    mean_corr = float(np.mean(corrs))\n",
    "                    results_data[\"mean_corrs\"][trait_name] = mean_corr\n",
    "                    results_data[\"mean_RMSE\"][trait_name] = float(np.mean(rmses))\n",
    "                    results_data[\"mean_R2\"][trait_name] = mean_r2\n",
    "                    \n",
    "                    print(f\" Done ({time.time()-start_time:.1f}s) | Avg R2: {mean_r2:.4f} | Avg Corr: {mean_corr:.4f}\")\n",
    "\n",
    "            # Save results to JSON\n",
    "            json_filename = f\"{species}_{T_folder}_{model_name}.json\"\n",
    "            with open(json_filename, \"w\") as f:\n",
    "                json.dump(results_data, f, indent=4)\n",
    "            print(f\"   [Saved] {json_filename}\")\n",
    "\n",
    "# Run the benchmark\n",
    "if __name__ == \"__main__\":\n",
    "    run_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8087ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2.1] Dataset validation + EDA: align genotype/phenotype IDs, plot trait distributions, and visualize Elastic Net feature-score (“Manhattan”) diagnostics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# --- Configuration (Mirrors your provided structure) ---\n",
    "species_config = {\n",
    "    \"rice\": {\n",
    "        \"label_path\": \"./data/rice_pheno.csv\",\n",
    "        \"geno_path\": \"./data/ricerawgeno.csv\",\n",
    "        \"traits\": ['SW', 'FLW', 'AC', 'PH', 'SNPP'],\n",
    "        \"id_col_pheno\": \"NSFTVID\", # Inferred from previous context\n",
    "        \"fix_rice_ids\": True       # Rice specific logic\n",
    "    },\n",
    "    \"sorghum\": {\n",
    "        \"label_path\": \"./data/sorghum_pheno.csv\",\n",
    "        \"geno_path\": \"./data/sorghumrawgeno.csv\",\n",
    "        \"traits\": ['HT', 'MO', 'YLD'],\n",
    "        \"id_col_pheno\": \"ID\",\n",
    "        \"fix_rice_ids\": False\n",
    "    },\n",
    "    \"bulls\": {\n",
    "        \"label_path\": \"./data/bulls_pheno.csv\",\n",
    "        \"geno_path\": \"./data/bullsrawgeno.csv\",\n",
    "        \"traits\": ['MS', 'NMSP', 'VE'],\n",
    "        \"id_col_pheno\": \"ID\",\n",
    "        \"fix_rice_ids\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Optimal Feature Counts (from your script)\n",
    "species_optimal_counts = {\n",
    "    \"rice\": 5000,\n",
    "    \"sorghum\": 5000,\n",
    "    \"bulls\": 5000\n",
    "}\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def clean_rice_ids(geno_id):\n",
    "    \"\"\"Specific cleaner for Rice Genotype IDs (removes suffixes).\"\"\"\n",
    "    if isinstance(geno_id, str) and '_' in geno_id:\n",
    "        return geno_id.rsplit('_', 1)[0]\n",
    "    return geno_id\n",
    "\n",
    "def load_and_align_data(species, config):\n",
    "    print(f\"\\n{'='*15} Loading Data for {species.upper()} {'='*15}\")\n",
    "    \n",
    "    # 1. Load Phenotype\n",
    "    if not os.path.exists(config['label_path']):\n",
    "        print(f\"[Missing] Phenotype file not found: {config['label_path']}\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        pheno = pd.read_csv(config['label_path'])\n",
    "        # Ensure ID is string\n",
    "        pheno[config['id_col_pheno']] = pheno[config['id_col_pheno']].astype(str)\n",
    "        pheno.set_index(config['id_col_pheno'], inplace=True)\n",
    "        print(f\"   Phenotype loaded: {pheno.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   [Error] Loading phenotype: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # 2. Load Genotype\n",
    "    if not os.path.exists(config['geno_path']):\n",
    "        print(f\"[Missing] Genotype file not found: {config['geno_path']}\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        # Read header only first to optimize\n",
    "        geno_preview = pd.read_csv(config['geno_path'], nrows=1)\n",
    "        meta_cols = [c for c in geno_preview.columns if c.lower() in ['chrom', 'pos', 'chromosome', 'position']]\n",
    "        sample_cols = [c for c in geno_preview.columns if c not in meta_cols]\n",
    "        \n",
    "        # Load full file\n",
    "        geno = pd.read_csv(config['geno_path'])\n",
    "        geno_samples = geno[sample_cols]\n",
    "\n",
    "        # Apply Rice Fix if needed\n",
    "        if config['fix_rice_ids']:\n",
    "            print(\"   Applying Rice ID fix...\")\n",
    "            new_names = {c: clean_rice_ids(c) for c in sample_cols}\n",
    "            geno_samples = geno_samples.rename(columns=new_names)\n",
    "\n",
    "        # Transpose\n",
    "        geno_t = geno_samples.T\n",
    "        geno_t.index = geno_t.index.astype(str)\n",
    "        print(f\"   Genotype loaded (Transposed): {geno_t.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   [Error] Loading genotype: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # 3. Align\n",
    "    common = geno_t.index.intersection(pheno.index)\n",
    "    print(f\"   Matched Samples: {len(common)}\")\n",
    "    \n",
    "    if len(common) == 0:\n",
    "        print(\"   [Warning] No matching IDs found!\")\n",
    "        return None, None\n",
    "        \n",
    "    pheno_aligned = pheno.loc[common]\n",
    "    \n",
    "    # Plot Trait Distributions\n",
    "    plot_distributions(species, pheno_aligned, config['traits'])\n",
    "    \n",
    "    return pheno_aligned, geno_t.loc[common]\n",
    "\n",
    "def plot_distributions(species, df, traits):\n",
    "    \"\"\"Plots histograms for the traits defined in config.\"\"\"\n",
    "    valid_traits = [t for t in traits if t in df.columns]\n",
    "    \n",
    "    if not valid_traits:\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    for i, trait in enumerate(valid_traits):\n",
    "        plt.subplot(1, len(valid_traits), i+1)\n",
    "        sns.histplot(df[trait].dropna(), kde=True)\n",
    "        plt.title(f\"{species}: {trait}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    out_path = f\"{species}_pheno_dist.png\"\n",
    "    plt.savefig(out_path)\n",
    "    print(f\"   [Saved] Distribution plot: {out_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def analyze_feature_selection(species_name, config, optimal_count):\n",
    "    \"\"\"\n",
    "    Analyzes the T* CSV files in ./EN/{species}/\n",
    "    to see which features are selected.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Analyzing Selection Files for {species_name.upper()} ---\")\n",
    "    \n",
    "    base_dir = f\"./EN/{species_name}/\"\n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"   [Skip] Directory not found: {base_dir}\")\n",
    "        return\n",
    "\n",
    "    summary_stats = []\n",
    "    \n",
    "    # Iterate over traits defined in config\n",
    "    for trait in config['traits']:\n",
    "        # Construct path pattern based on your logic: T{count}{trait}*.csv\n",
    "        # e.g. ./EN/rice/T5000SW*.csv\n",
    "        file_pattern = os.path.join(base_dir, f\"T{optimal_count}{trait}*.csv\")\n",
    "        files = glob.glob(file_pattern)\n",
    "        \n",
    "        if not files:\n",
    "            print(f\"   No files found for trait {trait} (Pattern: {file_pattern})\")\n",
    "            continue\n",
    "            \n",
    "        # Analyze the first file found (seed 0 usually) or aggregate all\n",
    "        for filepath in files[:1]: # Just look at one per trait for overview\n",
    "            try:\n",
    "                df = pd.read_csv(filepath)\n",
    "                \n",
    "                # Basic Stats\n",
    "                max_score = df['cs'].max()\n",
    "                non_zero = (df['cs'] > 0).sum()\n",
    "                \n",
    "                # Top hits\n",
    "                top = df.sort_values('cs', ascending=False).head(3)\n",
    "                top_str = \"; \".join([f\"idx:{r['index']:.0f}({r['cs']:.0f})\" for _, r in top.iterrows()])\n",
    "                \n",
    "                summary_stats.append({\n",
    "                    'Trait': trait,\n",
    "                    'File': os.path.basename(filepath),\n",
    "                    'Max_Score': max_score,\n",
    "                    'Non_Zero_Markers': non_zero,\n",
    "                    'Top_Markers': top_str\n",
    "                })\n",
    "                \n",
    "                # Manhattan Plot for this file\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.scatter(df['index'], df['cs'], s=10, alpha=0.6)\n",
    "                plt.title(f\"Feature Scores: {species_name} - {trait}\")\n",
    "                plt.xlabel(\"SNP Index\")\n",
    "                plt.ylabel(\"Score\")\n",
    "                plt.savefig(f\"{species_name}_{trait}_manhattan.png\")\n",
    "                plt.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   Error reading {filepath}: {e}\")\n",
    "\n",
    "    if summary_stats:\n",
    "        print(pd.DataFrame(summary_stats).to_string(index=False))\n",
    "        print(f\"   [Saved] Manhattan plots for {species_name} traits.\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "def main():\n",
    "    # Setup directories if they don't exist (just to prevent crashes)\n",
    "    if not os.path.exists(\"./data\"): os.makedirs(\"./data\")\n",
    "    \n",
    "    for species, config in species_config.items():\n",
    "        # 1. Check Data Alignment\n",
    "        load_and_align_data(species, config)\n",
    "        \n",
    "        # 2. Analyze Feature Selection Outputs\n",
    "        if species in species_optimal_counts:\n",
    "            analyze_feature_selection(species, config, species_optimal_counts[species])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01403f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2.2/3] Genomic baseline benchmarks: GBLUP (GRM+KRR), BayesB-proxy (ARD), XGBoost, and KNN; cross-validated Pearson correlation and CSV export\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import ARDRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==========================================\n",
    "# 1. MODEL DEFINITIONS (\"No-BS\" versions)\n",
    "# ==========================================\n",
    "\n",
    "class GBLUP:\n",
    "    \"\"\"Genomic Best Linear Unbiased Prediction via Kernel Ridge.\"\"\"\n",
    "    def __init__(self):\n",
    "        # alpha=1.0 corresponds to lambda in ridge regression\n",
    "        self.model = KernelRidge(alpha=1.0, kernel='precomputed')\n",
    "        self.X_train = None\n",
    "\n",
    "    def compute_grm(self, X):\n",
    "        \"\"\"VanRaden Genomic Relationship Matrix\"\"\"\n",
    "        X = np.array(X)\n",
    "        p_freq = np.mean(X, axis=0) / 2\n",
    "        Z = X - 2 * p_freq\n",
    "        scale = 2 * np.sum(p_freq * (1 - p_freq))\n",
    "        # Add small jitter to scale to prevent division by zero\n",
    "        if scale == 0: scale = 1 \n",
    "        K = np.dot(Z, Z.T) / scale\n",
    "        return K\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        K_train = self.compute_grm(X)\n",
    "        self.model.fit(K_train, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # We must compute kernel between Test and Train samples\n",
    "        X_train_np = np.array(self.X_train)\n",
    "        X_test_np = np.array(X_test)\n",
    "        \n",
    "        p_freq = np.mean(X_train_np, axis=0) / 2\n",
    "        scale = 2 * np.sum(p_freq * (1 - p_freq))\n",
    "        if scale == 0: scale = 1\n",
    "        \n",
    "        Z_train = X_train_np - 2 * p_freq\n",
    "        Z_test = X_test_np - 2 * p_freq\n",
    "        \n",
    "        K_test = np.dot(Z_test, Z_train.T) / scale\n",
    "        return self.model.predict(K_test)\n",
    "\n",
    "class BayesB_Proxy:\n",
    "    \"\"\"Approximation of BayesB using ARD Regression (Sparsity inducing).\"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = ARDRegression(max_iter=300)\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "class XGBoostGenomic:\n",
    "    \"\"\"XGBoost optimized for dense SNP data.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = XGBRegressor(\n",
    "            n_estimators=300, max_depth=6, learning_rate=0.05, \n",
    "            subsample=0.8, n_jobs=-1, verbosity=0\n",
    "        )\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "class GenomicKNN:\n",
    "    def __init__(self):\n",
    "        self.model = KNeighborsRegressor(n_neighbors=20, metric='correlation')\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# ==========================================\n",
    "# 2. DATA LOADING LOGIC (Specific to your folder)\n",
    "# ==========================================\n",
    "\n",
    "TRAIT_MAP = {\n",
    "    'rice': ['FLW', 'PH', 'SNPP', 'SW', 'AC'],\n",
    "    'sorghum': ['HT', 'YLD', 'MO'],\n",
    "    # 'soybean': ['protein', 'Yield', 'SdWgt', 'Steartic', 'R8'],\n",
    "    'bulls': ['MS', 'VE', 'NMSP']\n",
    "}\n",
    "\n",
    "PHENO_COL_MAP = {\n",
    "    \"rice\": {\n",
    "        \"FLW\": \"Flag leaf width\",\n",
    "        \"PH\": \"Plant height\",\n",
    "        \"SNPP\": \"Seed number per panicle\",\n",
    "        \"SW\": \"Seed width\",\n",
    "        \"AC\": \"Amylose content\",\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_dataset(species, trait, num_snps, seed):\n",
    "    \"\"\"\n",
    "    Loads X from ./EN/{species}/T{num_snps}{trait}{seed}.csv\n",
    "    Loads y from ./data/{species}_pheno.csv\n",
    "    \"\"\"\n",
    "    # 1. Construct Path for Genotypes (X)\n",
    "    # Note: Using your file list logic (e.g., ./EN/rice/T5000FLW0.csv)\n",
    "    en_path = f\"./EN/{species}/T{num_snps}{trait}{seed}.csv\"\n",
    "    \n",
    "    if not os.path.exists(en_path):\n",
    "        # Fallback for capitalization mismatches often seen in folders\n",
    "        # (e.g. 'protein' vs 'Protein')\n",
    "        possible_files = glob.glob(f\"./EN/{species}/T{num_snps}*{seed}.csv\")\n",
    "        # Try to find case-insensitive match\n",
    "        match = next((f for f in possible_files if trait.lower() in f.lower()), None)\n",
    "        if match:\n",
    "            en_path = match\n",
    "        else:\n",
    "            print(f\"  [Warning] File not found: {en_path}\")\n",
    "            return None, None\n",
    "\n",
    "    # Load Genotypes (Assumed to be purely numeric, no headers or simple headers)\n",
    "    # Your previous code implies these are just SNP matrices.\n",
    "    try:\n",
    "        X = pd.read_csv(en_path)\n",
    "        # Drop non-numeric columns if they sneak in (like IDs)\n",
    "        X = X.select_dtypes(include=[np.number]).values\n",
    "    except Exception as e:\n",
    "        print(f\"  [Error] Could not read X: {e}\")\n",
    "        return None, None\n",
    "    # 2. Load Phenotypes (y)\n",
    "    pheno_path = f\"./data/{species}_pheno.csv\"\n",
    "    if not os.path.exists(pheno_path):  # optional: helps when using attachments in this environment\n",
    "        pheno_path = f\"/mnt/data/{species}_pheno.csv\"\n",
    "\n",
    "    try:\n",
    "        df_pheno = pd.read_csv(pheno_path)\n",
    "\n",
    "        # ---- THE FIX: map rice short codes -> real column names ----\n",
    "        lookup_trait = PHENO_COL_MAP.get(species, {}).get(trait, trait)\n",
    "\n",
    "        cols = df_pheno.columns\n",
    "        target_col = next((c for c in cols if c.strip().lower() == lookup_trait.strip().lower()), None)\n",
    "\n",
    "        if target_col is None:\n",
    "            print(f\"  [Error] Trait '{trait}' not found in {pheno_path}\")\n",
    "            return None, None\n",
    "\n",
    "        y = df_pheno[target_col].values\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [Error] Could not read Phenotype: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # 3. Simple Alignment Check\n",
    "    # We assume X and y are already aligned by row index as is standard in these prep bundles\n",
    "    min_len = min(len(X), len(y))\n",
    "    return X[:min_len], y[:min_len]\n",
    "\n",
    "# ==========================================\n",
    "# 3. MAIN BENCHMARK LOOP\n",
    "# ==========================================\n",
    "\n",
    "def run_benchmarks():\n",
    "    # Only run for T5000 for now to save time, as indicated by your output folder focus\n",
    "    NUM_SNPS = 5000 \n",
    "    \n",
    "    # Define models\n",
    "    models = {\n",
    "        \"GBLUP\": GBLUP(),\n",
    "        \"BayesB\": BayesB_Proxy(),\n",
    "        \"XGBoost\": XGBoostGenomic(),\n",
    "        \"KNN\": GenomicKNN()\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    print(f\"Starting Benchmark for T{NUM_SNPS}...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for species, traits in TRAIT_MAP.items():\n",
    "        print(f\"Processing Species: {species.upper()}\")\n",
    "        \n",
    "        for trait in traits:\n",
    "            print(f\"  > Trait: {trait}\")\n",
    "            \n",
    "            trait_corrs = {m: [] for m in models}\n",
    "            \n",
    "            # Loop through Seeds 0 to 4\n",
    "            for seed in range(5):\n",
    "                X, y = load_dataset(species, trait, NUM_SNPS, seed)\n",
    "                \n",
    "                if X is None:\n",
    "                    continue\n",
    "\n",
    "                # 5-Fold CV on this specific Seed's dataset\n",
    "                # Note: Usually \"Seed 0\" file implies a specific split, \n",
    "                # but we will run a quick CV to get robust metrics.\n",
    "                kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "                \n",
    "                # We aggregate predictions for this seed to calculate one correlation\n",
    "                # Or average correlations across folds. Let's average folds.\n",
    "                \n",
    "                for name, model in models.items():\n",
    "                    fold_scores = []\n",
    "                    \n",
    "                    for train_idx, val_idx in kf.split(X):\n",
    "                        X_train, X_val = X[train_idx], X[val_idx]\n",
    "                        y_train, y_val = y[train_idx], y[val_idx]\n",
    "                        \n",
    "                        # Scale Y\n",
    "                        scaler = StandardScaler()\n",
    "                        y_train = scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "                        \n",
    "                        try:\n",
    "                            model.fit(X_train, y_train)\n",
    "                            preds = model.predict(X_val)\n",
    "                            \n",
    "                            # Inverse scale preds for fair comparison (optional, but good for MSE)\n",
    "                            # For Pearson Corr, scaling doesn't matter.\n",
    "                            \n",
    "                            # Pearson Correlation\n",
    "                            if len(np.unique(preds)) > 1: # Avoid constant output errors\n",
    "                                corr, _ = pearsonr(y_val, preds)\n",
    "                                fold_scores.append(corr)\n",
    "                            else:\n",
    "                                fold_scores.append(0.0)\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            # print(f\"    [Error] {name} failed: {e}\")\n",
    "                            fold_scores.append(0.0)\n",
    "                    \n",
    "                    avg_fold_score = np.mean(fold_scores)\n",
    "                    trait_corrs[name].append(avg_fold_score)\n",
    "\n",
    "            # Summarize Trait Results\n",
    "            print(f\"    Results for {trait} (Avg over 5 seeds):\")\n",
    "            for name in models:\n",
    "                final_score = np.mean(trait_corrs[name])\n",
    "                print(f\"      {name:<10}: {final_score:.4f}\")\n",
    "                \n",
    "                # Save to results list for CSV export\n",
    "                results.append({\n",
    "                    \"Species\": species,\n",
    "                    \"Trait\": trait,\n",
    "                    \"Model\": name,\n",
    "                    \"Pearson_Corr\": final_score\n",
    "                })\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    # Export\n",
    "    df_res = pd.DataFrame(results)\n",
    "    df_res.to_csv(\"benchmark_results_proper.csv\", index=False)\n",
    "    print(\"\\nBenchmark Complete. Saved to 'benchmark_results_proper.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_benchmarks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
