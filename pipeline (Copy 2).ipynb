{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89d3c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "from itertools import accumulate\n",
    "from typing import Optional, List, Tuple\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import PearsonCorrCoef\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from transformers import BertPreTrainedModel\n",
    "from transformers.modeling_utils import ModuleUtilsMixin\n",
    "from transformers.models.bert.modeling_bert import (\n",
    "    BertConfig, \n",
    "    BertEmbeddings, \n",
    "    BertEncoder, \n",
    "    BertLayer, \n",
    "    BertAttention\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7900a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNADataset(Dataset):\n",
    "    # Updated __init__ to accept ld_threshold\n",
    "    def __init__(self, data_path, label_path, geno_path, trait, seed, sel_num, ld_threshold=0.8, is_training=True):\n",
    "        cs = pd.read_csv(f\"{data_path}{seed}.csv\").sort_values(by='cs', ascending=False)\n",
    "        Top = sorted(cs.index[:sel_num])  \n",
    "        Rawgeno = pd.read_csv(geno_path)\n",
    "\n",
    "        # DROP the first row (SNP index row)\n",
    "        Rawgeno = Rawgeno.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "        Top = [i for i in Top if i in Rawgeno.index]\n",
    "\n",
    "        geno = Rawgeno.loc[Top].copy()\n",
    "\n",
    "        # explicitly separate columns\n",
    "        geno_cols = [c for c in geno.columns if c not in ['chrom']]\n",
    "\n",
    "        LD = self.calculate_LD(geno)\n",
    "        \n",
    "        # Pass the variable threshold here\n",
    "        geno['gap'] = self.assign_gap_labels(LD, ld_threshold)\n",
    "        \n",
    "        geno = geno.drop(columns=geno.columns[[-3, -2]])  \n",
    "        lines = self.generate_geno_sequences(geno)\n",
    "        annos = pd.read_csv(label_path, index_col=0).iloc[:, [trait]]\n",
    "        annos = annos.fillna(annos.mean()) \n",
    "        annos = StandardScaler().fit_transform(annos).astype(np.float32)\n",
    "\n",
    "        kfold = KFold(n_splits=5, shuffle=True, random_state=27)\n",
    "        for i, (train_idx, val_idx) in enumerate(kfold.split(lines, annos)):\n",
    "            if i == seed:\n",
    "                train_lines, val_lines = lines[train_idx], lines[val_idx]\n",
    "                train_annos, val_annos = annos[train_idx], annos[val_idx]                \n",
    "                break\n",
    "\n",
    "        train_seqs, train_type_ids = self.process_sequences(train_lines)\n",
    "        val_seqs, val_type_ids = self.process_sequences(val_lines)\n",
    "\n",
    "        if is_training:\n",
    "            self.seqs, self.type_ids, self.annos = train_seqs, train_type_ids, train_annos\n",
    "        else:\n",
    "            self.seqs, self.type_ids, self.annos = val_seqs, val_type_ids, val_annos\n",
    "\n",
    "    def calculate_LD(self, geno):\n",
    "        geno_values = np.select(\n",
    "            [geno.iloc[:, :-2].values == 'H', geno.iloc[:, :-2].values == 'M', geno.iloc[:, :-2].values == 'L'],\n",
    "            [0, 1, 2],\n",
    "            default=-1\n",
    "        ) \n",
    "\n",
    "        a, b = geno_values[:-1], geno_values[1:]  \n",
    "        var_a, var_b = np.var(a, axis=1), np.var(b, axis=1)\n",
    "        mean_a, mean_b = np.mean(a, axis=1), np.mean(b, axis=1)\n",
    "        d = np.mean((a - mean_a[:, None]) * (b - mean_b[:, None]), axis=1)\n",
    "        \n",
    "        LD = np.where((var_a == 0) | (var_b == 0), 0, (d ** 2) / (var_a * var_b))\n",
    "        LD = np.append(LD, -1)  \n",
    "\n",
    "        chrom = sorted(set(geno['chrom']))\n",
    "        index = list(accumulate([len(geno.groupby('chrom').get_group(i)) for i in chrom])) \n",
    "        for idx in index:\n",
    "            LD[idx - 1] = -1        \n",
    "        return LD\n",
    "\n",
    "    # Updated to accept threshold parameter\n",
    "    def assign_gap_labels(self, LD, threshold):\n",
    "        return np.where(LD == -1, 'N', np.where(LD >= threshold, 'J', 'Y'))\n",
    "\n",
    "    def generate_geno_sequences(self, geno):\n",
    "        lines = []\n",
    "        for i in range(geno.shape[1] - 1):\n",
    "            geno.iloc[:, i] = geno.iloc[:, i] + geno['gap']\n",
    "            lines.append(''.join(geno.iloc[:, i]))\n",
    "        return np.stack(lines, axis=0)\n",
    "\n",
    "    def process_sequences(self, lines):\n",
    "        vocabs = {f\"{a}{b}\": i + 1 for i, (a, b) in enumerate([(\"H\", \"J\"), (\"H\", \"Y\"), (\"H\", \"N\"), (\"L\", \"J\"), (\"L\", \"Y\"), (\"L\", \"N\"), (\"M\", \"J\"), (\"M\", \"Y\"), (\"M\", \"N\")])}\n",
    "        type_vocabs = {\"J\": 1, \"Y\": 2, \"N\": 3}\n",
    "\n",
    "        seqs, type_ids = [], []\n",
    "        for raw_seq in lines:\n",
    "            seq, type_id = [], []\n",
    "            for i in range(0, len(raw_seq), 2):\n",
    "                seq.append(vocabs[raw_seq[i:i + 2]])\n",
    "                type_id.append(type_vocabs[raw_seq[i + 1]])\n",
    "            seqs.append(seq)\n",
    "            type_ids.append(type_id)\n",
    "\n",
    "        return np.asarray(seqs), np.asarray(type_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annos)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq = torch.tensor(self.seqs[index], dtype=torch.float32)\n",
    "        type_ids = torch.tensor(self.type_ids[index], dtype=torch.float32)\n",
    "        annos = torch.tensor(self.annos[index], dtype=torch.float32)\n",
    "        return seq, type_ids, annos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8adbe71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class soft_pool1d(nn.Module):\n",
    "    def __init__(self,  kernel_size=2):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride= kernel_size\n",
    "    def forward(self, x):\n",
    "        e_x = torch.sum(torch.exp(x),dim=1,keepdim=True)\n",
    "        return F.avg_pool1d(x.mul(e_x), self.kernel_size, stride=self.stride).mul_(self.kernel_size).div_(F.avg_pool1d(e_x, self.kernel_size, stride=self.stride).mul_(self.kernel_size))\n",
    "\n",
    "def lip1d(x, logit, kernel=3, stride=2, padding=1):\n",
    "    weight = logit.exp()\n",
    "    return F.avg_pool1d(x*weight, kernel, stride, padding)/F.avg_pool1d(weight, kernel, stride, padding)\n",
    "    \n",
    "class LIP(nn.Module):\n",
    "    # Updated to accept pool_size to match AttentionPool's stride/reduction behavior\n",
    "    def __init__(self, channels, pool_size=2):\n",
    "        super(LIP, self).__init__()\n",
    "        self.pool_size = pool_size\n",
    "        self.logit = nn.Sequential(\n",
    "                nn.Conv1d(channels, channels, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(channels, affine=True),\n",
    "                nn.ReLU(),\n",
    "        )\n",
    "    def init_layer(self):\n",
    "        self.logit[0].weight.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass pool_size as kernel and stride to mimic other pooling layers\n",
    "        # padding is set to pool_size // 2 to maintain consistency\n",
    "        frac = lip1d(x, self.logit(x), kernel=self.pool_size, stride=self.pool_size, padding=self.pool_size//2)\n",
    "        return frac\n",
    "\n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, dim, pool_size=2,dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.pool_size = pool_size\n",
    "        self.attn_dropout = nn.Dropout(dropout_prob)\n",
    "        self.to_attn_logits1 = nn.Conv2d(dim, dim, 1, bias=False)\n",
    "        self.to_attn_logits2 = nn.ModuleList([nn.Conv1d(dim, dim, 1, bias=False) for _ in range(pool_size)])\n",
    "        self.BN = nn.BatchNorm1d(dim)\n",
    "    def forward(self, x):\n",
    "        b, s, n = x.shape  \n",
    "        remainder = n % self.pool_size\n",
    "        needs_padding = remainder > 0      \n",
    "        if needs_padding:\n",
    "            x = F.pad(x, (0, (self.pool_size-remainder)), value=0) \n",
    "        x = x.unfold(-1,self.pool_size,self.pool_size) \n",
    "        outx = []\n",
    "        i = 0\n",
    "        for conv in self.to_attn_logits2:                \n",
    "            nx = x[:,:,:,i]\n",
    "            nx = self.BN(nx)\n",
    "            logit = conv(nx)           \n",
    "            outx.append(logit)\n",
    "            i+=1 \n",
    "        outx = torch.stack(outx, dim=-1)       \n",
    "        logits =  self.to_attn_logits1(outx)\n",
    "        logits = self.attn_dropout(logits)             \n",
    "        attn = logits.softmax(dim=-1)             \n",
    "        outs = (outx * attn).sum(dim=-1)\n",
    "        return outs\n",
    "  \n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(1.702 * x) * x\n",
    "        \n",
    "def ConvBlock(dim, dimout, kernel_size = 1,stride=2):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm1d(dim),\n",
    "        GELU(),\n",
    "        nn.Conv1d(dim,  dimout, kernel_size,stride=stride, padding = kernel_size // 2)\n",
    "    )\n",
    "\n",
    "class EBMGP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 10,\n",
    "        type_vocab_size: int = 4,\n",
    "        hidden_size: int = 64,\n",
    "        num_layers: int = 1,\n",
    "        num_attention_heads: int = 8,\n",
    "        intermediate_size: int = 256,\n",
    "        hidden_act: str = \"gelu\",\n",
    "        dropout_rate: float = 0.3,\n",
    "        pooling_type: str = \"MAP\" # New Argument\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = BertConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_hidden_layers=num_layers,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            intermediate_size=intermediate_size,\n",
    "            hidden_act=hidden_act,\n",
    "            type_vocab_size=type_vocab_size,\n",
    "            max_position_embeddings=5000, \n",
    "            dropout_rate = dropout_rate,\n",
    "        )\n",
    "\n",
    "        self.embeddings = BertEmbeddings(self.config)\n",
    "\n",
    "        # Factory method to select pooling layer\n",
    "        def get_pool(dim, size):\n",
    "            if pooling_type == \"MAP\":\n",
    "                return AttentionPool(dim, pool_size=size)\n",
    "            elif pooling_type == \"AVG\":\n",
    "                return nn.AvgPool1d(kernel_size=size, stride=size)\n",
    "            elif pooling_type == \"MAX\":\n",
    "                return nn.MaxPool1d(kernel_size=size, stride=size)\n",
    "            elif pooling_type == \"LIP\":\n",
    "                return LIP(dim, pool_size=size)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown pooling type: {pooling_type}\")\n",
    "\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            ConvBlock(hidden_size, 64,30,stride=2),\n",
    "            get_pool(64, 1), # Replaced hardcoded AttentionPool\n",
    "            nn.Dropout(dropout_rate), \n",
    "\n",
    "            nn.BatchNorm1d(64),\n",
    "            ConvBlock(64, 64, 3,stride=2),            \n",
    "            get_pool(64, 2), # Replaced hardcoded AttentionPool\n",
    "            nn.Dropout(dropout_rate), \n",
    "\n",
    "            nn.BatchNorm1d(64),\n",
    "            ConvBlock(64, 64, 30,stride=2),           \n",
    "            get_pool(64, 3), # Replaced hardcoded AttentionPool\n",
    "            nn.Dropout(dropout_rate), \n",
    "\n",
    "            nn.BatchNorm1d(64),\n",
    "            ConvBlock(64, 64, 3,stride=2),            \n",
    "            get_pool(64, 2), # Replaced hardcoded AttentionPool\n",
    "            nn.Dropout(dropout_rate), \n",
    "\n",
    "            nn.BatchNorm1d(64),\n",
    "            ConvBlock(64, 64, 30,stride=2),           \n",
    "            get_pool(64, 3), # Replaced hardcoded AttentionPool\n",
    "            nn.Dropout(dropout_rate), \n",
    "        )\n",
    "          \n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),       \n",
    "            Rearrange('b c l -> b (c l)'), \n",
    "            nn.Linear(64, 1),              \n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        token_type_ids: torch.Tensor,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "        output_hidden_states: bool = False,\n",
    "        return_dict: bool = False,\n",
    "    ):\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids.long(),\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids.long(),\n",
    "        )\n",
    "        x = embedding_output.permute(0, 2, 1) \n",
    "        x = self.convs(x)\n",
    "        logits = self.predictor(x) \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35cc86b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, loss_fn, scheduler, device):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for batch_idx, (seqs, type_ids, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        seqs, type_ids, labels = seqs.to(device), type_ids.to(device), labels.to(device)\n",
    "        predict = model(seqs, type_ids)\n",
    "        loss = loss_fn(predict, labels)\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "\n",
    "def evaluate_epoch(model, test_loader, loss_fn, pearson, device):\n",
    "    model.eval()\n",
    "    valid_loss = []\n",
    "    vp, vt = [], []\n",
    "    with torch.no_grad():\n",
    "        for seqs, type_ids, labels in test_loader:\n",
    "            seqs, type_ids, labels = seqs.to(device), type_ids.to(device), labels.to(device)\n",
    "            pred = model(seqs, type_ids)\n",
    "            val_loss = loss_fn(pred, labels)\n",
    "            valid_loss.append(val_loss.item())\n",
    "            vp.extend(pred.squeeze().cpu().numpy())\n",
    "            vt.extend(labels.squeeze().cpu().numpy())\n",
    "    return np.mean(valid_loss), vp, vt\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Added pooling_type argument\n",
    "def train_and_evaluate(trait, data_path, label_path, geno_path, device, learning_rate, epochs, seed, sel_num, ld_threshold=0.8, pooling_type=\"MAP\"):\n",
    "    setup_seed(3407)\n",
    "\n",
    "    loss_fn = nn.L1Loss()\n",
    "    bs = 32\n",
    "    \n",
    "    # Dataset init (reusing your existing modified Dataset class)\n",
    "    traindataset = DNADataset(data_path, label_path, geno_path, trait, seed, sel_num, ld_threshold=ld_threshold, is_training=True)\n",
    "    testdataset = DNADataset(data_path, label_path, geno_path, trait, seed, sel_num, ld_threshold=ld_threshold, is_training=False)\n",
    "\n",
    "    train_loader = DataLoader(traindataset, batch_size=bs, shuffle=True)\n",
    "    test_loader = DataLoader(testdataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "    print(f\"Seed {seed} | Pool {pooling_type} | Train Samples: {len(traindataset)} | Test Samples: {len(testdataset)}\")\n",
    "\n",
    "    # Initialize model with pooling_type\n",
    "    model = EBMGP(pooling_type=pooling_type).to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    steps = math.ceil(len(traindataset) / bs) * epochs - 1\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=steps)\n",
    "    pearson = PearsonCorrCoef().to(device)\n",
    "\n",
    "    corrs, RMSE, pred, obser = [], [], [], []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, loss_fn, scheduler, device)\n",
    "        \n",
    "        if epoch == epochs:\n",
    "            valid_loss, vp, vt = evaluate_epoch(model, test_loader, loss_fn, pearson, device)\n",
    "            \n",
    "            valMSE = mean_squared_error(vp, vt)\n",
    "            val_r2 = r2_score(vt, vp) \n",
    "            v_corr = pearson(torch.tensor(vp).to(device), torch.tensor(vt).to(device))\n",
    "            \n",
    "            print(f\"   > Final Train Loss: {train_loss:.4f} | Final Val Loss: {valid_loss:.4f}\")\n",
    "            print(f\"   > R2 Score: {val_r2:.4f} | Corr: {v_corr.item():.4f}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            RMSE.append(valMSE)\n",
    "            corrs.append(v_corr.item())\n",
    "            pred.extend(vp)\n",
    "            obser.extend(vt)\n",
    "            \n",
    "    return np.mean(corrs), np.mean(RMSE), pred, obser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecf009a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "RUNNING EXPERIMENT: LD THRESHOLD ABLATION\n",
      "############################################################\n",
      "\n",
      "   >>> Processing LD Threshold: 0.2 | Species: rice | Feature Count: T5000\n",
      "      Processing Trait: SW\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4055 | Final Val Loss: 0.5361\n",
      "   > R2 Score: 0.2691 | Corr: 0.5834\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3943 | Final Val Loss: 0.5843\n",
      "   > R2 Score: 0.3154 | Corr: 0.5968\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3622 | Final Val Loss: 0.6269\n",
      "   > R2 Score: 0.3619 | Corr: 0.6520\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4369 | Final Val Loss: 0.5768\n",
      "   > R2 Score: 0.1582 | Corr: 0.4640\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4087 | Final Val Loss: 0.6692\n",
      "   > R2 Score: 0.1308 | Corr: 0.4759\n",
      "----------------------------------------\n",
      "      Processing Trait: FLW\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4288 | Final Val Loss: 0.6321\n",
      "   > R2 Score: 0.1832 | Corr: 0.4408\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4492 | Final Val Loss: 0.7327\n",
      "   > R2 Score: 0.1822 | Corr: 0.4326\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4380 | Final Val Loss: 0.7120\n",
      "   > R2 Score: 0.1025 | Corr: 0.4161\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4792 | Final Val Loss: 0.5878\n",
      "   > R2 Score: 0.0870 | Corr: 0.3621\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4319 | Final Val Loss: 0.6624\n",
      "   > R2 Score: -0.1317 | Corr: 0.3404\n",
      "----------------------------------------\n",
      "      Processing Trait: AC\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3771 | Final Val Loss: 0.5020\n",
      "   > R2 Score: 0.1010 | Corr: 0.3403\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4082 | Final Val Loss: 0.5343\n",
      "   > R2 Score: 0.0730 | Corr: 0.3782\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3588 | Final Val Loss: 0.5871\n",
      "   > R2 Score: 0.0639 | Corr: 0.3679\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.3748 | Final Val Loss: 0.6294\n",
      "   > R2 Score: 0.1645 | Corr: 0.4492\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4376 | Final Val Loss: 0.5247\n",
      "   > R2 Score: 0.2411 | Corr: 0.5175\n",
      "----------------------------------------\n",
      "      Processing Trait: PH\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4533 | Final Val Loss: 0.5102\n",
      "   > R2 Score: 0.4141 | Corr: 0.6495\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4218 | Final Val Loss: 0.5814\n",
      "   > R2 Score: 0.0711 | Corr: 0.4275\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3998 | Final Val Loss: 0.6696\n",
      "   > R2 Score: 0.0418 | Corr: 0.3136\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.3945 | Final Val Loss: 0.7296\n",
      "   > R2 Score: 0.1565 | Corr: 0.4015\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4568 | Final Val Loss: 0.5422\n",
      "   > R2 Score: 0.3170 | Corr: 0.5873\n",
      "----------------------------------------\n",
      "      Processing Trait: SNPP\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.5032 | Final Val Loss: 0.6159\n",
      "   > R2 Score: 0.1043 | Corr: 0.4495\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4803 | Final Val Loss: 0.5685\n",
      "   > R2 Score: 0.1742 | Corr: 0.4479\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4770 | Final Val Loss: 0.6675\n",
      "   > R2 Score: 0.2804 | Corr: 0.5462\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4372 | Final Val Loss: 0.7605\n",
      "   > R2 Score: -0.0876 | Corr: 0.1807\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4641 | Final Val Loss: 0.5826\n",
      "   > R2 Score: 0.0510 | Corr: 0.3114\n",
      "----------------------------------------\n",
      "   [Saved] rice_T5000_LD0.2.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.4 | Species: rice | Feature Count: T5000\n",
      "      Processing Trait: SW\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3729 | Final Val Loss: 0.5561\n",
      "   > R2 Score: 0.2804 | Corr: 0.6390\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4065 | Final Val Loss: 0.5987\n",
      "   > R2 Score: 0.2970 | Corr: 0.5837\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3858 | Final Val Loss: 0.5976\n",
      "   > R2 Score: 0.3833 | Corr: 0.6430\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4231 | Final Val Loss: 0.5535\n",
      "   > R2 Score: 0.2252 | Corr: 0.5199\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.3837 | Final Val Loss: 0.6698\n",
      "   > R2 Score: 0.0434 | Corr: 0.4624\n",
      "----------------------------------------\n",
      "      Processing Trait: FLW\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4195 | Final Val Loss: 0.6181\n",
      "   > R2 Score: 0.1932 | Corr: 0.4413\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4223 | Final Val Loss: 0.7233\n",
      "   > R2 Score: 0.1917 | Corr: 0.4490\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4338 | Final Val Loss: 0.6822\n",
      "   > R2 Score: 0.1650 | Corr: 0.4450\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4641 | Final Val Loss: 0.6114\n",
      "   > R2 Score: 0.0202 | Corr: 0.3403\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4360 | Final Val Loss: 0.6558\n",
      "   > R2 Score: -0.1665 | Corr: 0.3236\n",
      "----------------------------------------\n",
      "      Processing Trait: AC\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3821 | Final Val Loss: 0.5446\n",
      "   > R2 Score: -0.0333 | Corr: 0.2566\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3751 | Final Val Loss: 0.5260\n",
      "   > R2 Score: 0.0676 | Corr: 0.3717\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3366 | Final Val Loss: 0.6012\n",
      "   > R2 Score: 0.0189 | Corr: 0.3465\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.3881 | Final Val Loss: 0.6380\n",
      "   > R2 Score: 0.1900 | Corr: 0.4610\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4388 | Final Val Loss: 0.5685\n",
      "   > R2 Score: 0.1041 | Corr: 0.4074\n",
      "----------------------------------------\n",
      "      Processing Trait: PH\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4429 | Final Val Loss: 0.5317\n",
      "   > R2 Score: 0.3754 | Corr: 0.6428\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4310 | Final Val Loss: 0.5291\n",
      "   > R2 Score: 0.1355 | Corr: 0.4321\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3992 | Final Val Loss: 0.6508\n",
      "   > R2 Score: 0.1073 | Corr: 0.3930\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4171 | Final Val Loss: 0.7283\n",
      "   > R2 Score: 0.1552 | Corr: 0.4113\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4437 | Final Val Loss: 0.5696\n",
      "   > R2 Score: 0.2802 | Corr: 0.5449\n",
      "----------------------------------------\n",
      "      Processing Trait: SNPP\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4944 | Final Val Loss: 0.5815\n",
      "   > R2 Score: 0.1483 | Corr: 0.4328\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4782 | Final Val Loss: 0.5755\n",
      "   > R2 Score: 0.1926 | Corr: 0.4611\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4779 | Final Val Loss: 0.6666\n",
      "   > R2 Score: 0.2870 | Corr: 0.5428\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4818 | Final Val Loss: 0.7781\n",
      "   > R2 Score: -0.1477 | Corr: 0.1651\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4455 | Final Val Loss: 0.6038\n",
      "   > R2 Score: -0.0466 | Corr: 0.2615\n",
      "----------------------------------------\n",
      "   [Saved] rice_T5000_LD0.4.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.6 | Species: rice | Feature Count: T5000\n",
      "      Processing Trait: SW\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3812 | Final Val Loss: 0.5674\n",
      "   > R2 Score: 0.2448 | Corr: 0.5783\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4054 | Final Val Loss: 0.6025\n",
      "   > R2 Score: 0.3020 | Corr: 0.5866\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3552 | Final Val Loss: 0.6242\n",
      "   > R2 Score: 0.3903 | Corr: 0.6605\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4240 | Final Val Loss: 0.5747\n",
      "   > R2 Score: 0.1735 | Corr: 0.5038\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.3842 | Final Val Loss: 0.6486\n",
      "   > R2 Score: 0.1430 | Corr: 0.5037\n",
      "----------------------------------------\n",
      "      Processing Trait: FLW\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4076 | Final Val Loss: 0.6325\n",
      "   > R2 Score: 0.1655 | Corr: 0.4071\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4197 | Final Val Loss: 0.7274\n",
      "   > R2 Score: 0.1969 | Corr: 0.4539\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4413 | Final Val Loss: 0.7157\n",
      "   > R2 Score: 0.1178 | Corr: 0.4048\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4786 | Final Val Loss: 0.6241\n",
      "   > R2 Score: -0.0074 | Corr: 0.3123\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4340 | Final Val Loss: 0.5863\n",
      "   > R2 Score: -0.0332 | Corr: 0.3507\n",
      "----------------------------------------\n",
      "      Processing Trait: AC\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3859 | Final Val Loss: 0.5026\n",
      "   > R2 Score: 0.0091 | Corr: 0.2672\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3881 | Final Val Loss: 0.5298\n",
      "   > R2 Score: 0.0775 | Corr: 0.3483\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3359 | Final Val Loss: 0.6046\n",
      "   > R2 Score: 0.1057 | Corr: 0.3316\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.3768 | Final Val Loss: 0.6405\n",
      "   > R2 Score: 0.1407 | Corr: 0.4410\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4112 | Final Val Loss: 0.5205\n",
      "   > R2 Score: 0.2081 | Corr: 0.5059\n",
      "----------------------------------------\n",
      "      Processing Trait: PH\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4464 | Final Val Loss: 0.5303\n",
      "   > R2 Score: 0.3280 | Corr: 0.5763\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4377 | Final Val Loss: 0.5428\n",
      "   > R2 Score: 0.1103 | Corr: 0.4041\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3748 | Final Val Loss: 0.6682\n",
      "   > R2 Score: 0.0548 | Corr: 0.3508\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.3801 | Final Val Loss: 0.7350\n",
      "   > R2 Score: 0.1612 | Corr: 0.4222\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4298 | Final Val Loss: 0.5969\n",
      "   > R2 Score: 0.2809 | Corr: 0.5770\n",
      "----------------------------------------\n",
      "      Processing Trait: SNPP\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.5085 | Final Val Loss: 0.6167\n",
      "   > R2 Score: 0.1023 | Corr: 0.3963\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4507 | Final Val Loss: 0.6012\n",
      "   > R2 Score: 0.1271 | Corr: 0.4393\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4656 | Final Val Loss: 0.7016\n",
      "   > R2 Score: 0.2360 | Corr: 0.4984\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4693 | Final Val Loss: 0.7404\n",
      "   > R2 Score: -0.0712 | Corr: 0.1776\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4687 | Final Val Loss: 0.6056\n",
      "   > R2 Score: 0.0538 | Corr: 0.3432\n",
      "----------------------------------------\n",
      "   [Saved] rice_T5000_LD0.6.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.8 | Species: rice | Feature Count: T5000\n",
      "      Processing Trait: SW\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3724 | Final Val Loss: 0.5397\n",
      "   > R2 Score: 0.2835 | Corr: 0.6204\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3516 | Final Val Loss: 0.5952\n",
      "   > R2 Score: 0.3051 | Corr: 0.5894\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3476 | Final Val Loss: 0.6161\n",
      "   > R2 Score: 0.3787 | Corr: 0.6654\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4068 | Final Val Loss: 0.5850\n",
      "   > R2 Score: 0.1296 | Corr: 0.4776\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.3710 | Final Val Loss: 0.6445\n",
      "   > R2 Score: 0.1737 | Corr: 0.4889\n",
      "----------------------------------------\n",
      "      Processing Trait: FLW\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4424 | Final Val Loss: 0.6074\n",
      "   > R2 Score: 0.1931 | Corr: 0.4507\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4265 | Final Val Loss: 0.7393\n",
      "   > R2 Score: 0.1473 | Corr: 0.4074\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4254 | Final Val Loss: 0.7076\n",
      "   > R2 Score: 0.1194 | Corr: 0.4162\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4543 | Final Val Loss: 0.5920\n",
      "   > R2 Score: 0.0736 | Corr: 0.3706\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4334 | Final Val Loss: 0.6143\n",
      "   > R2 Score: -0.0837 | Corr: 0.3433\n",
      "----------------------------------------\n",
      "      Processing Trait: AC\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3424 | Final Val Loss: 0.4814\n",
      "   > R2 Score: 0.0853 | Corr: 0.3545\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4075 | Final Val Loss: 0.5434\n",
      "   > R2 Score: 0.0559 | Corr: 0.3350\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3708 | Final Val Loss: 0.5957\n",
      "   > R2 Score: 0.0921 | Corr: 0.3637\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.3876 | Final Val Loss: 0.6318\n",
      "   > R2 Score: 0.1710 | Corr: 0.4531\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4014 | Final Val Loss: 0.5340\n",
      "   > R2 Score: 0.2260 | Corr: 0.5090\n",
      "----------------------------------------\n",
      "      Processing Trait: PH\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4274 | Final Val Loss: 0.5713\n",
      "   > R2 Score: 0.1828 | Corr: 0.4966\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4226 | Final Val Loss: 0.5792\n",
      "   > R2 Score: 0.0220 | Corr: 0.3955\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3934 | Final Val Loss: 0.6470\n",
      "   > R2 Score: 0.1225 | Corr: 0.3889\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.3828 | Final Val Loss: 0.7181\n",
      "   > R2 Score: 0.1873 | Corr: 0.4452\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4426 | Final Val Loss: 0.5670\n",
      "   > R2 Score: 0.2885 | Corr: 0.5651\n",
      "----------------------------------------\n",
      "      Processing Trait: SNPP\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4618 | Final Val Loss: 0.6299\n",
      "   > R2 Score: 0.0641 | Corr: 0.4010\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4557 | Final Val Loss: 0.5830\n",
      "   > R2 Score: 0.0972 | Corr: 0.4031\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4666 | Final Val Loss: 0.6775\n",
      "   > R2 Score: 0.2841 | Corr: 0.5449\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4242 | Final Val Loss: 0.7559\n",
      "   > R2 Score: -0.0890 | Corr: 0.1786\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4366 | Final Val Loss: 0.5833\n",
      "   > R2 Score: 0.0482 | Corr: 0.3084\n",
      "----------------------------------------\n",
      "   [Saved] rice_T5000_LD0.8.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.2 | Species: sorghum | Feature Count: T5000\n",
      "      Processing Trait: HT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   > Final Train Loss: 0.2737 | Final Val Loss: 0.6440\n",
      "   > R2 Score: 0.1950 | Corr: 0.5240\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2516 | Final Val Loss: 0.6915\n",
      "   > R2 Score: 0.2032 | Corr: 0.4957\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2589 | Final Val Loss: 0.6334\n",
      "   > R2 Score: 0.3030 | Corr: 0.5549\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2572 | Final Val Loss: 0.6780\n",
      "   > R2 Score: 0.2810 | Corr: 0.5360\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2803 | Final Val Loss: 0.6569\n",
      "   > R2 Score: 0.2958 | Corr: 0.5554\n",
      "----------------------------------------\n",
      "      Processing Trait: MO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   > Final Train Loss: 0.2968 | Final Val Loss: 0.8285\n",
      "   > R2 Score: 0.0062 | Corr: 0.2331\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2762 | Final Val Loss: 0.6452\n",
      "   > R2 Score: 0.0754 | Corr: 0.3613\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2986 | Final Val Loss: 0.6832\n",
      "   > R2 Score: 0.0226 | Corr: 0.3513\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.3209 | Final Val Loss: 0.7226\n",
      "   > R2 Score: -0.0199 | Corr: 0.3411\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2948 | Final Val Loss: 0.7777\n",
      "   > R2 Score: 0.2315 | Corr: 0.4929\n",
      "----------------------------------------\n",
      "      Processing Trait: YLD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   > Final Train Loss: 0.2653 | Final Val Loss: 0.6458\n",
      "   > R2 Score: 0.3617 | Corr: 0.6023\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2670 | Final Val Loss: 0.7250\n",
      "   > R2 Score: 0.2407 | Corr: 0.5218\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2713 | Final Val Loss: 0.6867\n",
      "   > R2 Score: 0.2743 | Corr: 0.5746\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2740 | Final Val Loss: 0.6153\n",
      "   > R2 Score: 0.2841 | Corr: 0.5800\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2482 | Final Val Loss: 0.6831\n",
      "   > R2 Score: 0.2397 | Corr: 0.5167\n",
      "----------------------------------------\n",
      "   [Saved] sorghum_T5000_LD0.2.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.4 | Species: sorghum | Feature Count: T5000\n",
      "      Processing Trait: HT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   > Final Train Loss: 0.2578 | Final Val Loss: 0.6517\n",
      "   > R2 Score: 0.1835 | Corr: 0.4945\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2400 | Final Val Loss: 0.6868\n",
      "   > R2 Score: 0.2002 | Corr: 0.5247\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2709 | Final Val Loss: 0.6170\n",
      "   > R2 Score: 0.3120 | Corr: 0.5765\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2691 | Final Val Loss: 0.7384\n",
      "   > R2 Score: 0.1406 | Corr: 0.4640\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2712 | Final Val Loss: 0.6476\n",
      "   > R2 Score: 0.3349 | Corr: 0.5839\n",
      "----------------------------------------\n",
      "      Processing Trait: MO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   > Final Train Loss: 0.2944 | Final Val Loss: 0.8027\n",
      "   > R2 Score: 0.0674 | Corr: 0.2972\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2864 | Final Val Loss: 0.6410\n",
      "   > R2 Score: 0.1213 | Corr: 0.3884\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2777 | Final Val Loss: 0.6794\n",
      "   > R2 Score: 0.0453 | Corr: 0.3548\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.3131 | Final Val Loss: 0.7374\n",
      "   > R2 Score: -0.0645 | Corr: 0.2715\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.3021 | Final Val Loss: 0.7556\n",
      "   > R2 Score: 0.2373 | Corr: 0.4923\n",
      "----------------------------------------\n",
      "      Processing Trait: YLD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   > Final Train Loss: 0.2659 | Final Val Loss: 0.6539\n",
      "   > R2 Score: 0.3008 | Corr: 0.5929\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2675 | Final Val Loss: 0.7075\n",
      "   > R2 Score: 0.3099 | Corr: 0.5647\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2715 | Final Val Loss: 0.7068\n",
      "   > R2 Score: 0.2375 | Corr: 0.5711\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2582 | Final Val Loss: 0.5815\n",
      "   > R2 Score: 0.3258 | Corr: 0.5981\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2623 | Final Val Loss: 0.6632\n",
      "   > R2 Score: 0.2629 | Corr: 0.5472\n",
      "----------------------------------------\n",
      "   [Saved] sorghum_T5000_LD0.4.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.6 | Species: sorghum | Feature Count: T5000\n",
      "      Processing Trait: HT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   > Final Train Loss: 0.2547 | Final Val Loss: 0.6317\n",
      "   > R2 Score: 0.2561 | Corr: 0.5284\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2428 | Final Val Loss: 0.6837\n",
      "   > R2 Score: 0.1990 | Corr: 0.5156\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2729 | Final Val Loss: 0.5780\n",
      "   > R2 Score: 0.3649 | Corr: 0.6098\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2376 | Final Val Loss: 0.7254\n",
      "   > R2 Score: 0.2104 | Corr: 0.4999\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2593 | Final Val Loss: 0.6552\n",
      "   > R2 Score: 0.2753 | Corr: 0.5375\n",
      "----------------------------------------\n",
      "      Processing Trait: MO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   > Final Train Loss: 0.2915 | Final Val Loss: 0.8126\n",
      "   > R2 Score: 0.0201 | Corr: 0.2540\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2648 | Final Val Loss: 0.6653\n",
      "   > R2 Score: 0.1257 | Corr: 0.4637\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2765 | Final Val Loss: 0.6626\n",
      "   > R2 Score: 0.0968 | Corr: 0.3960\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.3085 | Final Val Loss: 0.7352\n",
      "   > R2 Score: 0.0038 | Corr: 0.3263\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2814 | Final Val Loss: 0.8344\n",
      "   > R2 Score: 0.1333 | Corr: 0.4261\n",
      "----------------------------------------\n",
      "      Processing Trait: YLD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   > Final Train Loss: 0.2789 | Final Val Loss: 0.6467\n",
      "   > R2 Score: 0.3415 | Corr: 0.5857\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2766 | Final Val Loss: 0.7121\n",
      "   > R2 Score: 0.3061 | Corr: 0.5591\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2630 | Final Val Loss: 0.7009\n",
      "   > R2 Score: 0.2515 | Corr: 0.5739\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2728 | Final Val Loss: 0.6194\n",
      "   > R2 Score: 0.2402 | Corr: 0.6011\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2695 | Final Val Loss: 0.6898\n",
      "   > R2 Score: 0.2098 | Corr: 0.5263\n",
      "----------------------------------------\n",
      "   [Saved] sorghum_T5000_LD0.6.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.8 | Species: sorghum | Feature Count: T5000\n",
      "      Processing Trait: HT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   > Final Train Loss: 0.2434 | Final Val Loss: 0.6504\n",
      "   > R2 Score: 0.1906 | Corr: 0.4949\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2559 | Final Val Loss: 0.6951\n",
      "   > R2 Score: 0.2177 | Corr: 0.5080\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2781 | Final Val Loss: 0.6064\n",
      "   > R2 Score: 0.3069 | Corr: 0.5619\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2585 | Final Val Loss: 0.6984\n",
      "   > R2 Score: 0.2637 | Corr: 0.5356\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2626 | Final Val Loss: 0.6697\n",
      "   > R2 Score: 0.2687 | Corr: 0.5562\n",
      "----------------------------------------\n",
      "      Processing Trait: MO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   > Final Train Loss: 0.2850 | Final Val Loss: 0.8147\n",
      "   > R2 Score: 0.0397 | Corr: 0.2826\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2651 | Final Val Loss: 0.6188\n",
      "   > R2 Score: 0.1304 | Corr: 0.3882\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2982 | Final Val Loss: 0.6969\n",
      "   > R2 Score: 0.0072 | Corr: 0.3947\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.3052 | Final Val Loss: 0.7501\n",
      "   > R2 Score: -0.0674 | Corr: 0.3254\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2825 | Final Val Loss: 0.7743\n",
      "   > R2 Score: 0.2086 | Corr: 0.4826\n",
      "----------------------------------------\n",
      "      Processing Trait: YLD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0 | Pool MAP | Train Samples: 360 | Test Samples: 91\n",
      "   > Final Train Loss: 0.2855 | Final Val Loss: 0.6863\n",
      "   > R2 Score: 0.2661 | Corr: 0.5414\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2506 | Final Val Loss: 0.7172\n",
      "   > R2 Score: 0.2881 | Corr: 0.5424\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2500 | Final Val Loss: 0.6652\n",
      "   > R2 Score: 0.3379 | Corr: 0.5895\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2689 | Final Val Loss: 0.6228\n",
      "   > R2 Score: 0.2761 | Corr: 0.5797\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n",
      "/tmp/ipykernel_833898/2554090289.py:6: DtypeWarning: Columns (452) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Rawgeno = pd.read_csv(geno_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 4 | Pool MAP | Train Samples: 361 | Test Samples: 90\n",
      "   > Final Train Loss: 0.2503 | Final Val Loss: 0.6354\n",
      "   > R2 Score: 0.2922 | Corr: 0.5496\n",
      "----------------------------------------\n",
      "   [Saved] sorghum_T5000_LD0.8.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.2 | Species: bulls | Feature Count: T5000\n",
      "      Processing Trait: MS\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2514 | Final Val Loss: 0.7584\n",
      "   > R2 Score: 0.0898 | Corr: 0.3803\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.3017 | Final Val Loss: 0.6725\n",
      "   > R2 Score: 0.2007 | Corr: 0.4757\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2543 | Final Val Loss: 0.7272\n",
      "   > R2 Score: 0.0968 | Corr: 0.3859\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2651 | Final Val Loss: 0.7071\n",
      "   > R2 Score: 0.1140 | Corr: 0.3941\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2614 | Final Val Loss: 0.7425\n",
      "   > R2 Score: 0.0819 | Corr: 0.3455\n",
      "----------------------------------------\n",
      "      Processing Trait: NMSP\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2859 | Final Val Loss: 0.5808\n",
      "   > R2 Score: 0.3092 | Corr: 0.5814\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2904 | Final Val Loss: 0.6621\n",
      "   > R2 Score: 0.2520 | Corr: 0.5537\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2557 | Final Val Loss: 0.6106\n",
      "   > R2 Score: 0.3087 | Corr: 0.5727\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2737 | Final Val Loss: 0.6848\n",
      "   > R2 Score: 0.1286 | Corr: 0.4138\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2659 | Final Val Loss: 0.7349\n",
      "   > R2 Score: 0.0966 | Corr: 0.3477\n",
      "----------------------------------------\n",
      "      Processing Trait: VE\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.3058 | Final Val Loss: 0.6705\n",
      "   > R2 Score: 0.1370 | Corr: 0.4025\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2897 | Final Val Loss: 0.7173\n",
      "   > R2 Score: 0.1451 | Corr: 0.4551\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2931 | Final Val Loss: 0.7441\n",
      "   > R2 Score: 0.0677 | Corr: 0.3753\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.3028 | Final Val Loss: 0.7525\n",
      "   > R2 Score: 0.1104 | Corr: 0.4007\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2920 | Final Val Loss: 0.6871\n",
      "   > R2 Score: 0.0742 | Corr: 0.3472\n",
      "----------------------------------------\n",
      "   [Saved] bulls_T5000_LD0.2.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.4 | Species: bulls | Feature Count: T5000\n",
      "      Processing Trait: MS\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2498 | Final Val Loss: 0.7360\n",
      "   > R2 Score: 0.1805 | Corr: 0.4487\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2869 | Final Val Loss: 0.6601\n",
      "   > R2 Score: 0.2009 | Corr: 0.4617\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2542 | Final Val Loss: 0.7217\n",
      "   > R2 Score: 0.1184 | Corr: 0.4011\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2632 | Final Val Loss: 0.6813\n",
      "   > R2 Score: 0.1629 | Corr: 0.4408\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2391 | Final Val Loss: 0.7642\n",
      "   > R2 Score: 0.0365 | Corr: 0.3191\n",
      "----------------------------------------\n",
      "      Processing Trait: NMSP\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2823 | Final Val Loss: 0.6110\n",
      "   > R2 Score: 0.2522 | Corr: 0.5519\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2882 | Final Val Loss: 0.6847\n",
      "   > R2 Score: 0.1974 | Corr: 0.5040\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2401 | Final Val Loss: 0.6273\n",
      "   > R2 Score: 0.2490 | Corr: 0.5457\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2899 | Final Val Loss: 0.6866\n",
      "   > R2 Score: 0.1206 | Corr: 0.4135\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2751 | Final Val Loss: 0.7314\n",
      "   > R2 Score: 0.0706 | Corr: 0.3629\n",
      "----------------------------------------\n",
      "      Processing Trait: VE\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.3055 | Final Val Loss: 0.7051\n",
      "   > R2 Score: 0.1269 | Corr: 0.3903\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2917 | Final Val Loss: 0.7189\n",
      "   > R2 Score: 0.1308 | Corr: 0.3980\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2843 | Final Val Loss: 0.6977\n",
      "   > R2 Score: 0.1797 | Corr: 0.4466\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2896 | Final Val Loss: 0.7401\n",
      "   > R2 Score: 0.1425 | Corr: 0.4150\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2821 | Final Val Loss: 0.6695\n",
      "   > R2 Score: 0.0923 | Corr: 0.3779\n",
      "----------------------------------------\n",
      "   [Saved] bulls_T5000_LD0.4.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.6 | Species: bulls | Feature Count: T5000\n",
      "      Processing Trait: MS\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2453 | Final Val Loss: 0.7412\n",
      "   > R2 Score: 0.1681 | Corr: 0.4373\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2966 | Final Val Loss: 0.6596\n",
      "   > R2 Score: 0.2254 | Corr: 0.4864\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2466 | Final Val Loss: 0.7023\n",
      "   > R2 Score: 0.1593 | Corr: 0.4328\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2634 | Final Val Loss: 0.6745\n",
      "   > R2 Score: 0.1474 | Corr: 0.4147\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2640 | Final Val Loss: 0.7976\n",
      "   > R2 Score: 0.0063 | Corr: 0.2641\n",
      "----------------------------------------\n",
      "      Processing Trait: NMSP\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2730 | Final Val Loss: 0.6186\n",
      "   > R2 Score: 0.2431 | Corr: 0.5483\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2964 | Final Val Loss: 0.6833\n",
      "   > R2 Score: 0.2001 | Corr: 0.5007\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2462 | Final Val Loss: 0.6144\n",
      "   > R2 Score: 0.2798 | Corr: 0.5621\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2822 | Final Val Loss: 0.6658\n",
      "   > R2 Score: 0.2207 | Corr: 0.4929\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2766 | Final Val Loss: 0.7540\n",
      "   > R2 Score: 0.0420 | Corr: 0.3032\n",
      "----------------------------------------\n",
      "      Processing Trait: VE\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2870 | Final Val Loss: 0.6879\n",
      "   > R2 Score: 0.1458 | Corr: 0.4134\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2910 | Final Val Loss: 0.7018\n",
      "   > R2 Score: 0.1825 | Corr: 0.4572\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2932 | Final Val Loss: 0.7222\n",
      "   > R2 Score: 0.1036 | Corr: 0.4143\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2911 | Final Val Loss: 0.7342\n",
      "   > R2 Score: 0.1753 | Corr: 0.4341\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2932 | Final Val Loss: 0.6800\n",
      "   > R2 Score: 0.0763 | Corr: 0.3431\n",
      "----------------------------------------\n",
      "   [Saved] bulls_T5000_LD0.6.json\n",
      "\n",
      "   >>> Processing LD Threshold: 0.8 | Species: bulls | Feature Count: T5000\n",
      "      Processing Trait: MS\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2477 | Final Val Loss: 0.7530\n",
      "   > R2 Score: 0.1554 | Corr: 0.4396\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2803 | Final Val Loss: 0.6810\n",
      "   > R2 Score: 0.1922 | Corr: 0.4518\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2423 | Final Val Loss: 0.7138\n",
      "   > R2 Score: 0.1089 | Corr: 0.3862\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2594 | Final Val Loss: 0.6965\n",
      "   > R2 Score: 0.1054 | Corr: 0.3911\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2531 | Final Val Loss: 0.7844\n",
      "   > R2 Score: 0.0123 | Corr: 0.2735\n",
      "----------------------------------------\n",
      "      Processing Trait: NMSP\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2740 | Final Val Loss: 0.6074\n",
      "   > R2 Score: 0.2713 | Corr: 0.5734\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2822 | Final Val Loss: 0.6843\n",
      "   > R2 Score: 0.2056 | Corr: 0.5340\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2383 | Final Val Loss: 0.6045\n",
      "   > R2 Score: 0.2743 | Corr: 0.5618\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2869 | Final Val Loss: 0.6828\n",
      "   > R2 Score: 0.1725 | Corr: 0.4378\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2814 | Final Val Loss: 0.7386\n",
      "   > R2 Score: 0.0521 | Corr: 0.3359\n",
      "----------------------------------------\n",
      "      Processing Trait: VE\n",
      "Seed 0 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2847 | Final Val Loss: 0.6591\n",
      "   > R2 Score: 0.1787 | Corr: 0.4397\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2900 | Final Val Loss: 0.7268\n",
      "   > R2 Score: 0.1126 | Corr: 0.4532\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 1206 | Test Samples: 302\n",
      "   > Final Train Loss: 0.2917 | Final Val Loss: 0.7105\n",
      "   > R2 Score: 0.1270 | Corr: 0.4076\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2909 | Final Val Loss: 0.7651\n",
      "   > R2 Score: 0.1173 | Corr: 0.3915\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 1207 | Test Samples: 301\n",
      "   > Final Train Loss: 0.2839 | Final Val Loss: 0.6536\n",
      "   > R2 Score: 0.1094 | Corr: 0.3713\n",
      "----------------------------------------\n",
      "   [Saved] bulls_T5000_LD0.8.json\n",
      "\n",
      "############################################################\n",
      "RUNNING EXPERIMENT: POOLING STRATEGY ABLATION\n",
      "############################################################\n",
      "\n",
      "   >>> Processing Pooling Type: MAP | Species: rice | Feature Count: T5000\n",
      "      Processing Trait: SW\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3724 | Final Val Loss: 0.5397\n",
      "   > R2 Score: 0.2835 | Corr: 0.6204\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3516 | Final Val Loss: 0.5952\n",
      "   > R2 Score: 0.3051 | Corr: 0.5894\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3476 | Final Val Loss: 0.6161\n",
      "   > R2 Score: 0.3787 | Corr: 0.6654\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4068 | Final Val Loss: 0.5850\n",
      "   > R2 Score: 0.1296 | Corr: 0.4776\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.3710 | Final Val Loss: 0.6445\n",
      "   > R2 Score: 0.1737 | Corr: 0.4889\n",
      "----------------------------------------\n",
      "      Processing Trait: FLW\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4424 | Final Val Loss: 0.6074\n",
      "   > R2 Score: 0.1931 | Corr: 0.4507\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4265 | Final Val Loss: 0.7393\n",
      "   > R2 Score: 0.1473 | Corr: 0.4074\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4254 | Final Val Loss: 0.7076\n",
      "   > R2 Score: 0.1194 | Corr: 0.4162\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4543 | Final Val Loss: 0.5920\n",
      "   > R2 Score: 0.0736 | Corr: 0.3706\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4334 | Final Val Loss: 0.6143\n",
      "   > R2 Score: -0.0837 | Corr: 0.3433\n",
      "----------------------------------------\n",
      "      Processing Trait: AC\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3424 | Final Val Loss: 0.4814\n",
      "   > R2 Score: 0.0853 | Corr: 0.3545\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4075 | Final Val Loss: 0.5434\n",
      "   > R2 Score: 0.0559 | Corr: 0.3350\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3708 | Final Val Loss: 0.5957\n",
      "   > R2 Score: 0.0921 | Corr: 0.3637\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.3876 | Final Val Loss: 0.6318\n",
      "   > R2 Score: 0.1710 | Corr: 0.4531\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4014 | Final Val Loss: 0.5340\n",
      "   > R2 Score: 0.2260 | Corr: 0.5090\n",
      "----------------------------------------\n",
      "      Processing Trait: PH\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4274 | Final Val Loss: 0.5713\n",
      "   > R2 Score: 0.1828 | Corr: 0.4966\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4226 | Final Val Loss: 0.5792\n",
      "   > R2 Score: 0.0220 | Corr: 0.3955\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.3934 | Final Val Loss: 0.6470\n",
      "   > R2 Score: 0.1225 | Corr: 0.3889\n",
      "----------------------------------------\n",
      "Seed 3 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.3828 | Final Val Loss: 0.7181\n",
      "   > R2 Score: 0.1873 | Corr: 0.4452\n",
      "----------------------------------------\n",
      "Seed 4 | Pool MAP | Train Samples: 331 | Test Samples: 82\n",
      "   > Final Train Loss: 0.4426 | Final Val Loss: 0.5670\n",
      "   > R2 Score: 0.2885 | Corr: 0.5651\n",
      "----------------------------------------\n",
      "      Processing Trait: SNPP\n",
      "Seed 0 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4618 | Final Val Loss: 0.6299\n",
      "   > R2 Score: 0.0641 | Corr: 0.4010\n",
      "----------------------------------------\n",
      "Seed 1 | Pool MAP | Train Samples: 330 | Test Samples: 83\n",
      "   > Final Train Loss: 0.4557 | Final Val Loss: 0.5830\n",
      "   > R2 Score: 0.0972 | Corr: 0.4031\n",
      "----------------------------------------\n",
      "Seed 2 | Pool MAP | Train Samples: 330 | Test Samples: 83\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 165\u001b[39m\n\u001b[32m    162\u001b[39m     run_pooling_ablation()\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 162\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# You can comment out one of these lines if you only want to run one experiment\u001b[39;00m\n\u001b[32m    161\u001b[39m     run_ld_ablation()\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[43mrun_pooling_ablation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 139\u001b[39m, in \u001b[36mrun_pooling_ablation\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m5\u001b[39m):\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    138\u001b[39m         \u001b[38;5;66;03m# Uses default ld_threshold=0.8\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m         fold_corrs, fold_RMSE, _, _ = \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrait_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabel_path\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgeno_path\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msel_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m            \u001b[49m\u001b[43mld_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooling_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp_type\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m         corrs.append(fold_corrs)\n\u001b[32m    145\u001b[39m         RMSE.append(fold_RMSE)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m(trait, data_path, label_path, geno_path, device, learning_rate, epochs, seed, sel_num, ld_threshold, pooling_type)\u001b[39m\n\u001b[32m     65\u001b[39m corrs, RMSE, pred, obser = [], [], [], []\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m epoch == epochs:\n\u001b[32m     70\u001b[39m         valid_loss, vp, vt = evaluate_epoch(model, test_loader, loss_fn, pearson, device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, train_loader, optimizer, loss_fn, scheduler, device)\u001b[39m\n\u001b[32m     14\u001b[39m optimizer.zero_grad()\n\u001b[32m     15\u001b[39m seqs, type_ids, labels = seqs.to(device), type_ids.to(device), labels.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m predict = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m loss = loss_fn(predict, labels)\n\u001b[32m     18\u001b[39m train_loss.append(loss.item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bioinf/lib/python3.11/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bioinf/lib/python3.11/site-packages/torch/nn/modules/module.py:1789\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1784\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1785\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1787\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1788\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1792\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 164\u001b[39m, in \u001b[36mEBMGP.forward\u001b[39m\u001b[34m(self, input_ids, token_type_ids, position_ids, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    158\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(\n\u001b[32m    159\u001b[39m     input_ids=input_ids.long(),\n\u001b[32m    160\u001b[39m     position_ids=position_ids,\n\u001b[32m    161\u001b[39m     token_type_ids=token_type_ids.long(),\n\u001b[32m    162\u001b[39m )\n\u001b[32m    163\u001b[39m x = embedding_output.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m) \n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.predictor(x) \n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bioinf/lib/python3.11/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bioinf/lib/python3.11/site-packages/torch/nn/modules/module.py:1789\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1784\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1785\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1787\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1788\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1792\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bioinf/lib/python3.11/site-packages/torch/nn/modules/container.py:253\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bioinf/lib/python3.11/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bioinf/lib/python3.11/site-packages/torch/nn/modules/module.py:1789\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1784\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1785\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1787\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1788\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1792\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mAttentionPool.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_attn_logits2:                \n\u001b[32m     51\u001b[39m     nx = x[:,:,:,i]\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     nx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mBN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     logit = conv(nx)           \n\u001b[32m     54\u001b[39m     outx.append(logit)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bioinf/lib/python3.11/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bioinf/lib/python3.11/site-packages/torch/nn/modules/module.py:1789\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1784\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1785\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1787\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1788\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1792\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bioinf/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:194\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    187\u001b[39m     bn_training = (\u001b[38;5;28mself\u001b[39m.running_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.running_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    189\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_mean\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bioinf/lib/python3.11/site-packages/torch/nn/functional.py:2846\u001b[39m, in \u001b[36mbatch_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[39m\n\u001b[32m   2843\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eps <= \u001b[32m0.0\u001b[39m:\n\u001b[32m   2844\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbatch_norm eps must be positive, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2846\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2847\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2848\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2849\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2850\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2851\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2852\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2853\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2854\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2855\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2856\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#ABLATION EXPERIMENTS PIPELINE\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# --- Configuration ---\n",
    "learning_rate = 0.0005\n",
    "epochs = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Optimal Feature Counts per Species (Used for both ablations)\n",
    "species_optimal_counts = {\n",
    "    \"rice\": 5000,\n",
    "    \"sorghum\": 5000,\n",
    "    # \"soybean\": 3000,\n",
    "    \"bulls\": 5000\n",
    "}\n",
    "\n",
    "species_config = {\n",
    "    \"rice\": {\n",
    "        \"label_path\": \"./data/rice_pheno.csv\",\n",
    "        \"geno_path\": \"./data/ricerawgeno.csv\",\n",
    "        \"traits\": ['SW', 'FLW', 'AC', 'PH', 'SNPP']\n",
    "    },\n",
    "    \"sorghum\": {\n",
    "        \"label_path\": \"./data/sorghum_pheno.csv\",\n",
    "        \"geno_path\": \"./data/sorghumrawgeno.csv\",\n",
    "        \"traits\": ['HT', 'MO', 'YLD']\n",
    "    },\n",
    "    # \"soybean\": {\n",
    "    #     \"label_path\": \"./data/soybean_pheno.csv\",\n",
    "    #     \"geno_path\": \"./data/soybeanrawgeno.csv\",\n",
    "    #     \"traits\": ['protein', 'Steartic', 'R8', 'SdWgt', 'Yield']\n",
    "    # },\n",
    "    \"bulls\": {\n",
    "        \"label_path\": \"./data/bulls_pheno.csv\",\n",
    "        \"geno_path\": \"./data/bullsrawgeno.csv\",\n",
    "        \"traits\": ['MS', 'NMSP', 'VE']\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Experiment 1: LD Threshold Ablation ---\n",
    "def run_ld_ablation():\n",
    "    print(\"\\n\" + \"#\"*60)\n",
    "    print(\"RUNNING EXPERIMENT: LD THRESHOLD ABLATION\")\n",
    "    print(\"#\"*60)\n",
    "    \n",
    "    ld_thresholds = [0.2, 0.4, 0.6, 0.8]\n",
    "    \n",
    "    for species, optimal_count in species_optimal_counts.items():\n",
    "        config = species_config[species]\n",
    "        sel_num = optimal_count\n",
    "        T_folder = f\"T{optimal_count}\"\n",
    "        \n",
    "        for ld_thresh in ld_thresholds:\n",
    "            print(f\"\\n   >>> Processing LD Threshold: {ld_thresh} | Species: {species} | Feature Count: {T_folder}\")\n",
    "            \n",
    "            results_data = {\n",
    "                \"species\": species,\n",
    "                \"feature_count\": optimal_count,\n",
    "                \"ld_threshold\": ld_thresh,\n",
    "                \"pooling_type\": \"MAP\", # Default for LD ablation\n",
    "                \"mean_corrs\": {},\n",
    "                \"mean_RMSE\": {}\n",
    "            }\n",
    "\n",
    "            for trait_idx, trait_name in enumerate(config[\"traits\"]):\n",
    "                data_path = f\"./EN/{species}/{T_folder}{trait_name}\"\n",
    "                \n",
    "                if not os.path.exists(f\"{data_path}0.csv\"):\n",
    "                    continue\n",
    "\n",
    "                corrs, RMSE = [], []\n",
    "                print(f\"      Processing Trait: {trait_name}\")\n",
    "                \n",
    "                for seed in range(5):\n",
    "                    try:\n",
    "                        # Uses default pooling_type=\"MAP\"\n",
    "                        fold_corrs, fold_RMSE, _, _ = train_and_evaluate(\n",
    "                            trait_idx, data_path, config[\"label_path\"], config[\"geno_path\"], \n",
    "                            device, learning_rate, epochs, seed, sel_num,\n",
    "                            ld_threshold=ld_thresh, pooling_type=\"MAP\"\n",
    "                        )\n",
    "                        corrs.append(fold_corrs)\n",
    "                        RMSE.append(fold_RMSE)\n",
    "                    except Exception as e:\n",
    "                        print(f\"         [Error] Seed {seed}: {e}\")\n",
    "\n",
    "                if corrs:\n",
    "                    results_data[\"mean_corrs\"][trait_name] = float(np.mean(corrs))\n",
    "                    results_data[\"mean_RMSE\"][trait_name] = float(np.mean(RMSE))\n",
    "\n",
    "            # Save JSON\n",
    "            json_filename = f\"{species}_{T_folder}_LD{ld_thresh}.json\"\n",
    "            with open(json_filename, \"w\") as f:\n",
    "                json.dump(results_data, f, indent=4)\n",
    "            print(f\"   [Saved] {json_filename}\")\n",
    "\n",
    "\n",
    "# --- Experiment 2: Pooling Strategy Ablation ---\n",
    "def run_pooling_ablation():\n",
    "    print(\"\\n\" + \"#\"*60)\n",
    "    print(\"RUNNING EXPERIMENT: POOLING STRATEGY ABLATION\")\n",
    "    print(\"#\"*60)\n",
    "    \n",
    "    pooling_types = [\"MAP\", \"AVG\", \"MAX\", \"LIP\"]\n",
    "    \n",
    "    for species, optimal_count in species_optimal_counts.items():\n",
    "        config = species_config[species]\n",
    "        sel_num = optimal_count\n",
    "        T_folder = f\"T{optimal_count}\"\n",
    "        \n",
    "        for p_type in pooling_types:\n",
    "            print(f\"\\n   >>> Processing Pooling Type: {p_type} | Species: {species} | Feature Count: {T_folder}\")\n",
    "            \n",
    "            results_data = {\n",
    "                \"species\": species,\n",
    "                \"feature_count\": optimal_count,\n",
    "                \"ld_threshold\": 0.8, # Default for Pooling ablation\n",
    "                \"pooling_type\": p_type,\n",
    "                \"mean_corrs\": {},\n",
    "                \"mean_RMSE\": {}\n",
    "            }\n",
    "\n",
    "            for trait_idx, trait_name in enumerate(config[\"traits\"]):\n",
    "                data_path = f\"./EN/{species}/{T_folder}{trait_name}\"\n",
    "                \n",
    "                if not os.path.exists(f\"{data_path}0.csv\"):\n",
    "                    continue\n",
    "\n",
    "                corrs, RMSE = [], []\n",
    "                print(f\"      Processing Trait: {trait_name}\")\n",
    "                \n",
    "                for seed in range(5):\n",
    "                    try:\n",
    "                        # Uses default ld_threshold=0.8\n",
    "                        fold_corrs, fold_RMSE, _, _ = train_and_evaluate(\n",
    "                            trait_idx, data_path, config[\"label_path\"], config[\"geno_path\"], \n",
    "                            device, learning_rate, epochs, seed, sel_num,\n",
    "                            ld_threshold=0.8, pooling_type=p_type\n",
    "                        )\n",
    "                        corrs.append(fold_corrs)\n",
    "                        RMSE.append(fold_RMSE)\n",
    "                    except Exception as e:\n",
    "                        print(f\"         [Error] Seed {seed}: {e}\")\n",
    "\n",
    "                if corrs:\n",
    "                    results_data[\"mean_corrs\"][trait_name] = float(np.mean(corrs))\n",
    "                    results_data[\"mean_RMSE\"][trait_name] = float(np.mean(RMSE))\n",
    "\n",
    "            # Save JSON\n",
    "            json_filename = f\"{species}_{T_folder}_{p_type}.json\"\n",
    "            with open(json_filename, \"w\") as f:\n",
    "                json.dump(results_data, f, indent=4)\n",
    "            print(f\"   [Saved] {json_filename}\")\n",
    "\n",
    "def main():\n",
    "    # You can comment out one of these lines if you only want to run one experiment\n",
    "    run_ld_ablation()\n",
    "    run_pooling_ablation()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b0b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Benchy no know\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, BayesianRidge\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATA LOADER FOR NUMERICAL MODELS\n",
    "# ==========================================\n",
    "class BenchmarkDataLoader:\n",
    "    \"\"\"\n",
    "    Extracts NUMERICAL genotype matrices (0,1,2) matching the exact \n",
    "    feature selection and CV splits of the Deep Learning pipeline.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, label_path, geno_path, trait_idx, seed, sel_num):\n",
    "        # 1. Load Pre-selected Features (Elastic Net Output)\n",
    "        # We must load the exact same features used in the DL pipeline\n",
    "        cs = pd.read_csv(f\"{data_path}{seed}.csv\").sort_values(by='cs', ascending=False)\n",
    "        Top = sorted(cs.index[:sel_num])\n",
    "        \n",
    "        # 2. Load Raw Genotype Data\n",
    "        Rawgeno = pd.read_csv(geno_path, low_memory=False)\n",
    "        # Drop the first row (SNP index row) usually present in your format\n",
    "        Rawgeno = Rawgeno.iloc[1:].reset_index(drop=True)\n",
    "        \n",
    "        # Filter for valid indices\n",
    "        Top = [i for i in Top if i in Rawgeno.index]\n",
    "        geno = Rawgeno.loc[Top].copy()\n",
    "\n",
    "        # 3. Convert H/M/L to 0/1/2 (Numerical Encoding)\n",
    "        # Note: Your DL pipeline does this inside 'calculate_LD'. \n",
    "        # We assume columns before the last 2 are samples.\n",
    "        geno_vals = geno.iloc[:, :-2].values\n",
    "        \n",
    "        # Vectorized replacement for speed\n",
    "        X_num = np.select(\n",
    "            [geno_vals == 'H', geno_vals == 'M', geno_vals == 'L'],\n",
    "            [0, 1, 2],\n",
    "            default=1 # Default to heterozygous/mean if missing/unknown\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        # Transpose: (SNPs, Samples) -> (Samples, SNPs)\n",
    "        self.X = X_num.T\n",
    "        \n",
    "        # 4. Load Labels\n",
    "        annos = pd.read_csv(label_path, index_col=0).iloc[:, [trait_idx]]\n",
    "        annos = annos.fillna(annos.mean())\n",
    "        \n",
    "        # Standardize labels (Critical for convergence in Ridge/Bayes)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.y = self.scaler.fit_transform(annos).flatten().astype(np.float32)\n",
    "        \n",
    "        # 5. Reproduce Exact CV Splits\n",
    "        # Must use same random_state=27 as DL pipeline\n",
    "        self.kfold = KFold(n_splits=5, shuffle=True, random_state=27)\n",
    "        self.current_seed = seed\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"Returns X_train, X_test, y_train, y_test for the specific seed.\"\"\"\n",
    "        # Iterate to find the split matching the current seed\n",
    "        for i, (train_idx, val_idx) in enumerate(self.kfold.split(self.X, self.y)):\n",
    "            if i == self.current_seed:\n",
    "                return (\n",
    "                    self.X[train_idx], \n",
    "                    self.X[val_idx], \n",
    "                    self.y[train_idx], \n",
    "                    self.y[val_idx]\n",
    "                )\n",
    "        return None, None, None, None\n",
    "\n",
    "# ==========================================\n",
    "# 2. MODEL DEFINITIONS\n",
    "# ==========================================\n",
    "\n",
    "def run_gblup_proxy(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    GBLUP is mathematically equivalent to Ridge Regression (L2).\n",
    "    We use RidgeCV to automatically tune the regularization parameter (alpha).\n",
    "    This is 'Research Grade' because it optimizes hyperparams internally.\n",
    "    \"\"\"\n",
    "    # Alphas to search: logarithmic scale 0.1 to 1000\n",
    "    alphas = np.logspace(-1, 3, 10) \n",
    "    model = RidgeCV(alphas=alphas)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "def run_bayes_proxy(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    BayesianRidge is a robust proxy for BayesC/BayesB.\n",
    "    It infers precision parameters from data, handling 'large p, small n'.\n",
    "    \"\"\"\n",
    "    model = BayesianRidge()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "def run_sota_tabular(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    HistGradientBoosting is Scikit-Learn's implementation of LightGBM.\n",
    "    It handles non-linearities and epistasis better than linear models.\n",
    "    \"\"\"\n",
    "    model = HistGradientBoostingRegressor(\n",
    "        max_iter=100, \n",
    "        max_depth=5, \n",
    "        learning_rate=0.1, \n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "# ==========================================\n",
    "# 3. BENCHMARKING ENGINE\n",
    "# ==========================================\n",
    "\n",
    "def run_benchmarks():\n",
    "    print(\"\\n\" + \"#\"*60)\n",
    "    print(\"RUNNING BENCHMARKING (GBLUP, Bayes, SOTA-Tree)\")\n",
    "    print(\"#\"*60)\n",
    "\n",
    "    # Models to evaluate\n",
    "    benchmark_models = {\n",
    "        \"GBLUP\": run_gblup_proxy,\n",
    "        \"BayesB\": run_bayes_proxy,\n",
    "        \"LightGBM\": run_sota_tabular\n",
    "    }\n",
    "\n",
    "    # Configuration (Reusing your species_config and optimal counts)\n",
    "    # Ensure these variables exist in your notebook context\n",
    "    global species_config, species_optimal_counts\n",
    "    \n",
    "    for species, optimal_count in species_optimal_counts.items():\n",
    "        config = species_config[species]\n",
    "        sel_num = optimal_count\n",
    "        T_folder = f\"T{optimal_count}\"\n",
    "        \n",
    "        # We iterate over models first to save organized JSONs\n",
    "        for model_name, model_func in benchmark_models.items():\n",
    "            print(f\"\\n   >>> Benchmarking Model: {model_name} | Species: {species} | Feature Count: {T_folder}\")\n",
    "            \n",
    "            results_data = {\n",
    "                \"species\": species,\n",
    "                \"feature_count\": optimal_count,\n",
    "                \"model\": model_name,\n",
    "                \"mean_corrs\": {},\n",
    "                \"mean_RMSE\": {},\n",
    "                \"mean_R2\": {}\n",
    "            }\n",
    "\n",
    "            for trait_idx, trait_name in enumerate(config[\"traits\"]):\n",
    "                data_path = f\"./EN/{species}/{T_folder}{trait_name}\"\n",
    "                \n",
    "                # Check if feature selection file exists\n",
    "                if not os.path.exists(f\"{data_path}0.csv\"):\n",
    "                    print(f\"      [Skip] Trait {trait_name}: Feature file not found.\")\n",
    "                    continue\n",
    "\n",
    "                corrs, rmses, r2s = [], [], []\n",
    "                print(f\"      Processing Trait: {trait_name}...\", end=\"\")\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                for seed in range(5):\n",
    "                    try:\n",
    "                        # 1. Load Data\n",
    "                        loader = BenchmarkDataLoader(\n",
    "                            data_path, config[\"label_path\"], config[\"geno_path\"], \n",
    "                            trait_idx, seed, sel_num\n",
    "                        )\n",
    "                        X_train, X_test, y_train, y_test = loader.get_data()\n",
    "                        \n",
    "                        # 2. Train & Predict\n",
    "                        preds = model_func(X_train, y_train, X_test)\n",
    "                        \n",
    "                        # 3. Evaluate\n",
    "                        # Pearson returns (corr, p-value), we take [0]\n",
    "                        p_corr = pearsonr(y_test, preds)[0]\n",
    "                        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "                        r2 = r2_score(y_test, preds)\n",
    "                        \n",
    "                        corrs.append(p_corr)\n",
    "                        rmses.append(rmse)\n",
    "                        r2s.append(r2)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"\\n         [Error] Seed {seed}: {e}\")\n",
    "                \n",
    "                # Aggregate results\n",
    "                if corrs:\n",
    "                    mean_r2 = float(np.mean(r2s))\n",
    "                    mean_corr = float(np.mean(corrs))\n",
    "                    results_data[\"mean_corrs\"][trait_name] = mean_corr\n",
    "                    results_data[\"mean_RMSE\"][trait_name] = float(np.mean(rmses))\n",
    "                    results_data[\"mean_R2\"][trait_name] = mean_r2\n",
    "                    \n",
    "                    print(f\" Done ({time.time()-start_time:.1f}s) | Avg R2: {mean_r2:.4f} | Avg Corr: {mean_corr:.4f}\")\n",
    "\n",
    "            # Save results to JSON\n",
    "            json_filename = f\"{species}_{T_folder}_{model_name}.json\"\n",
    "            with open(json_filename, \"w\") as f:\n",
    "                json.dump(results_data, f, indent=4)\n",
    "            print(f\"   [Saved] {json_filename}\")\n",
    "\n",
    "# Run the benchmark\n",
    "if __name__ == \"__main__\":\n",
    "    run_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8087ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# --- Configuration (Mirrors your provided structure) ---\n",
    "species_config = {\n",
    "    \"rice\": {\n",
    "        \"label_path\": \"./data/rice_pheno.csv\",\n",
    "        \"geno_path\": \"./data/ricerawgeno.csv\",\n",
    "        \"traits\": ['SW', 'FLW', 'AC', 'PH', 'SNPP'],\n",
    "        \"id_col_pheno\": \"NSFTVID\", # Inferred from previous context\n",
    "        \"fix_rice_ids\": True       # Rice specific logic\n",
    "    },\n",
    "    \"sorghum\": {\n",
    "        \"label_path\": \"./data/sorghum_pheno.csv\",\n",
    "        \"geno_path\": \"./data/sorghumrawgeno.csv\",\n",
    "        \"traits\": ['HT', 'MO', 'YLD'],\n",
    "        \"id_col_pheno\": \"ID\",\n",
    "        \"fix_rice_ids\": False\n",
    "    },\n",
    "    \"bulls\": {\n",
    "        \"label_path\": \"./data/bulls_pheno.csv\",\n",
    "        \"geno_path\": \"./data/bullsrawgeno.csv\",\n",
    "        \"traits\": ['MS', 'NMSP', 'VE'],\n",
    "        \"id_col_pheno\": \"ID\",\n",
    "        \"fix_rice_ids\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Optimal Feature Counts (from your script)\n",
    "species_optimal_counts = {\n",
    "    \"rice\": 5000,\n",
    "    \"sorghum\": 5000,\n",
    "    \"bulls\": 5000\n",
    "}\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def clean_rice_ids(geno_id):\n",
    "    \"\"\"Specific cleaner for Rice Genotype IDs (removes suffixes).\"\"\"\n",
    "    if isinstance(geno_id, str) and '_' in geno_id:\n",
    "        return geno_id.rsplit('_', 1)[0]\n",
    "    return geno_id\n",
    "\n",
    "def load_and_align_data(species, config):\n",
    "    print(f\"\\n{'='*15} Loading Data for {species.upper()} {'='*15}\")\n",
    "    \n",
    "    # 1. Load Phenotype\n",
    "    if not os.path.exists(config['label_path']):\n",
    "        print(f\"[Missing] Phenotype file not found: {config['label_path']}\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        pheno = pd.read_csv(config['label_path'])\n",
    "        # Ensure ID is string\n",
    "        pheno[config['id_col_pheno']] = pheno[config['id_col_pheno']].astype(str)\n",
    "        pheno.set_index(config['id_col_pheno'], inplace=True)\n",
    "        print(f\"   Phenotype loaded: {pheno.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   [Error] Loading phenotype: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # 2. Load Genotype\n",
    "    if not os.path.exists(config['geno_path']):\n",
    "        print(f\"[Missing] Genotype file not found: {config['geno_path']}\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        # Read header only first to optimize\n",
    "        geno_preview = pd.read_csv(config['geno_path'], nrows=1)\n",
    "        meta_cols = [c for c in geno_preview.columns if c.lower() in ['chrom', 'pos', 'chromosome', 'position']]\n",
    "        sample_cols = [c for c in geno_preview.columns if c not in meta_cols]\n",
    "        \n",
    "        # Load full file\n",
    "        geno = pd.read_csv(config['geno_path'])\n",
    "        geno_samples = geno[sample_cols]\n",
    "\n",
    "        # Apply Rice Fix if needed\n",
    "        if config['fix_rice_ids']:\n",
    "            print(\"   Applying Rice ID fix...\")\n",
    "            new_names = {c: clean_rice_ids(c) for c in sample_cols}\n",
    "            geno_samples = geno_samples.rename(columns=new_names)\n",
    "\n",
    "        # Transpose\n",
    "        geno_t = geno_samples.T\n",
    "        geno_t.index = geno_t.index.astype(str)\n",
    "        print(f\"   Genotype loaded (Transposed): {geno_t.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   [Error] Loading genotype: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # 3. Align\n",
    "    common = geno_t.index.intersection(pheno.index)\n",
    "    print(f\"   Matched Samples: {len(common)}\")\n",
    "    \n",
    "    if len(common) == 0:\n",
    "        print(\"   [Warning] No matching IDs found!\")\n",
    "        return None, None\n",
    "        \n",
    "    pheno_aligned = pheno.loc[common]\n",
    "    \n",
    "    # Plot Trait Distributions\n",
    "    plot_distributions(species, pheno_aligned, config['traits'])\n",
    "    \n",
    "    return pheno_aligned, geno_t.loc[common]\n",
    "\n",
    "def plot_distributions(species, df, traits):\n",
    "    \"\"\"Plots histograms for the traits defined in config.\"\"\"\n",
    "    valid_traits = [t for t in traits if t in df.columns]\n",
    "    \n",
    "    if not valid_traits:\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    for i, trait in enumerate(valid_traits):\n",
    "        plt.subplot(1, len(valid_traits), i+1)\n",
    "        sns.histplot(df[trait].dropna(), kde=True)\n",
    "        plt.title(f\"{species}: {trait}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    out_path = f\"{species}_pheno_dist.png\"\n",
    "    plt.savefig(out_path)\n",
    "    print(f\"   [Saved] Distribution plot: {out_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def analyze_feature_selection(species_name, config, optimal_count):\n",
    "    \"\"\"\n",
    "    Analyzes the T* CSV files in ./EN/{species}/\n",
    "    to see which features are selected.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Analyzing Selection Files for {species_name.upper()} ---\")\n",
    "    \n",
    "    base_dir = f\"./EN/{species_name}/\"\n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"   [Skip] Directory not found: {base_dir}\")\n",
    "        return\n",
    "\n",
    "    summary_stats = []\n",
    "    \n",
    "    # Iterate over traits defined in config\n",
    "    for trait in config['traits']:\n",
    "        # Construct path pattern based on your logic: T{count}{trait}*.csv\n",
    "        # e.g. ./EN/rice/T5000SW*.csv\n",
    "        file_pattern = os.path.join(base_dir, f\"T{optimal_count}{trait}*.csv\")\n",
    "        files = glob.glob(file_pattern)\n",
    "        \n",
    "        if not files:\n",
    "            print(f\"   No files found for trait {trait} (Pattern: {file_pattern})\")\n",
    "            continue\n",
    "            \n",
    "        # Analyze the first file found (seed 0 usually) or aggregate all\n",
    "        for filepath in files[:1]: # Just look at one per trait for overview\n",
    "            try:\n",
    "                df = pd.read_csv(filepath)\n",
    "                \n",
    "                # Basic Stats\n",
    "                max_score = df['cs'].max()\n",
    "                non_zero = (df['cs'] > 0).sum()\n",
    "                \n",
    "                # Top hits\n",
    "                top = df.sort_values('cs', ascending=False).head(3)\n",
    "                top_str = \"; \".join([f\"idx:{r['index']:.0f}({r['cs']:.0f})\" for _, r in top.iterrows()])\n",
    "                \n",
    "                summary_stats.append({\n",
    "                    'Trait': trait,\n",
    "                    'File': os.path.basename(filepath),\n",
    "                    'Max_Score': max_score,\n",
    "                    'Non_Zero_Markers': non_zero,\n",
    "                    'Top_Markers': top_str\n",
    "                })\n",
    "                \n",
    "                # Manhattan Plot for this file\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.scatter(df['index'], df['cs'], s=10, alpha=0.6)\n",
    "                plt.title(f\"Feature Scores: {species_name} - {trait}\")\n",
    "                plt.xlabel(\"SNP Index\")\n",
    "                plt.ylabel(\"Score\")\n",
    "                plt.savefig(f\"{species_name}_{trait}_manhattan.png\")\n",
    "                plt.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   Error reading {filepath}: {e}\")\n",
    "\n",
    "    if summary_stats:\n",
    "        print(pd.DataFrame(summary_stats).to_string(index=False))\n",
    "        print(f\"   [Saved] Manhattan plots for {species_name} traits.\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "def main():\n",
    "    # Setup directories if they don't exist (just to prevent crashes)\n",
    "    if not os.path.exists(\"./data\"): os.makedirs(\"./data\")\n",
    "    \n",
    "    for species, config in species_config.items():\n",
    "        # 1. Check Data Alignment\n",
    "        load_and_align_data(species, config)\n",
    "        \n",
    "        # 2. Analyze Feature Selection Outputs\n",
    "        if species in species_optimal_counts:\n",
    "            analyze_feature_selection(species, config, species_optimal_counts[species])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01403f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import ARDRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==========================================\n",
    "# 1. MODEL DEFINITIONS (\"No-BS\" versions)\n",
    "# ==========================================\n",
    "\n",
    "class GBLUP:\n",
    "    \"\"\"Genomic Best Linear Unbiased Prediction via Kernel Ridge.\"\"\"\n",
    "    def __init__(self):\n",
    "        # alpha=1.0 corresponds to lambda in ridge regression\n",
    "        self.model = KernelRidge(alpha=1.0, kernel='precomputed')\n",
    "        self.X_train = None\n",
    "\n",
    "    def compute_grm(self, X):\n",
    "        \"\"\"VanRaden Genomic Relationship Matrix\"\"\"\n",
    "        X = np.array(X)\n",
    "        p_freq = np.mean(X, axis=0) / 2\n",
    "        Z = X - 2 * p_freq\n",
    "        scale = 2 * np.sum(p_freq * (1 - p_freq))\n",
    "        # Add small jitter to scale to prevent division by zero\n",
    "        if scale == 0: scale = 1 \n",
    "        K = np.dot(Z, Z.T) / scale\n",
    "        return K\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        K_train = self.compute_grm(X)\n",
    "        self.model.fit(K_train, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # We must compute kernel between Test and Train samples\n",
    "        X_train_np = np.array(self.X_train)\n",
    "        X_test_np = np.array(X_test)\n",
    "        \n",
    "        p_freq = np.mean(X_train_np, axis=0) / 2\n",
    "        scale = 2 * np.sum(p_freq * (1 - p_freq))\n",
    "        if scale == 0: scale = 1\n",
    "        \n",
    "        Z_train = X_train_np - 2 * p_freq\n",
    "        Z_test = X_test_np - 2 * p_freq\n",
    "        \n",
    "        K_test = np.dot(Z_test, Z_train.T) / scale\n",
    "        return self.model.predict(K_test)\n",
    "\n",
    "class BayesB_Proxy:\n",
    "    \"\"\"Approximation of BayesB using ARD Regression (Sparsity inducing).\"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = ARDRegression(max_iter=300)\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "class XGBoostGenomic:\n",
    "    \"\"\"XGBoost optimized for dense SNP data.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = XGBRegressor(\n",
    "            n_estimators=300, max_depth=6, learning_rate=0.05, \n",
    "            subsample=0.8, n_jobs=-1, verbosity=0\n",
    "        )\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "class GenomicKNN:\n",
    "    def __init__(self):\n",
    "        self.model = KNeighborsRegressor(n_neighbors=20, metric='correlation')\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# ==========================================\n",
    "# 2. DATA LOADING LOGIC (Specific to your folder)\n",
    "# ==========================================\n",
    "\n",
    "TRAIT_MAP = {\n",
    "    'rice': ['FLW', 'PH', 'SNPP', 'SW', 'AC'],\n",
    "    'sorghum': ['HT', 'YLD', 'MO'],\n",
    "    # 'soybean': ['protein', 'Yield', 'SdWgt', 'Steartic', 'R8'],\n",
    "    'bulls': ['MS', 'VE', 'NMSP']\n",
    "}\n",
    "\n",
    "PHENO_COL_MAP = {\n",
    "    \"rice\": {\n",
    "        \"FLW\": \"Flag leaf width\",\n",
    "        \"PH\": \"Plant height\",\n",
    "        \"SNPP\": \"Seed number per panicle\",\n",
    "        \"SW\": \"Seed width\",\n",
    "        \"AC\": \"Amylose content\",\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_dataset(species, trait, num_snps, seed):\n",
    "    \"\"\"\n",
    "    Loads X from ./EN/{species}/T{num_snps}{trait}{seed}.csv\n",
    "    Loads y from ./data/{species}_pheno.csv\n",
    "    \"\"\"\n",
    "    # 1. Construct Path for Genotypes (X)\n",
    "    # Note: Using your file list logic (e.g., ./EN/rice/T5000FLW0.csv)\n",
    "    en_path = f\"./EN/{species}/T{num_snps}{trait}{seed}.csv\"\n",
    "    \n",
    "    if not os.path.exists(en_path):\n",
    "        # Fallback for capitalization mismatches often seen in folders\n",
    "        # (e.g. 'protein' vs 'Protein')\n",
    "        possible_files = glob.glob(f\"./EN/{species}/T{num_snps}*{seed}.csv\")\n",
    "        # Try to find case-insensitive match\n",
    "        match = next((f for f in possible_files if trait.lower() in f.lower()), None)\n",
    "        if match:\n",
    "            en_path = match\n",
    "        else:\n",
    "            print(f\"  [Warning] File not found: {en_path}\")\n",
    "            return None, None\n",
    "\n",
    "    # Load Genotypes (Assumed to be purely numeric, no headers or simple headers)\n",
    "    # Your previous code implies these are just SNP matrices.\n",
    "    try:\n",
    "        X = pd.read_csv(en_path)\n",
    "        # Drop non-numeric columns if they sneak in (like IDs)\n",
    "        X = X.select_dtypes(include=[np.number]).values\n",
    "    except Exception as e:\n",
    "        print(f\"  [Error] Could not read X: {e}\")\n",
    "        return None, None\n",
    "    # 2. Load Phenotypes (y)\n",
    "    pheno_path = f\"./data/{species}_pheno.csv\"\n",
    "    if not os.path.exists(pheno_path):  # optional: helps when using attachments in this environment\n",
    "        pheno_path = f\"/mnt/data/{species}_pheno.csv\"\n",
    "\n",
    "    try:\n",
    "        df_pheno = pd.read_csv(pheno_path)\n",
    "\n",
    "        # ---- THE FIX: map rice short codes -> real column names ----\n",
    "        lookup_trait = PHENO_COL_MAP.get(species, {}).get(trait, trait)\n",
    "\n",
    "        cols = df_pheno.columns\n",
    "        target_col = next((c for c in cols if c.strip().lower() == lookup_trait.strip().lower()), None)\n",
    "\n",
    "        if target_col is None:\n",
    "            print(f\"  [Error] Trait '{trait}' not found in {pheno_path}\")\n",
    "            return None, None\n",
    "\n",
    "        y = df_pheno[target_col].values\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [Error] Could not read Phenotype: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # 3. Simple Alignment Check\n",
    "    # We assume X and y are already aligned by row index as is standard in these prep bundles\n",
    "    min_len = min(len(X), len(y))\n",
    "    return X[:min_len], y[:min_len]\n",
    "\n",
    "# ==========================================\n",
    "# 3. MAIN BENCHMARK LOOP\n",
    "# ==========================================\n",
    "\n",
    "def run_benchmarks():\n",
    "    # Only run for T5000 for now to save time, as indicated by your output folder focus\n",
    "    NUM_SNPS = 5000 \n",
    "    \n",
    "    # Define models\n",
    "    models = {\n",
    "        \"GBLUP\": GBLUP(),\n",
    "        \"BayesB\": BayesB_Proxy(),\n",
    "        \"XGBoost\": XGBoostGenomic(),\n",
    "        \"KNN\": GenomicKNN()\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    print(f\"Starting Benchmark for T{NUM_SNPS}...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for species, traits in TRAIT_MAP.items():\n",
    "        print(f\"Processing Species: {species.upper()}\")\n",
    "        \n",
    "        for trait in traits:\n",
    "            print(f\"  > Trait: {trait}\")\n",
    "            \n",
    "            trait_corrs = {m: [] for m in models}\n",
    "            \n",
    "            # Loop through Seeds 0 to 4\n",
    "            for seed in range(5):\n",
    "                X, y = load_dataset(species, trait, NUM_SNPS, seed)\n",
    "                \n",
    "                if X is None:\n",
    "                    continue\n",
    "\n",
    "                # 5-Fold CV on this specific Seed's dataset\n",
    "                # Note: Usually \"Seed 0\" file implies a specific split, \n",
    "                # but we will run a quick CV to get robust metrics.\n",
    "                kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "                \n",
    "                # We aggregate predictions for this seed to calculate one correlation\n",
    "                # Or average correlations across folds. Let's average folds.\n",
    "                \n",
    "                for name, model in models.items():\n",
    "                    fold_scores = []\n",
    "                    \n",
    "                    for train_idx, val_idx in kf.split(X):\n",
    "                        X_train, X_val = X[train_idx], X[val_idx]\n",
    "                        y_train, y_val = y[train_idx], y[val_idx]\n",
    "                        \n",
    "                        # Scale Y\n",
    "                        scaler = StandardScaler()\n",
    "                        y_train = scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "                        \n",
    "                        try:\n",
    "                            model.fit(X_train, y_train)\n",
    "                            preds = model.predict(X_val)\n",
    "                            \n",
    "                            # Inverse scale preds for fair comparison (optional, but good for MSE)\n",
    "                            # For Pearson Corr, scaling doesn't matter.\n",
    "                            \n",
    "                            # Pearson Correlation\n",
    "                            if len(np.unique(preds)) > 1: # Avoid constant output errors\n",
    "                                corr, _ = pearsonr(y_val, preds)\n",
    "                                fold_scores.append(corr)\n",
    "                            else:\n",
    "                                fold_scores.append(0.0)\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            # print(f\"    [Error] {name} failed: {e}\")\n",
    "                            fold_scores.append(0.0)\n",
    "                    \n",
    "                    avg_fold_score = np.mean(fold_scores)\n",
    "                    trait_corrs[name].append(avg_fold_score)\n",
    "\n",
    "            # Summarize Trait Results\n",
    "            print(f\"    Results for {trait} (Avg over 5 seeds):\")\n",
    "            for name in models:\n",
    "                final_score = np.mean(trait_corrs[name])\n",
    "                print(f\"      {name:<10}: {final_score:.4f}\")\n",
    "                \n",
    "                # Save to results list for CSV export\n",
    "                results.append({\n",
    "                    \"Species\": species,\n",
    "                    \"Trait\": trait,\n",
    "                    \"Model\": name,\n",
    "                    \"Pearson_Corr\": final_score\n",
    "                })\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    # Export\n",
    "    df_res = pd.DataFrame(results)\n",
    "    df_res.to_csv(\"benchmark_results_proper.csv\", index=False)\n",
    "    print(\"\\nBenchmark Complete. Saved to 'benchmark_results_proper.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4d1be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "# Assuming EBMGP, DNADataset, setup_seed, train_epoch, evaluate_epoch, PearsonCorrCoef\n",
    "# are defined in previous cells (Cell 1-3).\n",
    "\n",
    "def train_and_evaluate(trait_idx, species, data_path, label_path, geno_path, device,\n",
    "                       learning_rate, epochs, seed, sel_num, ld_threshold=0.8, pooling_type=\"MAP\"):\n",
    "    \"\"\"\n",
    "    Standard training & evaluation loop.\n",
    "    Returns: (mean_pearson_corr, history_dict, final_predictions, final_targets)\n",
    "    \"\"\"\n",
    "    setup_seed(seed)\n",
    "\n",
    "    # 1. Create Datasets & Loaders\n",
    "    traindataset = DNADataset(data_path, label_path, geno_path, trait_idx, seed, sel_num, ld_threshold, is_training=True)\n",
    "    testdataset = DNADataset(data_path, label_path, geno_path, trait_idx, seed, sel_num, ld_threshold, is_training=False)\n",
    "\n",
    "    bs = 32\n",
    "    train_loader = DataLoader(traindataset, batch_size=bs, shuffle=True)\n",
    "    # Important: Shuffle=False for test to keep order for scatter plots\n",
    "    test_loader = DataLoader(testdataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "    # 2. Model, Optimizer, Scheduler, Loss\n",
    "    model = EBMGP(pooling_type=pooling_type).to(device)\n",
    "    \n",
    "    # Hyperparameters from paper\n",
    "    lr = learning_rate\n",
    "    weight_decay = 1e-5\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Cosine Annealing Scheduler\n",
    "    steps = math.ceil(len(traindataset) / bs) * epochs - 1\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=steps)\n",
    "\n",
    "    loss_fn = nn.L1Loss() # MAE Loss\n",
    "    pearson = PearsonCorrCoef().to(device)\n",
    "\n",
    "    # Data capture containers\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    best_test_corr = -1.0\n",
    "    final_preds = []\n",
    "    final_targets = []\n",
    "\n",
    "    # 3. Training Loop\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Train\n",
    "        t_loss = train_epoch(model, train_loader, optimizer, loss_fn, scheduler, device)\n",
    "        history['train_loss'].append(t_loss)\n",
    "        \n",
    "        # Validate\n",
    "        v_loss, vp, vt = evaluate_epoch(model, test_loader, loss_fn, pearson, device)\n",
    "        history['val_loss'].append(v_loss)\n",
    "        test_corr = pearson(vp, vt).item()\n",
    "        \n",
    "        # Save predictions from the final epoch\n",
    "        if epoch == epochs:\n",
    "            final_preds = vp.cpu().numpy().flatten()\n",
    "            final_targets = vt.cpu().numpy().flatten()\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs:\n",
    "            print(f\"   Epoch {epoch}/{epochs} | Train Loss: {t_loss:.4f} | Val Loss: {v_loss:.4f} | Val Corr: {test_corr:.4f}\")\n",
    "\n",
    "    # Final Evaluation on Test Set\n",
    "    mean_corr = pearson(vp, vt).item()\n",
    "    print(f\"Finished. Final Test Correlation: {mean_corr:.4f}\")\n",
    "    \n",
    "    return mean_corr, history, final_preds, final_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e6129a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating LD Ablation Curves...\n",
      "Saved: LD_Ablation_Curves.png\n",
      "\n",
      "Generating Benchmark Heatmap...\n",
      "\n",
      "Generating Pooling Comparison...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1308.12x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Set global style\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "def load_results_from_json(pattern):\n",
    "    \"\"\"Helper to load all JSONs matching a pattern into a DataFrame\"\"\"\n",
    "    data = []\n",
    "    files = glob.glob(pattern)\n",
    "    for f in files:\n",
    "        with open(f, 'r') as file:\n",
    "            content = json.load(file)\n",
    "            # Flatten the 'mean_corrs' dict into rows\n",
    "            for trait, score in content.get('mean_corrs', {}).items():\n",
    "                row = content.copy()\n",
    "                del row['mean_corrs'] # remove dict\n",
    "                if 'mean_RMSE' in row: del row['mean_RMSE']\n",
    "                row['Trait'] = trait\n",
    "                row['Pearson_Corr'] = score\n",
    "                data.append(row)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. LD Ablation Curves (Line Plot)\n",
    "# ---------------------------------------------------------\n",
    "print(\"Generating LD Ablation Curves...\")\n",
    "df_ld = load_results_from_json(\"*_LD*.json\")\n",
    "\n",
    "if not df_ld.empty:\n",
    "    # Filter for MAP pooling if mixed\n",
    "    if 'pooling_type' in df_ld.columns:\n",
    "        df_ld = df_ld[df_ld['pooling_type'] == 'MAP']\n",
    "    \n",
    "    g = sns.relplot(\n",
    "        data=df_ld, \n",
    "        x=\"ld_threshold\", y=\"Pearson_Corr\", \n",
    "        hue=\"Trait\", col=\"species\", \n",
    "        kind=\"line\", marker=\"o\", linewidth=2.5,\n",
    "        height=4, aspect=1, facet_kws={'sharey': False}\n",
    "    )\n",
    "    g.set_titles(\"{col_name}\")\n",
    "    g.set_axis_labels(\"LD Threshold ()\", \"Pearson Correlation\")\n",
    "    plt.savefig(\"LD_Ablation_Curves.png\", dpi=300, bbox_inches='tight')\n",
    "    print(\"Saved: LD_Ablation_Curves.png\")\n",
    "else:\n",
    "    print(\"No LD ablation JSON files found.\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Benchmark Heatmap (Method x Trait)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nGenerating Benchmark Heatmap...\")\n",
    "# Load EBMGP results (assuming LD0.8 MAP is the standard EBMGP)\n",
    "df_ebmgp = load_results_from_json(\"*_LD0.8.json\")\n",
    "if not df_ebmgp.empty:\n",
    "    df_ebmgp['Model'] = 'EBMGP'\n",
    "    df_ebmgp = df_ebmgp[['species', 'Trait', 'Model', 'Pearson_Corr']]\n",
    "\n",
    "# Load Benchmark results\n",
    "df_bench = load_results_from_json(\"*_GBLUP.json\") \n",
    "df_bench = pd.concat([df_bench, load_results_from_json(\"*_BayesB.json\")])\n",
    "df_bench = pd.concat([df_bench, load_results_from_json(\"*_LightGBM.json\")])\n",
    "\n",
    "if not df_bench.empty and not df_ebmgp.empty:\n",
    "    df_final = pd.concat([df_ebmgp, df_bench], ignore_index=True)\n",
    "    \n",
    "    # Create a pivot table for the heatmap\n",
    "    # Pivot: Index=Trait(Species), Columns=Model, Values=Corr\n",
    "    df_final['Label'] = df_final['species'].str.capitalize() + \" - \" + df_final['Trait']\n",
    "    pivot_df = df_final.pivot_table(index='Label', columns='Model', values='Pearson_Corr')\n",
    "    \n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.heatmap(pivot_df, annot=True, cmap=\"viridis\", fmt=\".3f\", linewidths=.5)\n",
    "    plt.title(\"Pearson Correlation: Methods vs Traits\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Benchmark_Heatmap.png\", dpi=300)\n",
    "    print(\"Saved: Benchmark_Heatmap.png\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Pooling Comparison (Bar Chart)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nGenerating Pooling Comparison...\")\n",
    "# We look for files that look like species_T5000_TYPE.json where TYPE is MAP, AVG, etc.\n",
    "# Note: Reuse logic from run_pooling_ablation naming convention\n",
    "df_pool = pd.DataFrame()\n",
    "for p_type in [\"MAP\", \"AVG\", \"MAX\", \"LIP\"]:\n",
    "    temp = load_results_from_json(f\"*_{p_type}.json\")\n",
    "    temp['Pooling'] = p_type\n",
    "    df_pool = pd.concat([df_pool, temp])\n",
    "\n",
    "if not df_pool.empty:\n",
    "    g = sns.catplot(\n",
    "        data=df_pool, \n",
    "        x=\"Trait\", y=\"Pearson_Corr\", hue=\"Pooling\", col=\"species\",\n",
    "        kind=\"bar\", height=4, aspect=1.2, sharey=False\n",
    "    )\n",
    "    g.set_axis_labels(\"Trait\", \"Pearson Correlation\")\n",
    "    g.set_titles(\"{col_name}\")\n",
    "    plt.savefig(\"Pooling_Comparison_Bar.png\", dpi=300, bbox_inches='tight')\n",
    "    print(\"Saved: Pooling_Comparison_Bar.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4319ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Configuration for the \"Hero\" plots in the LaTeX\n",
    "hero_plots = [\n",
    "    {\"species\": \"rice\", \"trait\": \"SW\", \"seed\": 0},\n",
    "    {\"species\": \"sorghum\", \"trait\": \"YLD\", \"seed\": 0},\n",
    "    {\"species\": \"bulls\", \"trait\": \"NMSP\", \"seed\": 0}\n",
    "]\n",
    "\n",
    "# Ensure species_config is defined (from your notebook)\n",
    "# If not already in memory, uncomment the dictionary definition from your Cell 6\n",
    "\n",
    "for item in hero_plots:\n",
    "    sp = item['species']\n",
    "    tr = item['trait']\n",
    "    sd = item['seed']\n",
    "    \n",
    "    print(f\"Generating Diagnostics for {sp} - {tr}...\")\n",
    "    \n",
    "    # Setup paths\n",
    "    # Note: Adjust Feature Count (T5000) if you used something else\n",
    "    t_folder = \"T5000\" \n",
    "    data_path = f\"./EN/{sp}/{t_folder}{tr}\"\n",
    "    \n",
    "    config = species_config[sp]\n",
    "    trait_idx = config['traits'].index(tr)\n",
    "    \n",
    "    # Check if data exists\n",
    "    if not os.path.exists(f\"{data_path}{sd}.csv\"):\n",
    "        print(f\"Skipping {sp}-{tr}: File {data_path}{sd}.csv not found.\")\n",
    "        continue\n",
    "\n",
    "    # RUN TRAINING (Using the function that returns history)\n",
    "    # Ensure train_and_evaluate from CELL 13 is loaded!\n",
    "    corr, history, preds, targets = train_and_evaluate(\n",
    "        trait_idx=trait_idx,\n",
    "        species=sp, # Added species arg if your modified func needs it, else remove\n",
    "        data_path=data_path,\n",
    "        label_path=config[\"label_path\"],\n",
    "        geno_path=config[\"geno_path\"],\n",
    "        device=device,\n",
    "        learning_rate=0.0005,\n",
    "        epochs=100,\n",
    "        seed=sd,\n",
    "        sel_num=5000,\n",
    "        ld_threshold=0.8,\n",
    "        pooling_type=\"MAP\" # Or \"LIP\" if that was your best for Rice\n",
    "    )\n",
    "    \n",
    "    # 1. Plot Loss Curve\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title(f\"Training Convergence: {sp.capitalize()} {tr}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"L1 Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    loss_filename = f\"{sp}_{tr}_loss_curve.png\"\n",
    "    plt.savefig(loss_filename, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"   Saved {loss_filename}\")\n",
    "    \n",
    "    # 2. Plot Scatter (Pred vs Obs)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    \n",
    "    # Calculate limits for a square plot\n",
    "    d_min = min(np.min(preds), np.min(targets))\n",
    "    d_max = max(np.max(preds), np.max(targets))\n",
    "    pad = (d_max - d_min) * 0.1\n",
    "    \n",
    "    plt.scatter(targets, preds, alpha=0.6, edgecolors='w', s=50)\n",
    "    plt.plot([d_min-pad, d_max+pad], [d_min-pad, d_max+pad], 'r--', label=\"Perfect Fit\")\n",
    "    \n",
    "    plt.title(f\"{sp.capitalize()} {tr} (r={corr:.3f})\")\n",
    "    plt.xlabel(\"Observed Phenotype (Standardized)\")\n",
    "    plt.ylabel(\"Predicted Phenotype\")\n",
    "    plt.xlim(d_min-pad, d_max+pad)\n",
    "    plt.ylim(d_min-pad, d_max+pad)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    scat_filename = f\"{sp}_{tr}_scatter.png\"\n",
    "    plt.savefig(scat_filename, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"   Saved {scat_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b8cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the specific traits you listed in the LaTeX images section\n",
    "target_plots = [\n",
    "    (\"rice\", \"AC\"),\n",
    "    (\"rice\", \"FLW\"),\n",
    "    (\"rice\", \"PH\"),\n",
    "    (\"rice\", \"SNPP\"),\n",
    "    (\"rice\", \"SW\")\n",
    "]\n",
    "\n",
    "print(\"Generating Manhattan Plots...\")\n",
    "\n",
    "for species, trait in target_plots:\n",
    "    # Construct pattern to find the file (e.g., ./EN/rice/T5000AC0.csv)\n",
    "    # Using T5000 as default based on your text\n",
    "    file_pattern = f\"./EN/{species}/T5000{trait}0.csv\" \n",
    "    \n",
    "    files = glob.glob(file_pattern)\n",
    "    if not files:\n",
    "        print(f\"  [Warning] No file found for {species} - {trait}\")\n",
    "        continue\n",
    "        \n",
    "    # Use the first seed (0) found\n",
    "    filepath = files[0]\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        # 'index' is the SNP location, 'cs' is the Elastic Net absolute coefficient\n",
    "        plt.scatter(df['index'], df['cs'], c='navy', alpha=0.6, s=15, edgecolors='none')\n",
    "        \n",
    "        plt.title(f\"Feature Importance (Elastic Net): {species.capitalize()} - {trait}\")\n",
    "        plt.xlabel(\"SNP Index\")\n",
    "        plt.ylabel(\"Selection Score (Abs Coeff)\")\n",
    "        \n",
    "        # Add a baseline threshold line for visual context\n",
    "        plt.axhline(y=0, color='grey', linewidth=0.5)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        out_name = f\"{species}_{trait}_manhattan.png\"\n",
    "        plt.savefig(out_name, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"  Saved {out_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error plotting {species}-{trait}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
